{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequence Model Research\n",
    "\n",
    "The scope of this notebook is to assess and train different sequence models given the training data generated.\n",
    "\n",
    "Training data is generated based on financial time series data labeled with potential profits using a buy-sell system.\n",
    "\n",
    "The goal is to create a sequence model that can choose favourable stock charts equal to or better than a human can via traditional technical analysis.\n",
    "\n",
    "# Import Libraries and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# Define the data directory relative to the script location\n",
    "data_dir = 'data'\n",
    "\n",
    "# Define the file paths\n",
    "sequences_path = os.path.join(data_dir, 'sequences.npy')\n",
    "labels_path = os.path.join(data_dir, 'labels.npy')\n",
    "metadata_path = os.path.join(data_dir, 'metadata.npy')\n",
    "\n",
    "# List of feature names\n",
    "feature_names = [\n",
    "    'Consol_Len_Bars', 'Consol_Depth_Percent',\n",
    "    'Distance_to_21EMA', 'Distance_to_50SMA', 'Distance_to_200SMA', \n",
    "    'RSL_NH_Count', 'RSL_Slope', 'Up_Down_Days', \n",
    "    'Stage 2', 'UpDownVolumeRatio', 'ATR', '%B'\n",
    "]\n",
    "\n",
    "# Load the data\n",
    "try:\n",
    "    data_sequences = np.load(sequences_path)\n",
    "    data_labels = np.load(labels_path)\n",
    "    data_metadata = np.load(metadata_path)\n",
    "\n",
    "    # Number of examples to select\n",
    "    num_examples = 115000\n",
    "\n",
    "    # Generate a random permutation of indices\n",
    "    indices = np.random.permutation(len(data_sequences))\n",
    "\n",
    "    # Select the first `num_examples` indices\n",
    "    selected_indices = indices[:num_examples]\n",
    "\n",
    "    # Use the selected indices to create the random subset\n",
    "    data_sequences = data_sequences[selected_indices, :, :]\n",
    "    data_labels = data_labels[selected_indices]\n",
    "    data_metadata = data_metadata[selected_indices]\n",
    "\n",
    "    # Inspect the shape and size of the loaded data before slicing\n",
    "    print(f'Loaded sequences shape: {data_sequences.shape}')\n",
    "    print(f'Loaded sequences size: {data_sequences.size}')\n",
    "    print(f'Loaded labels shape: {data_labels.shape}')\n",
    "    print(f'Loaded metadata shape: {data_metadata.shape}')\n",
    "\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Error loading files: {e}\")\n",
    "except ValueError as e:\n",
    "    print(f\"Value error: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "### NaN anf INF Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Dictionary to map variable names to their corresponding data arrays\n",
    "data_dict = {\n",
    "    'data_sequences': data_sequences,\n",
    "    'data_labels': data_labels,\n",
    "}\n",
    "\n",
    "# Using a dictionary to iterate over variables\n",
    "for var_name, data in data_dict.items():\n",
    "    num_nans = np.sum(np.isnan(data))\n",
    "    num_infs = np.sum(np.isinf(data))\n",
    "    print(f\"NaNs in {var_name}: {num_nans}\")\n",
    "    print(f\"Infs in {var_name}: {num_infs}\")\n",
    "\n",
    "    # Remove NaNs and Infs\n",
    "    if num_nans > 0 or num_infs > 0:\n",
    "        data_dict[var_name][:] = np.nan_to_num(data, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "        num_nans_after = np.sum(np.isnan(data))\n",
    "        num_infs_after = np.sum(np.isinf(data))\n",
    "        print(f\"NaNs remaining in {var_name} after removal: {num_nans_after}\")\n",
    "        print(f\"Infs remaining in {var_name} after removal: {num_infs_after}\")\n",
    "\n",
    "print(\"NaN and Inf removal completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Corrupted sequence removal\n",
    "\n",
    "99% of stocks I buy will be below 1000, with a few above 1000, although they are important.\n",
    "\n",
    "I also noticed quite a few training examples have weird price data, which I filter out below.\n",
    "\n",
    "I noticed with thresholds above 3e3, the max is the threshold, which is very suspect.\n",
    "\n",
    "The loss of training examples is insignificant, and the result is better normalization of the data and obviously no corrupted sequences.\n",
    "\n",
    "#### Feature Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def print_feature_stats(data_sequences, feature_names):\n",
    "    print(\"Feature Statistics:\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    for i, feature_name in enumerate(feature_names):\n",
    "        feature_data = data_sequences[:, :, i].flatten()\n",
    "        \n",
    "        stats = {\n",
    "            \"Mean\": np.mean(feature_data),\n",
    "            \"Median\": np.median(feature_data),\n",
    "            \"Std Dev\": np.std(feature_data),\n",
    "            \"Min\": np.min(feature_data),\n",
    "            \"Max\": np.max(feature_data),\n",
    "            \"25th Percentile\": np.percentile(feature_data, 25),\n",
    "            \"75th Percentile\": np.percentile(feature_data, 75),\n",
    "            \"Skewness\": pd.Series(feature_data).skew(),\n",
    "            \"Kurtosis\": pd.Series(feature_data).kurtosis(),\n",
    "            \"Zero Count\": np.sum(feature_data == 0),\n",
    "            \"Zero Percentage\": np.mean(feature_data == 0) * 100\n",
    "        }\n",
    "        \n",
    "        print(f\"Feature: {feature_name}\")\n",
    "        for stat_name, stat_value in stats.items():\n",
    "            print(f\"  {stat_name}: {stat_value:.4f}\")\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "# Call the function to print statistics\n",
    "print_feature_stats(data_sequences, feature_names)\n",
    "\n",
    "# Additional overall statistics\n",
    "print(\"Overall Dataset Statistics:\")\n",
    "print(f\"Total number of sequences: {data_sequences.shape[0]}\")\n",
    "print(f\"Sequence length: {data_sequences.shape[1]}\")\n",
    "print(f\"Number of features: {data_sequences.shape[2]}\")\n",
    "print(f\"Total number of data points: {data_sequences.size}\")\n",
    "print(f\"Memory usage: {data_sequences.nbytes / (1024 * 1024):.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization of Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Define the indices based on the provided feature names\n",
    "feature_names = [\n",
    "    'Consol_Len_Bars', 'Consol_Depth_Percent',\n",
    "    'Distance_to_21EMA', 'Distance_to_50SMA', 'Distance_to_200SMA', \n",
    "    'RSL_NH_Count', 'RSL_Slope', 'Up_Down_Days', \n",
    "    'Stage 2', 'UpDownVolumeRatio', 'ATR', '%B'\n",
    "]\n",
    "\n",
    "feature_indices = {name: idx for idx, name in enumerate(feature_names)}\n",
    "\n",
    "# Function to remove outliers and cap values\n",
    "def preprocess_data(sequences, labels):\n",
    "    # Reshape sequences to 2D array for easier processing (flatten the timesteps)\n",
    "    num_sequences, num_timesteps, num_features = sequences.shape\n",
    "    sequences_reshaped = sequences.reshape(-1, num_features)\n",
    "    \n",
    "    # Create a mask to filter out invalid sequences\n",
    "    valid_mask = (\n",
    "        (sequences_reshaped[:, feature_indices['Distance_to_21EMA']] <= 100) &\n",
    "        (sequences_reshaped[:, feature_indices['Distance_to_50SMA']] <= 200) &\n",
    "        (sequences_reshaped[:, feature_indices['Distance_to_200SMA']] <= 500)\n",
    "    )\n",
    "    \n",
    "    # Reshape the valid_mask to match the original sequence shape\n",
    "    valid_mask_reshaped = valid_mask.reshape(num_sequences, num_timesteps)\n",
    "    \n",
    "    # Filter out sequences with any invalid timesteps\n",
    "    valid_sequences_mask = valid_mask_reshaped.all(axis=1)\n",
    "    filtered_sequences = sequences[valid_sequences_mask]\n",
    "    filtered_labels = labels[valid_sequences_mask]\n",
    "    \n",
    "    # Cap 'UpDownVolumeRatio' at 10\n",
    "    filtered_sequences[:, :, feature_indices['UpDownVolumeRatio']] = np.minimum(\n",
    "        filtered_sequences[:, :, feature_indices['UpDownVolumeRatio']], 10\n",
    "    )\n",
    "    \n",
    "    # Normalize the features using Z-score normalization\n",
    "    scaler = StandardScaler()\n",
    "    \n",
    "    # Flatten the sequence again for normalization\n",
    "    filtered_sequences_reshaped = filtered_sequences.reshape(-1, num_features)\n",
    "    \n",
    "    # Normalize\n",
    "    normalized_data_reshaped = scaler.fit_transform(filtered_sequences_reshaped)\n",
    "    \n",
    "    # Reshape back to the original 3D shape\n",
    "    normalized_data = normalized_data_reshaped.reshape(filtered_sequences.shape)\n",
    "    \n",
    "    return normalized_data, filtered_labels\n",
    "\n",
    "# Function to print feature statistics\n",
    "def print_feature_stats(data_sequences, feature_names):\n",
    "    print(\"Feature Statistics:\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    for i, feature_name in enumerate(feature_names):\n",
    "        feature_data = data_sequences[:, :, i].flatten()\n",
    "        \n",
    "        stats = {\n",
    "            \"Mean\": np.mean(feature_data),\n",
    "            \"Median\": np.median(feature_data),\n",
    "            \"Std Dev\": np.std(feature_data),\n",
    "            \"Min\": np.min(feature_data),\n",
    "            \"Max\": np.max(feature_data),\n",
    "            \"25th Percentile\": np.percentile(feature_data, 25),\n",
    "            \"75th Percentile\": np.percentile(feature_data, 75),\n",
    "            \"Skewness\": pd.Series(feature_data).skew(),\n",
    "            \"Kurtosis\": pd.Series(feature_data).kurtosis(),\n",
    "            \"Zero Count\": np.sum(feature_data == 0),\n",
    "            \"Zero Percentage\": np.mean(feature_data == 0) * 100\n",
    "        }\n",
    "        \n",
    "        print(f\"Feature: {feature_name}\")\n",
    "        for stat_name, stat_value in stats.items():\n",
    "            print(f\"  {stat_name}: {stat_value:.4f}\")\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "# Example usage with data_sequences and data_labels\n",
    "# Assuming data_sequences is loaded and has shape (115000, 63, 12)\n",
    "\n",
    "# Process the data\n",
    "normalized_data, processed_labels = preprocess_data(data_sequences, data_labels)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Candidate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import mlflow\n",
    "import mlflow.pytorch\n",
    "from tqdm.notebook import tqdm\n",
    "from IPython.display import display\n",
    "import ipywidgets as widgets\n",
    "import psutil\n",
    "import GPUtil\n",
    "import time\n",
    "import threading\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "RANDOM_SEED = 42\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "# Check for CUDA availability\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "def get_system_metrics():\n",
    "    \"\"\"\n",
    "    Get system metrics including CPU usage, memory usage, and GPU utilization.\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary containing system metrics.\n",
    "    \"\"\"\n",
    "    metrics = {}\n",
    "    metrics['cpu_percent'] = psutil.cpu_percent(interval=1)\n",
    "    metrics['memory_percent'] = psutil.virtual_memory().percent\n",
    "    \n",
    "    gpus = GPUtil.getGPUs()\n",
    "    if gpus:\n",
    "        metrics['gpu_utilization'] = gpus[0].load * 100\n",
    "        metrics['gpu_memory_used'] = gpus[0].memoryUsed\n",
    "        metrics['gpu_memory_total'] = gpus[0].memoryTotal\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "def log_system_metrics():\n",
    "    \"\"\"\n",
    "    Continuously log system metrics to MLflow.\n",
    "    \"\"\"\n",
    "    while True:\n",
    "        metrics = get_system_metrics()\n",
    "        for key, value in metrics.items():\n",
    "            mlflow.log_metric(key, value)\n",
    "        time.sleep(5)  # Log every 5 seconds\n",
    "\n",
    "def display_model_details(model, params):\n",
    "    \"\"\"\n",
    "    Display the model architecture and hyperparameters.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): The PyTorch model.\n",
    "        params (dict): The hyperparameters of the model.\n",
    "    \"\"\"\n",
    "    print(\"\\nModel Architecture:\")\n",
    "    print(model)\n",
    "    \n",
    "    print(\"\\nModel Hyperparameters:\")\n",
    "    for param, value in params.items():\n",
    "        print(f\"{param}: {value}\")\n",
    "\n",
    "def process_labels(labels, threshold):\n",
    "    \"\"\"\n",
    "    Process labels based on a given threshold.\n",
    "\n",
    "    Args:\n",
    "        labels (numpy array): The array of labels.\n",
    "        threshold (float): The threshold value to binarize the labels.\n",
    "\n",
    "    Returns:\n",
    "        numpy array: Binarized labels.\n",
    "    \"\"\"\n",
    "    return (labels > threshold).astype(int)\n",
    "\n",
    "def split_data(normalized_data, processed_labels):\n",
    "    \"\"\"\n",
    "    Split data into training, validation, and test sets using sklearn's train_test_split.\n",
    "\n",
    "    Args:\n",
    "        normalized_data (numpy array): Normalized input data.\n",
    "        processed_labels (numpy array): Corresponding processed labels.\n",
    "\n",
    "    Returns:\n",
    "        tuple: Train, validation, and test data and labels.\n",
    "    \"\"\"\n",
    "    print(\"Splitting data into train, validation, and test sets...\")\n",
    "    \n",
    "    # First split: separate test set\n",
    "    X_train_val, X_test, y_train_val, y_test = train_test_split(\n",
    "        normalized_data, processed_labels, test_size=0.2, random_state=RANDOM_SEED, stratify=processed_labels\n",
    "    )\n",
    "    \n",
    "    # Second split: separate train and validation sets\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X_train_val, y_train_val, test_size=0.25, random_state=RANDOM_SEED, stratify=y_train_val\n",
    "    )  # 0.25 x 0.8 = 0.2, so validation set is 20% of original data\n",
    "    \n",
    "    print(f\"Train set size: {len(X_train)}\")\n",
    "    print(f\"Validation set size: {len(X_val)}\")\n",
    "    print(f\"Test set size: {len(X_test)}\")\n",
    "    \n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test\n",
    "\n",
    "def list_mlflow_runs(experiment_name):\n",
    "    \"\"\"\n",
    "    List all MLflow runs for a given experiment.\n",
    "\n",
    "    Args:\n",
    "        experiment_name (str): Name of the MLflow experiment.\n",
    "\n",
    "    Returns:\n",
    "        list: List of MLflow runs.\n",
    "    \"\"\"\n",
    "    mlflow.set_experiment(experiment_name)\n",
    "    client = mlflow.tracking.MlflowClient()\n",
    "    experiment = client.get_experiment_by_name(experiment_name)\n",
    "    runs = client.search_runs(experiment_ids=[experiment.experiment_id])\n",
    "    return runs\n",
    "\n",
    "def select_model_ui(runs):\n",
    "    \"\"\"\n",
    "    Create a UI for selecting a model from available MLflow runs.\n",
    "\n",
    "    Args:\n",
    "        runs (list): List of MLflow runs.\n",
    "\n",
    "    Returns:\n",
    "        widgets.Dropdown: A dropdown widget for model selection.\n",
    "    \"\"\"\n",
    "    options = []\n",
    "    for run in runs:\n",
    "        run_name = run.data.tags.get('mlflow.runName', 'N/A')\n",
    "        test_f1 = run.data.metrics.get('test_f1', 'N/A')\n",
    "        try:\n",
    "            test_f1 = float(test_f1)\n",
    "            test_f1 = f\"{test_f1:.4f}\"\n",
    "        except (ValueError, TypeError):\n",
    "            pass  # Keep test_f1 as 'N/A' or any non-numeric value\n",
    "        \n",
    "        options.append(f\"{run_name} (F1: {test_f1})\")\n",
    "    \n",
    "    if not options:\n",
    "        print(\"No available models to select.\")\n",
    "        return None\n",
    "    \n",
    "    dropdown = widgets.Dropdown(\n",
    "        options=options,\n",
    "        description='Select Model:',\n",
    "        disabled=False,\n",
    "    )\n",
    "    display(dropdown)\n",
    "\n",
    "    def on_change(change):\n",
    "        if change['type'] == 'change' and change['name'] == 'value':\n",
    "            selected_index = dropdown.index\n",
    "            selected_run = runs[selected_index]\n",
    "            print(f\"\\nYou selected:\")\n",
    "            print(f\"Run Name: {selected_run.data.tags.get('mlflow.runName', 'N/A')}\")\n",
    "            print(f\"Run ID: {selected_run.info.run_id}\")\n",
    "            print(f\"F1 Score: {selected_run.data.metrics.get('test_f1', 'N/A')}\")\n",
    "    \n",
    "    dropdown.observe(on_change, names='value')\n",
    "    return dropdown\n",
    "\n",
    "def load_model(run):\n",
    "    \"\"\"\n",
    "    Load a PyTorch model from a given MLflow run.\n",
    "\n",
    "    Args:\n",
    "        run (mlflow.entities.Run): The MLflow run containing the model.\n",
    "\n",
    "    Returns:\n",
    "        torch.nn.Module: The loaded PyTorch model.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        model_uri = f\"runs:/{run.info.run_id}/model\"\n",
    "        model = mlflow.pytorch.load_model(model_uri)\n",
    "        return model\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading model: {e}\")\n",
    "        raise\n",
    "\n",
    "class EarlyStopping:\n",
    "    \"\"\"\n",
    "    Early stopping utility to stop the training when the monitored metric has stopped improving.\n",
    "    \"\"\"\n",
    "    def __init__(self, patience=5, verbose=False):\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_precision_max = 0\n",
    "\n",
    "    def __call__(self, val_precision, model, epoch):\n",
    "        score = val_precision\n",
    "\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_precision, model)\n",
    "        elif score < self.best_score:\n",
    "            self.counter += 1\n",
    "            if self.verbose:\n",
    "                print(f\"EarlyStopping counter: {self.counter} out of {self.patience}\")\n",
    "            mlflow.log_metric(\"early_stopping_counter\", self.counter, step=epoch)\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_precision, model)\n",
    "            self.counter = 0\n",
    "            mlflow.log_metric(\"early_stopping_counter\", self.counter, step=epoch)\n",
    "\n",
    "    def save_checkpoint(self, val_precision, model):\n",
    "        \"\"\"Saves model when validation precision increases.\"\"\"\n",
    "        if self.verbose:\n",
    "            print(f\"Validation precision increased ({self.val_precision_max:.6f} --> {val_precision:.6f}).  Saving model ...\")\n",
    "        torch.save(model.state_dict(), 'checkpoints/checkpoint.pt')\n",
    "        self.val_precision_max = val_precision\n",
    "\n",
    "def train_model(model, train_loader, val_loader, params, n_epochs=10):\n",
    "    \"\"\"\n",
    "    Train a PyTorch model.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): The model to train.\n",
    "        train_loader (DataLoader): DataLoader for training data.\n",
    "        val_loader (DataLoader): DataLoader for validation data.\n",
    "        params (dict): Training parameters.\n",
    "        n_epochs (int): Number of epochs to train.\n",
    "\n",
    "    Returns:\n",
    "        torch.nn.Module: The trained model.\n",
    "    \"\"\"\n",
    "    print(\"\\nPreparing for training...\")\n",
    "    \n",
    "    # Compute class weights\n",
    "    all_labels = torch.cat([y for _, y in train_loader])\n",
    "    class_weights = compute_class_weight('balanced', classes=np.unique(all_labels.numpy()), y=all_labels.numpy())\n",
    "    class_weights = torch.FloatTensor(class_weights).to(device)\n",
    "    \n",
    "    criterion = nn.BCEWithLogitsLoss(pos_weight=class_weights[1])  # Assuming binary classification\n",
    "    \n",
    "    lr = float(params.get('LEARNING_RATE', 0.001))\n",
    "    weight_decay = float(params.get('L2_REGULARIZATION', 0.0))\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "    best_val_precision = 0\n",
    "    best_model = None\n",
    "\n",
    "    print(f\"\\nStarting training for {n_epochs} epochs...\")\n",
    "    \n",
    "    # Create a progress bar for epochs\n",
    "    epoch_pbar = tqdm(range(n_epochs), desc=\"Training Progress\")\n",
    "    \n",
    "    early_stopping = EarlyStopping(patience=250, verbose=True)\n",
    "    \n",
    "    for epoch in epoch_pbar:\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        batch_pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{n_epochs}\", leave=False)\n",
    "        for batch_X, batch_y in batch_pbar:\n",
    "            batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_X)\n",
    "            \n",
    "            outputs = outputs.view(-1)\n",
    "            batch_y = batch_y.float()\n",
    "            \n",
    "            loss = criterion(outputs, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "            batch_pbar.set_postfix({'loss': total_loss / len(train_loader)})\n",
    "\n",
    "        # Evaluate model after each epoch\n",
    "        val_precision, val_recall, val_f1, _, _ = evaluate_model(model, val_loader, params)\n",
    "        \n",
    "        # Calculate validation loss\n",
    "        val_loss = 0\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for val_X, val_y in val_loader:\n",
    "                val_X, val_y = val_X.to(device), val_y.to(device)\n",
    "                val_outputs = model(val_X)\n",
    "                val_outputs = val_outputs.view(-1)\n",
    "                val_y = val_y.float()\n",
    "                val_loss += criterion(val_outputs, val_y).item()\n",
    "        \n",
    "        val_loss /= len(val_loader)\n",
    "        \n",
    "        # Update epoch progress bar with validation precision\n",
    "        epoch_pbar.set_postfix({'Val Precision': f'{val_precision:.4f}'})\n",
    "        \n",
    "        # Log metrics to MLflow\n",
    "        mlflow.log_metric(\"train_loss\", total_loss / len(train_loader), step=epoch)\n",
    "        mlflow.log_metric(\"val_loss\", val_loss, step=epoch)\n",
    "        mlflow.log_metric(\"val_precision\", val_precision, step=epoch)\n",
    "        mlflow.log_metric(\"val_recall\", val_recall, step=epoch)\n",
    "        mlflow.log_metric(\"val_f1\", val_f1, step=epoch)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{n_epochs}: Loss: {total_loss/len(train_loader):.4f}, \"\n",
    "              f\"Val Loss: {val_loss:.4f}, Val Precision: {val_precision:.4f}, Val Recall: {val_recall:.4f}, Val F1: {val_f1:.4f}\")\n",
    "\n",
    "        early_stopping(val_precision, model, epoch)\n",
    "        \n",
    "        if early_stopping.early_stop:\n",
    "            print(f\"Early stopping at epoch {epoch+1}\")\n",
    "            break\n",
    "\n",
    "        if val_precision > best_val_precision:\n",
    "            best_val_precision = val_precision\n",
    "            best_model = model.state_dict()\n",
    "            print(f\"New best model found at epoch {epoch+1}\")\n",
    "\n",
    "    print(\"\\nTraining complete. Loading best model...\")\n",
    "    model.load_state_dict(best_model)\n",
    "    return model\n",
    "\n",
    "def evaluate_model(model, data_loader, params):\n",
    "    \"\"\"\n",
    "    Evaluate a PyTorch model.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): The model to evaluate.\n",
    "        data_loader (DataLoader): DataLoader for evaluation data.\n",
    "        params (dict): Evaluation parameters.\n",
    "\n",
    "    Returns:\n",
    "        tuple: Precision, recall, f1 score, all_labels, all_preds.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    # Convert prediction threshold to float\n",
    "    prediction_threshold = float(params.get('PREDICTION_THRESHOLD', 0.5))\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_X, batch_y in data_loader:\n",
    "            batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "            outputs = model(batch_X)\n",
    "            preds = (outputs > prediction_threshold).float()\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(batch_y.cpu().numpy())\n",
    "\n",
    "    precision = precision_score(all_labels, all_preds, zero_division=0)\n",
    "    recall = recall_score(all_labels, all_preds, zero_division=0)\n",
    "    f1 = f1_score(all_labels, all_preds, zero_division=0)\n",
    "    \n",
    "    return precision, recall, f1, all_labels, all_preds\n",
    "\n",
    "def plot_confusion_matrix(cm, class_names):\n",
    "    \"\"\"\n",
    "    Plot confusion matrix and save it as an image.\n",
    "\n",
    "    Args:\n",
    "        cm (ndarray): Confusion matrix.\n",
    "        class_names (list): List of class names.\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.close(fig)\n",
    "    return fig\n",
    "\n",
    "def main(normalized_data, processed_labels):\n",
    "    \"\"\"\n",
    "    Main function to handle the entire training and evaluation pipeline.\n",
    "\n",
    "    Args:\n",
    "        normalized_data (numpy array): Normalized input data.\n",
    "        processed_labels (numpy array): Corresponding processed labels.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(\"Loading data...\")\n",
    "\n",
    "        print(\"Listing available models...\")\n",
    "        runs = list_mlflow_runs(\"Stock Analysis Model Tuning 2\")\n",
    "\n",
    "        print(\"Please select a model:\")\n",
    "        model_dropdown = select_model_ui(runs)\n",
    "        if model_dropdown is None:\n",
    "            print(\"No models available for selection.\")\n",
    "            return\n",
    "        \n",
    "        # Number of epochs widget\n",
    "        n_epochs = widgets.IntText(value=1000, description='Epochs:', min=100, max=10000)\n",
    "        display(n_epochs)\n",
    "        \n",
    "        # Start training button\n",
    "        start_button = widgets.Button(description=\"Start Training\")\n",
    "        display(start_button)\n",
    "        \n",
    "        def on_button_click(b):\n",
    "            try:\n",
    "                if model_dropdown.value is None:\n",
    "                    print(\"No model selected.\")\n",
    "                    return\n",
    "                \n",
    "                selected_index = model_dropdown.index\n",
    "                selected_run = runs[selected_index]\n",
    "\n",
    "                print(\"\\nLoading the selected model...\")\n",
    "                model = load_model(selected_run)\n",
    "                model = model.to(device)\n",
    "                print(f\"Loaded model from run: {selected_run.info.run_id}\")\n",
    "                print(f\"Run name: {selected_run.data.tags.get('mlflow.runName', 'N/A')}\")\n",
    "\n",
    "                print(\"\\nRetrieving model parameters...\")\n",
    "                params = selected_run.data.params\n",
    "\n",
    "                # Display model architecture and hyperparameters\n",
    "                display_model_details(model, params)\n",
    "\n",
    "                # Process labels using the PROFIT_THRESH from the selected model\n",
    "                profit_thresh = float(params['PROFIT_THRESH'])\n",
    "                processed_labels_thresholded = process_labels(processed_labels, profit_thresh)\n",
    "\n",
    "                print(f\"Processing labels with PROFIT_THRESH: {profit_thresh}\")\n",
    "                print(f\"Processed labels shape: {processed_labels_thresholded.shape}\")\n",
    "                print(f\"Unique values in processed labels: {np.unique(processed_labels_thresholded)}\")\n",
    "\n",
    "                print(\"Splitting data...\")\n",
    "                X_train, X_val, X_test, y_train, y_val, y_test = split_data(normalized_data, processed_labels_thresholded)\n",
    "\n",
    "                # Create data loaders\n",
    "                batch_size = 64  # You can adjust this\n",
    "                train_dataset = TensorDataset(torch.FloatTensor(X_train), torch.FloatTensor(y_train))\n",
    "                val_dataset = TensorDataset(torch.FloatTensor(X_val), torch.FloatTensor(y_val))\n",
    "                test_dataset = TensorDataset(torch.FloatTensor(X_test), torch.FloatTensor(y_test))\n",
    "                \n",
    "                train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "                val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "                test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "                # Check a batch from the train_loader\n",
    "                sample_batch = next(iter(train_loader))\n",
    "                print(f\"Sample batch X shape: {sample_batch[0].shape}\")\n",
    "                print(f\"Sample batch y shape: {sample_batch[1].shape}\")\n",
    "                print(f\"Sample batch X type: {sample_batch[0].dtype}\")\n",
    "                print(f\"Sample batch y type: {sample_batch[1].dtype}\")\n",
    "                print(f\"Unique values in sample batch y: {torch.unique(sample_batch[1])}\")\n",
    "\n",
    "                print(\"\\nStarting MLflow run for continued training...\")\n",
    "                with mlflow.start_run(run_name=f\"continued_{selected_run.data.tags.get('mlflow.runName', 'unnamed')}\"):\n",
    "                    # Log parameters\n",
    "                    mlflow.log_params(params)\n",
    "                    mlflow.log_param(\"continued_epochs\", n_epochs.value)\n",
    "                    \n",
    "                    print(f\"\\nStarting training for {n_epochs.value} epochs...\")\n",
    "                    trained_model = train_model(model, train_loader, val_loader, params, n_epochs.value)\n",
    "                    print(\"Training completed.\")\n",
    "\n",
    "                    print(\"\\nEvaluating model on test set...\")\n",
    "                    test_precision, test_recall, test_f1, all_labels, all_preds = evaluate_model(trained_model, test_loader, params)\n",
    "                    print(f\"Test set performance - Precision: {test_precision:.4f}, Recall: {test_recall:.4f}, F1: {test_f1:.4f}\")\n",
    "\n",
    "                    # Log final metrics\n",
    "                    mlflow.log_metric(\"test_precision\", test_precision)\n",
    "                    mlflow.log_metric(\"test_recall\", test_recall)\n",
    "                    mlflow.log_metric(\"test_f1\", test_f1)\n",
    "\n",
    "                    # Confusion matrix\n",
    "                    cm = confusion_matrix(all_labels, all_preds)\n",
    "                    class_names = ['Class 0', 'Class 1']  # Adjust based on your classes\n",
    "                    cm_fig = plot_confusion_matrix(cm, class_names)\n",
    "                    cm_img_path = os.path.join(\"confusion_matrix.png\")\n",
    "                    cm_fig.savefig(cm_img_path)\n",
    "                    mlflow.log_artifact(cm_img_path)\n",
    "                    \n",
    "                    # Log the model\n",
    "                    print(\"Logging model...\")\n",
    "                    mlflow.pytorch.log_model(trained_model, \"continued_model\")\n",
    "                    print(\"Saved continued model in MLflow\")\n",
    "\n",
    "                print(\"\\nTraining and evaluation complete!\")\n",
    "                print(f\"Final test set performance:\")\n",
    "                print(f\"Precision: {test_precision:.4f}\")\n",
    "                print(f\"Recall: {test_recall:.4f}\")\n",
    "                print(f\"F1 Score: {test_f1:.4f}\")\n",
    "                print(f\"Model saved in MLflow with run name: continued_{selected_run.data.tags.get('mlflow.runName', 'unnamed')}\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"An error occurred in on_button_click: {str(e)}\")\n",
    "                print(\"Full traceback:\", e)\n",
    "\n",
    "        start_button.on_click(on_button_click)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred in main function: {str(e)}\")\n",
    "        print(\"Full traceback:\", e)\n",
    "\n",
    "# Assuming normalized_data and processed_labels are already defined in the notebook\n",
    "# Example call to the main function\n",
    "if __name__ == \"__main__\":\n",
    "    main(normalized_data, processed_labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
