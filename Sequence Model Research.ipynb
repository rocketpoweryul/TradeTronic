{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequence Model Research\n",
    "\n",
    "The scope of this notebook is to assess and train different sequence models given the training data generated.\n",
    "\n",
    "Training data is generated based on financial time series data labeled with potential profits using a buy-sell system.\n",
    "\n",
    "The goal is to create a sequence model that can choose favourable stock charts equal to or better than a human can via traditional technical analysis.\n",
    "\n",
    "# Import Libraries and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded sequences shape: (115000, 63, 12)\n",
      "Loaded sequences size: 86940000\n",
      "Loaded labels shape: (115000,)\n",
      "Loaded metadata shape: (115000, 2)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# Define the data directory relative to the script location\n",
    "data_dir = 'data'\n",
    "\n",
    "# Define the file paths\n",
    "sequences_path = os.path.join(data_dir, 'sequences.npy')\n",
    "labels_path = os.path.join(data_dir, 'labels.npy')\n",
    "metadata_path = os.path.join(data_dir, 'metadata.npy')\n",
    "\n",
    "# List of feature names\n",
    "feature_names = [\n",
    "    'Consol_Len_Bars', 'Consol_Depth_Percent',\n",
    "    'Distance_to_21EMA', 'Distance_to_50SMA', 'Distance_to_200SMA', \n",
    "    'RSL_NH_Count', 'RSL_Slope', 'Up_Down_Days', \n",
    "    'Stage 2', 'UpDownVolumeRatio', 'ATR', '%B'\n",
    "]\n",
    "\n",
    "# Load the data\n",
    "try:\n",
    "    data_sequences = np.load(sequences_path)\n",
    "    data_labels = np.load(labels_path)\n",
    "    data_metadata = np.load(metadata_path)\n",
    "\n",
    "    # Number of examples to select\n",
    "    num_examples = 115000\n",
    "\n",
    "    # Generate a random permutation of indices\n",
    "    indices = np.random.permutation(len(data_sequences))\n",
    "\n",
    "    # Select the first `num_examples` indices\n",
    "    selected_indices = indices[:num_examples]\n",
    "\n",
    "    # Use the selected indices to create the random subset\n",
    "    data_sequences = data_sequences[selected_indices, :, :]\n",
    "    data_labels = data_labels[selected_indices]\n",
    "    data_metadata = data_metadata[selected_indices]\n",
    "\n",
    "    # Inspect the shape and size of the loaded data before slicing\n",
    "    print(f'Loaded sequences shape: {data_sequences.shape}')\n",
    "    print(f'Loaded sequences size: {data_sequences.size}')\n",
    "    print(f'Loaded labels shape: {data_labels.shape}')\n",
    "    print(f'Loaded metadata shape: {data_metadata.shape}')\n",
    "\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Error loading files: {e}\")\n",
    "except ValueError as e:\n",
    "    print(f\"Value error: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "### NaN anf INF Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaNs in data_sequences: 900101\n",
      "Infs in data_sequences: 18\n",
      "NaNs remaining in data_sequences after removal: 0\n",
      "Infs remaining in data_sequences after removal: 0\n",
      "NaNs in data_labels: 0\n",
      "Infs in data_labels: 0\n",
      "NaN and Inf removal completed.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Dictionary to map variable names to their corresponding data arrays\n",
    "data_dict = {\n",
    "    'data_sequences': data_sequences,\n",
    "    'data_labels': data_labels,\n",
    "}\n",
    "\n",
    "# Using a dictionary to iterate over variables\n",
    "for var_name, data in data_dict.items():\n",
    "    num_nans = np.sum(np.isnan(data))\n",
    "    num_infs = np.sum(np.isinf(data))\n",
    "    print(f\"NaNs in {var_name}: {num_nans}\")\n",
    "    print(f\"Infs in {var_name}: {num_infs}\")\n",
    "\n",
    "    # Remove NaNs and Infs\n",
    "    if num_nans > 0 or num_infs > 0:\n",
    "        data_dict[var_name][:] = np.nan_to_num(data, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "        num_nans_after = np.sum(np.isnan(data))\n",
    "        num_infs_after = np.sum(np.isinf(data))\n",
    "        print(f\"NaNs remaining in {var_name} after removal: {num_nans_after}\")\n",
    "        print(f\"Infs remaining in {var_name} after removal: {num_infs_after}\")\n",
    "\n",
    "print(\"NaN and Inf removal completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Corrupted sequence removal\n",
    "\n",
    "99% of stocks I buy will be below 1000, with a few above 1000, although they are important.\n",
    "\n",
    "I also noticed quite a few training examples have weird price data, which I filter out below.\n",
    "\n",
    "I noticed with thresholds above 3e3, the max is the threshold, which is very suspect.\n",
    "\n",
    "The loss of training examples is insignificant, and the result is better normalization of the data and obviously no corrupted sequences.\n",
    "\n",
    "#### Feature Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Statistics:\n",
      "--------------------------------------------------\n",
      "Feature: Consol_Len_Bars\n",
      "  Mean: 69.4405\n",
      "  Median: 27.0000\n",
      "  Std Dev: 120.8940\n",
      "  Min: 0.0000\n",
      "  Max: 2897.0000\n",
      "  25th Percentile: 6.0000\n",
      "  75th Percentile: 77.0000\n",
      "  Skewness: 4.4165\n",
      "  Kurtosis: 35.8468\n",
      "  Zero Count: 1415968.0000\n",
      "  Zero Percentage: 19.5441\n",
      "--------------------------------------------------\n",
      "Feature: Consol_Depth_Percent\n",
      "  Mean: 16.4741\n",
      "  Median: 15.6812\n",
      "  Std Dev: 11.7435\n",
      "  Min: 0.0000\n",
      "  Max: 53.9655\n",
      "  25th Percentile: 7.3817\n",
      "  75th Percentile: 25.9440\n",
      "  Skewness: 0.2056\n",
      "  Kurtosis: -0.8862\n",
      "  Zero Count: 1340321.0000\n",
      "  Zero Percentage: 18.4999\n",
      "--------------------------------------------------\n",
      "Feature: Distance_to_21EMA\n",
      "  Mean: 0.8498\n",
      "  Median: 0.8908\n",
      "  Std Dev: 5.5161\n",
      "  Min: -99.9886\n",
      "  Max: 712.1856\n",
      "  25th Percentile: -1.2121\n",
      "  75th Percentile: 3.0189\n",
      "  Skewness: 0.9109\n",
      "  Kurtosis: 86.4163\n",
      "  Zero Count: 17683.0000\n",
      "  Zero Percentage: 0.2441\n",
      "--------------------------------------------------\n",
      "Feature: Distance_to_50SMA\n",
      "  Mean: 2.0705\n",
      "  Median: 1.9104\n",
      "  Std Dev: 10.0741\n",
      "  Min: -99.9896\n",
      "  Max: 1092.4742\n",
      "  25th Percentile: -1.5341\n",
      "  75th Percentile: 5.9316\n",
      "  Skewness: 1.5815\n",
      "  Kurtosis: 73.0291\n",
      "  Zero Count: 117898.0000\n",
      "  Zero Percentage: 1.6273\n",
      "--------------------------------------------------\n",
      "Feature: Distance_to_200SMA\n",
      "  Mean: 5.5968\n",
      "  Median: 5.5150\n",
      "  Std Dev: 21.2709\n",
      "  Min: -98.0358\n",
      "  Max: 2523.0145\n",
      "  25th Percentile: -0.1450\n",
      "  75th Percentile: 13.8736\n",
      "  Skewness: 2.2983\n",
      "  Kurtosis: 115.0040\n",
      "  Zero Count: 533902.0000\n",
      "  Zero Percentage: 7.3692\n",
      "--------------------------------------------------\n",
      "Feature: RSL_NH_Count\n",
      "  Mean: 116.2344\n",
      "  Median: 95.0000\n",
      "  Std Dev: 99.3099\n",
      "  Min: 0.0000\n",
      "  Max: 1253.0000\n",
      "  25th Percentile: 32.0000\n",
      "  75th Percentile: 180.0000\n",
      "  Skewness: 1.0268\n",
      "  Kurtosis: 1.8398\n",
      "  Zero Count: 472689.0000\n",
      "  Zero Percentage: 6.5243\n",
      "--------------------------------------------------\n",
      "Feature: RSL_Slope\n",
      "  Mean: 0.0000\n",
      "  Median: 0.0000\n",
      "  Std Dev: 0.0004\n",
      "  Min: -0.1085\n",
      "  Max: 0.0804\n",
      "  25th Percentile: -0.0000\n",
      "  75th Percentile: 0.0000\n",
      "  Skewness: -8.7721\n",
      "  Kurtosis: 6162.9139\n",
      "  Zero Count: 126490.0000\n",
      "  Zero Percentage: 1.7459\n",
      "--------------------------------------------------\n",
      "Feature: Up_Down_Days\n",
      "  Mean: 0.7157\n",
      "  Median: 0.0000\n",
      "  Std Dev: 3.4861\n",
      "  Min: -14.0000\n",
      "  Max: 14.0000\n",
      "  25th Percentile: -2.0000\n",
      "  75th Percentile: 3.0000\n",
      "  Skewness: 0.0043\n",
      "  Kurtosis: -0.0726\n",
      "  Zero Count: 1333529.0000\n",
      "  Zero Percentage: 18.4062\n",
      "--------------------------------------------------\n",
      "Feature: Stage 2\n",
      "  Mean: 0.3490\n",
      "  Median: 0.0000\n",
      "  Std Dev: 0.4767\n",
      "  Min: 0.0000\n",
      "  Max: 1.0000\n",
      "  25th Percentile: 0.0000\n",
      "  75th Percentile: 1.0000\n",
      "  Skewness: 0.6334\n",
      "  Kurtosis: -1.5989\n",
      "  Zero Count: 4716143.0000\n",
      "  Zero Percentage: 65.0951\n",
      "--------------------------------------------------\n",
      "Feature: UpDownVolumeRatio\n",
      "  Mean: 1.2793\n",
      "  Median: 1.1214\n",
      "  Std Dev: 8.7891\n",
      "  Min: 0.0000\n",
      "  Max: 9897.7948\n",
      "  25th Percentile: 0.8672\n",
      "  75th Percentile: 1.4424\n",
      "  Skewness: 831.6372\n",
      "  Kurtosis: 849762.7940\n",
      "  Zero Count: 117908.0000\n",
      "  Zero Percentage: 1.6274\n",
      "--------------------------------------------------\n",
      "Feature: ATR\n",
      "  Mean: 1.1334\n",
      "  Median: 0.4662\n",
      "  Std Dev: 3.3200\n",
      "  Min: 0.0000\n",
      "  Max: 311.1429\n",
      "  25th Percentile: 0.2198\n",
      "  75th Percentile: 0.9931\n",
      "  Skewness: 16.5001\n",
      "  Kurtosis: 506.5689\n",
      "  Zero Count: 35373.0000\n",
      "  Zero Percentage: 0.4882\n",
      "--------------------------------------------------\n",
      "Feature: %B\n",
      "  Mean: 0.5695\n",
      "  Median: 0.6076\n",
      "  Std Dev: 0.3137\n",
      "  Min: -0.5621\n",
      "  Max: 1.5611\n",
      "  25th Percentile: 0.3295\n",
      "  75th Percentile: 0.8153\n",
      "  Skewness: -0.2716\n",
      "  Kurtosis: -0.6329\n",
      "  Zero Count: 46736.0000\n",
      "  Zero Percentage: 0.6451\n",
      "--------------------------------------------------\n",
      "Overall Dataset Statistics:\n",
      "Total number of sequences: 115000\n",
      "Sequence length: 63\n",
      "Number of features: 12\n",
      "Total number of data points: 86940000\n",
      "Memory usage: 663.30 MB\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def print_feature_stats(data_sequences, feature_names):\n",
    "    print(\"Feature Statistics:\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    for i, feature_name in enumerate(feature_names):\n",
    "        feature_data = data_sequences[:, :, i].flatten()\n",
    "        \n",
    "        stats = {\n",
    "            \"Mean\": np.mean(feature_data),\n",
    "            \"Median\": np.median(feature_data),\n",
    "            \"Std Dev\": np.std(feature_data),\n",
    "            \"Min\": np.min(feature_data),\n",
    "            \"Max\": np.max(feature_data),\n",
    "            \"25th Percentile\": np.percentile(feature_data, 25),\n",
    "            \"75th Percentile\": np.percentile(feature_data, 75),\n",
    "            \"Skewness\": pd.Series(feature_data).skew(),\n",
    "            \"Kurtosis\": pd.Series(feature_data).kurtosis(),\n",
    "            \"Zero Count\": np.sum(feature_data == 0),\n",
    "            \"Zero Percentage\": np.mean(feature_data == 0) * 100\n",
    "        }\n",
    "        \n",
    "        print(f\"Feature: {feature_name}\")\n",
    "        for stat_name, stat_value in stats.items():\n",
    "            print(f\"  {stat_name}: {stat_value:.4f}\")\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "# Call the function to print statistics\n",
    "print_feature_stats(data_sequences, feature_names)\n",
    "\n",
    "# Additional overall statistics\n",
    "print(\"Overall Dataset Statistics:\")\n",
    "print(f\"Total number of sequences: {data_sequences.shape[0]}\")\n",
    "print(f\"Sequence length: {data_sequences.shape[1]}\")\n",
    "print(f\"Number of features: {data_sequences.shape[2]}\")\n",
    "print(f\"Total number of data points: {data_sequences.size}\")\n",
    "print(f\"Memory usage: {data_sequences.nbytes / (1024 * 1024):.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization of Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Define the indices based on the provided feature names\n",
    "feature_names = [\n",
    "    'Consol_Len_Bars', 'Consol_Depth_Percent',\n",
    "    'Distance_to_21EMA', 'Distance_to_50SMA', 'Distance_to_200SMA', \n",
    "    'RSL_NH_Count', 'RSL_Slope', 'Up_Down_Days', \n",
    "    'Stage 2', 'UpDownVolumeRatio', 'ATR', '%B'\n",
    "]\n",
    "\n",
    "feature_indices = {name: idx for idx, name in enumerate(feature_names)}\n",
    "\n",
    "# Function to remove outliers and cap values\n",
    "def preprocess_data(sequences, labels):\n",
    "    # Reshape sequences to 2D array for easier processing (flatten the timesteps)\n",
    "    num_sequences, num_timesteps, num_features = sequences.shape\n",
    "    sequences_reshaped = sequences.reshape(-1, num_features)\n",
    "    \n",
    "    # Create a mask to filter out invalid sequences\n",
    "    valid_mask = (\n",
    "        (sequences_reshaped[:, feature_indices['Distance_to_21EMA']] <= 100) &\n",
    "        (sequences_reshaped[:, feature_indices['Distance_to_50SMA']] <= 200) &\n",
    "        (sequences_reshaped[:, feature_indices['Distance_to_200SMA']] <= 500)\n",
    "    )\n",
    "    \n",
    "    # Reshape the valid_mask to match the original sequence shape\n",
    "    valid_mask_reshaped = valid_mask.reshape(num_sequences, num_timesteps)\n",
    "    \n",
    "    # Filter out sequences with any invalid timesteps\n",
    "    valid_sequences_mask = valid_mask_reshaped.all(axis=1)\n",
    "    filtered_sequences = sequences[valid_sequences_mask]\n",
    "    filtered_labels = labels[valid_sequences_mask]\n",
    "    \n",
    "    # Cap 'UpDownVolumeRatio' at 10\n",
    "    filtered_sequences[:, :, feature_indices['UpDownVolumeRatio']] = np.minimum(\n",
    "        filtered_sequences[:, :, feature_indices['UpDownVolumeRatio']], 10\n",
    "    )\n",
    "    \n",
    "    # Normalize the features using Z-score normalization\n",
    "    scaler = StandardScaler()\n",
    "    \n",
    "    # Flatten the sequence again for normalization\n",
    "    filtered_sequences_reshaped = filtered_sequences.reshape(-1, num_features)\n",
    "    \n",
    "    # Normalize\n",
    "    normalized_data_reshaped = scaler.fit_transform(filtered_sequences_reshaped)\n",
    "    \n",
    "    # Reshape back to the original 3D shape\n",
    "    normalized_data = normalized_data_reshaped.reshape(filtered_sequences.shape)\n",
    "    \n",
    "    return normalized_data, filtered_labels\n",
    "\n",
    "# Function to print feature statistics\n",
    "def print_feature_stats(data_sequences, feature_names):\n",
    "    print(\"Feature Statistics:\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    for i, feature_name in enumerate(feature_names):\n",
    "        feature_data = data_sequences[:, :, i].flatten()\n",
    "        \n",
    "        stats = {\n",
    "            \"Mean\": np.mean(feature_data),\n",
    "            \"Median\": np.median(feature_data),\n",
    "            \"Std Dev\": np.std(feature_data),\n",
    "            \"Min\": np.min(feature_data),\n",
    "            \"Max\": np.max(feature_data),\n",
    "            \"25th Percentile\": np.percentile(feature_data, 25),\n",
    "            \"75th Percentile\": np.percentile(feature_data, 75),\n",
    "            \"Skewness\": pd.Series(feature_data).skew(),\n",
    "            \"Kurtosis\": pd.Series(feature_data).kurtosis(),\n",
    "            \"Zero Count\": np.sum(feature_data == 0),\n",
    "            \"Zero Percentage\": np.mean(feature_data == 0) * 100\n",
    "        }\n",
    "        \n",
    "        print(f\"Feature: {feature_name}\")\n",
    "        for stat_name, stat_value in stats.items():\n",
    "            print(f\"  {stat_name}: {stat_value:.4f}\")\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "# Example usage with data_sequences and data_labels\n",
    "# Assuming data_sequences is loaded and has shape (115000, 63, 12)\n",
    "\n",
    "# Process the data\n",
    "normalized_data, processed_labels = preprocess_data(data_sequences, data_labels)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stats again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Statistics:\n",
      "--------------------------------------------------\n",
      "Feature: Consol_Len_Bars\n",
      "  Mean: 0.0000\n",
      "  Median: -0.3513\n",
      "  Std Dev: 1.0000\n",
      "  Min: -0.5746\n",
      "  Max: 23.3827\n",
      "  25th Percentile: -0.5250\n",
      "  75th Percentile: 0.0622\n",
      "  Skewness: 4.4153\n",
      "  Kurtosis: 35.8290\n",
      "  Zero Count: 0.0000\n",
      "  Zero Percentage: 0.0000\n",
      "--------------------------------------------------\n",
      "Feature: Consol_Depth_Percent\n",
      "  Mean: 0.0000\n",
      "  Median: -0.0676\n",
      "  Std Dev: 1.0000\n",
      "  Min: -1.4034\n",
      "  Max: 3.1930\n",
      "  25th Percentile: -0.7739\n",
      "  75th Percentile: 0.8060\n",
      "  Skewness: 0.2058\n",
      "  Kurtosis: -0.8852\n",
      "  Zero Count: 0.0000\n",
      "  Zero Percentage: 0.0000\n",
      "--------------------------------------------------\n",
      "Feature: Distance_to_21EMA\n",
      "  Mean: -0.0000\n",
      "  Median: 0.0093\n",
      "  Std Dev: 1.0000\n",
      "  Min: -18.5754\n",
      "  Max: 18.1148\n",
      "  25th Percentile: -0.3780\n",
      "  75th Percentile: 0.4009\n",
      "  Skewness: -0.2015\n",
      "  Kurtosis: 14.1964\n",
      "  Zero Count: 0.0000\n",
      "  Zero Percentage: 0.0000\n",
      "--------------------------------------------------\n",
      "Feature: Distance_to_50SMA\n",
      "  Mean: -0.0000\n",
      "  Median: -0.0138\n",
      "  Std Dev: 1.0000\n",
      "  Min: -10.3410\n",
      "  Max: 19.2520\n",
      "  25th Percentile: -0.3628\n",
      "  75th Percentile: 0.3933\n",
      "  Skewness: 0.3884\n",
      "  Kurtosis: 10.9629\n",
      "  Zero Count: 0.0000\n",
      "  Zero Percentage: 0.0000\n",
      "--------------------------------------------------\n",
      "Feature: Distance_to_200SMA\n",
      "  Mean: 0.0000\n",
      "  Median: -0.0016\n",
      "  Std Dev: 1.0000\n",
      "  Min: -4.9770\n",
      "  Max: 23.3212\n",
      "  25th Percentile: -0.2735\n",
      "  75th Percentile: 0.3996\n",
      "  Skewness: 0.7729\n",
      "  Kurtosis: 9.5185\n",
      "  Zero Count: 0.0000\n",
      "  Zero Percentage: 0.0000\n",
      "--------------------------------------------------\n",
      "Feature: RSL_NH_Count\n",
      "  Mean: -0.0000\n",
      "  Median: -0.2143\n",
      "  Std Dev: 1.0000\n",
      "  Min: -1.1708\n",
      "  Max: 11.4455\n",
      "  25th Percentile: -0.8486\n",
      "  75th Percentile: 0.6416\n",
      "  Skewness: 1.0264\n",
      "  Kurtosis: 1.8394\n",
      "  Zero Count: 0.0000\n",
      "  Zero Percentage: 0.0000\n",
      "--------------------------------------------------\n",
      "Feature: RSL_Slope\n",
      "  Mean: 0.0000\n",
      "  Median: -0.0167\n",
      "  Std Dev: 1.0000\n",
      "  Min: -291.0320\n",
      "  Max: 215.6553\n",
      "  25th Percentile: -0.0634\n",
      "  75th Percentile: 0.0491\n",
      "  Skewness: -9.1224\n",
      "  Kurtosis: 6238.9156\n",
      "  Zero Count: 0.0000\n",
      "  Zero Percentage: 0.0000\n",
      "--------------------------------------------------\n",
      "Feature: Up_Down_Days\n",
      "  Mean: 0.0000\n",
      "  Median: -0.2054\n",
      "  Std Dev: 1.0000\n",
      "  Min: -4.2215\n",
      "  Max: 3.8106\n",
      "  25th Percentile: -0.7791\n",
      "  75th Percentile: 0.6552\n",
      "  Skewness: 0.0043\n",
      "  Kurtosis: -0.0727\n",
      "  Zero Count: 0.0000\n",
      "  Zero Percentage: 0.0000\n",
      "--------------------------------------------------\n",
      "Feature: Stage 2\n",
      "  Mean: -0.0000\n",
      "  Median: -0.7324\n",
      "  Std Dev: 1.0000\n",
      "  Min: -0.7324\n",
      "  Max: 1.3654\n",
      "  25th Percentile: -0.7324\n",
      "  75th Percentile: 1.3654\n",
      "  Skewness: 0.6330\n",
      "  Kurtosis: -1.5993\n",
      "  Zero Count: 0.0000\n",
      "  Zero Percentage: 0.0000\n",
      "--------------------------------------------------\n",
      "Feature: UpDownVolumeRatio\n",
      "  Mean: -0.0000\n",
      "  Median: -0.1564\n",
      "  Std Dev: 1.0000\n",
      "  Min: -1.6637\n",
      "  Max: 11.7793\n",
      "  25th Percentile: -0.4979\n",
      "  75th Percentile: 0.2746\n",
      "  Skewness: 4.9761\n",
      "  Kurtosis: 45.5950\n",
      "  Zero Count: 0.0000\n",
      "  Zero Percentage: 0.0000\n",
      "--------------------------------------------------\n",
      "Feature: ATR\n",
      "  Mean: 0.0000\n",
      "  Median: -0.2009\n",
      "  Std Dev: 1.0000\n",
      "  Min: -0.3416\n",
      "  Max: 93.5410\n",
      "  25th Percentile: -0.2752\n",
      "  75th Percentile: -0.0420\n",
      "  Skewness: 16.5316\n",
      "  Kurtosis: 509.0354\n",
      "  Zero Count: 0.0000\n",
      "  Zero Percentage: 0.0000\n",
      "--------------------------------------------------\n",
      "Feature: %B\n",
      "  Mean: 0.0000\n",
      "  Median: 0.1215\n",
      "  Std Dev: 1.0000\n",
      "  Min: -3.6078\n",
      "  Max: 3.1602\n",
      "  25th Percentile: -0.7653\n",
      "  75th Percentile: 0.7835\n",
      "  Skewness: -0.2719\n",
      "  Kurtosis: -0.6333\n",
      "  Zero Count: 0.0000\n",
      "  Zero Percentage: 0.0000\n",
      "--------------------------------------------------\n",
      "Overall Dataset Statistics:\n",
      "Total number of sequences: 115000\n",
      "Sequence length: 63\n",
      "Number of features: 12\n",
      "Total number of data points: 86940000\n",
      "Memory usage: 663.30 MB\n"
     ]
    }
   ],
   "source": [
    "# Print statistics\n",
    "print_feature_stats(normalized_data, feature_names)\n",
    "\n",
    "# Additional overall statistics\n",
    "print(\"Overall Dataset Statistics:\")\n",
    "print(f\"Total number of sequences: {data_sequences.shape[0]}\")\n",
    "print(f\"Sequence length: {data_sequences.shape[1]}\")\n",
    "print(f\"Number of features: {data_sequences.shape[2]}\")\n",
    "print(f\"Total number of data points: {data_sequences.size}\")\n",
    "print(f\"Memory usage: {data_sequences.nbytes / (1024 * 1024):.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Definition\n",
    "\n",
    "## Model Class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HP Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-29 11:46:09,953] A new study created in memory with name: no-name-cdcfd984-845e-4c6c-a9e7-907f30279b77\n",
      "[W 2024-06-29 11:47:30,419] Trial 0 failed with parameters: {'PROFIT_THRESH': 0.06750044767340001, 'LEARNING_RATE': 1.1876857527331063e-05, 'DROPOUT_RATE_LSTM': 0.4044669654245465, 'DROPOUT_RATE_LINEAR': 0.1313808528251971, 'L2_REGULARIZATION': 2.014550497045114e-06, 'PREDICTION_THRESHOLD': 0.7383524590106062, 'LSTM_LAYERS': '128,64', 'LINEAR_LAYERS': '64', 'BATCH_SIZE': 256, 'SKIP_CONNECTIONS': True} because of the following error: KeyboardInterrupt().\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\admin\\GitHub\\TradeTronic\\.venv\\lib\\site-packages\\optuna\\study\\_optimize.py\", line 196, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_16456\\912392297.py\", line 172, in objective\n",
      "    for batch_X, batch_y in train_loader:\n",
      "  File \"c:\\Users\\admin\\GitHub\\TradeTronic\\.venv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\", line 631, in __next__\n",
      "    data = self._next_data()\n",
      "  File \"c:\\Users\\admin\\GitHub\\TradeTronic\\.venv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\", line 675, in _next_data\n",
      "    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration\n",
      "  File \"c:\\Users\\admin\\GitHub\\TradeTronic\\.venv\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\", line 54, in fetch\n",
      "    return self.collate_fn(data)\n",
      "  File \"c:\\Users\\admin\\GitHub\\TradeTronic\\.venv\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py\", line 316, in default_collate\n",
      "    return collate(batch, collate_fn_map=default_collate_fn_map)\n",
      "  File \"c:\\Users\\admin\\GitHub\\TradeTronic\\.venv\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py\", line 173, in collate\n",
      "    return [collate(samples, collate_fn_map=collate_fn_map) for samples in transposed]  # Backwards compatibility.\n",
      "  File \"c:\\Users\\admin\\GitHub\\TradeTronic\\.venv\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py\", line 173, in <listcomp>\n",
      "    return [collate(samples, collate_fn_map=collate_fn_map) for samples in transposed]  # Backwards compatibility.\n",
      "  File \"c:\\Users\\admin\\GitHub\\TradeTronic\\.venv\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py\", line 141, in collate\n",
      "    return collate_fn_map[elem_type](batch, collate_fn_map=collate_fn_map)\n",
      "  File \"c:\\Users\\admin\\GitHub\\TradeTronic\\.venv\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py\", line 213, in collate_tensor_fn\n",
      "    return torch.stack(batch, 0, out=out)\n",
      "KeyboardInterrupt\n",
      "[W 2024-06-29 11:47:30,420] Trial 0 failed with value None.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 296\u001b[0m\n\u001b[0;32m    294\u001b[0m \u001b[38;5;66;03m# Run Optuna optimization\u001b[39;00m\n\u001b[0;32m    295\u001b[0m study \u001b[38;5;241m=\u001b[39m optuna\u001b[38;5;241m.\u001b[39mcreate_study(direction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmaximize\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 296\u001b[0m \u001b[43mstudy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobjective\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    298\u001b[0m \u001b[38;5;66;03m# Print and log the best parameters and score\u001b[39;00m\n\u001b[0;32m    299\u001b[0m best_params \u001b[38;5;241m=\u001b[39m study\u001b[38;5;241m.\u001b[39mbest_params\n",
      "File \u001b[1;32mc:\\Users\\admin\\GitHub\\TradeTronic\\.venv\\lib\\site-packages\\optuna\\study\\study.py:451\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[1;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m    348\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21moptimize\u001b[39m(\n\u001b[0;32m    349\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    350\u001b[0m     func: ObjectiveFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    357\u001b[0m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    358\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    359\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[0;32m    360\u001b[0m \n\u001b[0;32m    361\u001b[0m \u001b[38;5;124;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    449\u001b[0m \u001b[38;5;124;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[0;32m    450\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 451\u001b[0m     \u001b[43m_optimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    452\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstudy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    453\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    454\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    455\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    456\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    457\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    458\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    459\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    460\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    461\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\admin\\GitHub\\TradeTronic\\.venv\\lib\\site-packages\\optuna\\study\\_optimize.py:62\u001b[0m, in \u001b[0;36m_optimize\u001b[1;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     61\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m---> 62\u001b[0m         \u001b[43m_optimize_sequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     63\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     64\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     65\u001b[0m \u001b[43m            \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     66\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     67\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     68\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     69\u001b[0m \u001b[43m            \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     70\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreseed_sampler_rng\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     71\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     72\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     73\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     74\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     75\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\admin\\GitHub\\TradeTronic\\.venv\\lib\\site-packages\\optuna\\study\\_optimize.py:159\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[1;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[0;32m    156\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m    158\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 159\u001b[0m     frozen_trial \u001b[38;5;241m=\u001b[39m \u001b[43m_run_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    160\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    161\u001b[0m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[0;32m    162\u001b[0m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[0;32m    163\u001b[0m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[0;32m    164\u001b[0m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[0;32m    165\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[1;32mc:\\Users\\admin\\GitHub\\TradeTronic\\.venv\\lib\\site-packages\\optuna\\study\\_optimize.py:247\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    240\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShould not reach.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    242\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    243\u001b[0m     frozen_trial\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m==\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mFAIL\n\u001b[0;32m    244\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    245\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[0;32m    246\u001b[0m ):\n\u001b[1;32m--> 247\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[0;32m    248\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m frozen_trial\n",
      "File \u001b[1;32mc:\\Users\\admin\\GitHub\\TradeTronic\\.venv\\lib\\site-packages\\optuna\\study\\_optimize.py:196\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    194\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[38;5;241m.\u001b[39m_trial_id, study\u001b[38;5;241m.\u001b[39m_storage):\n\u001b[0;32m    195\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 196\u001b[0m         value_or_values \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    197\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mTrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    198\u001b[0m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[0;32m    199\u001b[0m         state \u001b[38;5;241m=\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mPRUNED\n",
      "Cell \u001b[1;32mIn[6], line 172\u001b[0m, in \u001b[0;36mobjective\u001b[1;34m(trial)\u001b[0m\n\u001b[0;32m    170\u001b[0m all_train_preds \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    171\u001b[0m all_train_labels \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m--> 172\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_X, batch_y \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[0;32m    173\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m    174\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\admin\\GitHub\\TradeTronic\\.venv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\admin\\GitHub\\TradeTronic\\.venv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\admin\\GitHub\\TradeTronic\\.venv\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:54\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[1;32m---> 54\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\admin\\GitHub\\TradeTronic\\.venv\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:316\u001b[0m, in \u001b[0;36mdefault_collate\u001b[1;34m(batch)\u001b[0m\n\u001b[0;32m    255\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdefault_collate\u001b[39m(batch):\n\u001b[0;32m    256\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    257\u001b[0m \u001b[38;5;124;03m    Take in a batch of data and put the elements within the batch into a tensor with an additional outer dimension - batch size.\u001b[39;00m\n\u001b[0;32m    258\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    314\u001b[0m \u001b[38;5;124;03m        >>> default_collate(batch)  # Handle `CustomType` automatically\u001b[39;00m\n\u001b[0;32m    315\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 316\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefault_collate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\admin\\GitHub\\TradeTronic\\.venv\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:173\u001b[0m, in \u001b[0;36mcollate\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    170\u001b[0m transposed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch))  \u001b[38;5;66;03m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[0;32m    172\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m--> 173\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [collate(samples, collate_fn_map\u001b[38;5;241m=\u001b[39mcollate_fn_map) \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed]  \u001b[38;5;66;03m# Backwards compatibility.\u001b[39;00m\n\u001b[0;32m    174\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    175\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\admin\\GitHub\\TradeTronic\\.venv\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:173\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    170\u001b[0m transposed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch))  \u001b[38;5;66;03m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[0;32m    172\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m--> 173\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msamples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed]  \u001b[38;5;66;03m# Backwards compatibility.\u001b[39;00m\n\u001b[0;32m    174\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    175\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\admin\\GitHub\\TradeTronic\\.venv\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:141\u001b[0m, in \u001b[0;36mcollate\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    139\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m collate_fn_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    140\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m elem_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[1;32m--> 141\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate_fn_map\u001b[49m\u001b[43m[\u001b[49m\u001b[43melem_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    143\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m collate_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[0;32m    144\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, collate_type):\n",
      "File \u001b[1;32mc:\\Users\\admin\\GitHub\\TradeTronic\\.venv\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:213\u001b[0m, in \u001b[0;36mcollate_tensor_fn\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    211\u001b[0m     storage \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39m_typed_storage()\u001b[38;5;241m.\u001b[39m_new_shared(numel, device\u001b[38;5;241m=\u001b[39melem\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m    212\u001b[0m     out \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39mnew(storage)\u001b[38;5;241m.\u001b[39mresize_(\u001b[38;5;28mlen\u001b[39m(batch), \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mlist\u001b[39m(elem\u001b[38;5;241m.\u001b[39msize()))\n\u001b[1;32m--> 213\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score\n",
    "import seaborn as sns\n",
    "import copy\n",
    "import time\n",
    "import optuna\n",
    "import json\n",
    "import logging\n",
    "import mlflow\n",
    "import mlflow.pytorch\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "RANDOM_SEED = 42\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "# Check for CUDA availability\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "logger.info(f\"Using device: {device}\")\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    normalized_data, processed_labels, test_size=0.2, random_state=RANDOM_SEED\n",
    ")\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train_tensor = torch.FloatTensor(X_train).to(device)\n",
    "y_train_tensor = torch.FloatTensor(y_train).to(device)\n",
    "X_test_tensor = torch.FloatTensor(X_test).to(device)\n",
    "y_test_tensor = torch.FloatTensor(y_test).to(device)\n",
    "\n",
    "# Define the model architecture (updated)\n",
    "class StockAnalysisModel(nn.Module):\n",
    "    def __init__(self, input_size, lstm_layers, linear_layers, dropout_rate_lstm, dropout_rate_linear, skip_connections):\n",
    "        super(StockAnalysisModel, self).__init__()\n",
    "        self.lstm_layers = nn.ModuleList()\n",
    "        self.linear_layers = nn.ModuleList()\n",
    "        self.skip_connections = skip_connections\n",
    "        \n",
    "        # LSTM layers\n",
    "        lstm_sizes = [int(size) for size in lstm_layers.split(',')]\n",
    "        for i, lstm_size in enumerate(lstm_sizes):\n",
    "            if i == 0:\n",
    "                self.lstm_layers.append(nn.LSTM(input_size, lstm_size, batch_first=True))\n",
    "            else:\n",
    "                self.lstm_layers.append(nn.LSTM(lstm_sizes[i-1], lstm_size, batch_first=True))\n",
    "        \n",
    "        # Linear layers\n",
    "        linear_sizes = [int(size) for size in linear_layers.split(',')]\n",
    "        input_size = lstm_sizes[-1]  # Use the last LSTM layer's output size as input to the first linear layer\n",
    "        for i, linear_size in enumerate(linear_sizes):\n",
    "            self.linear_layers.append(nn.Linear(input_size, linear_size))\n",
    "            input_size = linear_size  # Update input_size for the next layer\n",
    "        \n",
    "        self.final_layer = nn.Linear(linear_sizes[-1], 1)\n",
    "        self.dropout_lstm = nn.Dropout(dropout_rate_lstm)\n",
    "        self.dropout_linear = nn.Dropout(dropout_rate_linear)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Flatten parameters for all LSTM layers\n",
    "        for lstm in self.lstm_layers:\n",
    "            lstm.flatten_parameters()\n",
    "\n",
    "        # LSTM layers\n",
    "        lstm_out = x\n",
    "        for lstm_layer in self.lstm_layers:\n",
    "            lstm_out, _ = lstm_layer(lstm_out)\n",
    "            lstm_out = self.dropout_lstm(lstm_out)\n",
    "        \n",
    "        # Take the last output of the LSTM\n",
    "        linear_out = lstm_out[:, -1, :]\n",
    "        \n",
    "        # Linear layers\n",
    "        for i, linear_layer in enumerate(self.linear_layers):\n",
    "            new_linear_out = F.relu(linear_layer(linear_out))\n",
    "            if self.skip_connections and i > 0 and new_linear_out.shape == linear_out.shape:\n",
    "                linear_out = new_linear_out + linear_out\n",
    "            else:\n",
    "                linear_out = new_linear_out\n",
    "            linear_out = self.dropout_linear(linear_out)\n",
    "        \n",
    "        # Final layer\n",
    "        output = torch.sigmoid(self.final_layer(linear_out))\n",
    "        return output.squeeze()\n",
    "\n",
    "def objective(trial):\n",
    "    # Hyperparameter search space\n",
    "    params = {\n",
    "        'PROFIT_THRESH': trial.suggest_float('PROFIT_THRESH', 0.0, 0.1),\n",
    "        'LEARNING_RATE': trial.suggest_float('LEARNING_RATE', 1e-5, 1e-2, log=True),\n",
    "        'DROPOUT_RATE_LSTM': trial.suggest_float('DROPOUT_RATE_LSTM', 0.1, 0.5),\n",
    "        'DROPOUT_RATE_LINEAR': trial.suggest_float('DROPOUT_RATE_LINEAR', 0.1, 0.5),\n",
    "        'L2_REGULARIZATION': trial.suggest_float('L2_REGULARIZATION', 1e-6, 1e-3, log=True),\n",
    "        'PREDICTION_THRESHOLD': trial.suggest_float('PREDICTION_THRESHOLD', 0.1, 0.9),\n",
    "        'LSTM_LAYERS': trial.suggest_categorical('LSTM_LAYERS', ['64', '128', '64,32', '128,64', '128,64,32']),\n",
    "        'LINEAR_LAYERS': trial.suggest_categorical('LINEAR_LAYERS', ['32', '64', '32,16', '64,32', '64,32,16']),\n",
    "        'BATCH_SIZE': trial.suggest_categorical('BATCH_SIZE', [32, 64, 128, 256]),\n",
    "        'SKIP_CONNECTIONS': trial.suggest_categorical('SKIP_CONNECTIONS', [False, True])\n",
    "    }\n",
    "    \n",
    "    # Set up MLflow\n",
    "    mlflow.set_experiment(\"Stock Analysis Model Tuning\")\n",
    "    with mlflow.start_run():\n",
    "        # Log hyperparameters\n",
    "        mlflow.log_params(params)\n",
    "        \n",
    "        # Prepare data\n",
    "        y_train_binary = (y_train > params['PROFIT_THRESH']).astype(int)\n",
    "        y_test_binary = (y_test > params['PROFIT_THRESH']).astype(int)\n",
    "        \n",
    "        # Log class distribution\n",
    "        train_class_distribution = np.bincount(y_train_binary)\n",
    "        test_class_distribution = np.bincount(y_test_binary)\n",
    "        logger.info(f\"Train class distribution: {train_class_distribution}\")\n",
    "        logger.info(f\"Test class distribution: {test_class_distribution}\")\n",
    "        train_positive_class_percentage = train_class_distribution[1] / len(y_train_binary) * 100\n",
    "        test_positive_class_percentage = test_class_distribution[1] / len(y_test_binary) * 100\n",
    "        mlflow.log_metric(\"train_positive_class_percentage\", train_positive_class_percentage)\n",
    "        mlflow.log_metric(\"test_positive_class_percentage\", test_positive_class_percentage)\n",
    "        logger.info(f\"Train positive class percentage: {train_positive_class_percentage:.2f}%\")\n",
    "        logger.info(f\"Test positive class percentage: {test_positive_class_percentage:.2f}%\")\n",
    "        \n",
    "        # Compute class weights\n",
    "        class_weights = compute_class_weight('balanced', classes=np.unique(y_train_binary), y=y_train_binary)\n",
    "        class_weights = torch.FloatTensor(class_weights).to(device)\n",
    "        logger.info(f\"Class weights: {class_weights}\")\n",
    "        \n",
    "        train_dataset = TensorDataset(X_train_tensor, torch.FloatTensor(y_train_binary).to(device))\n",
    "        train_loader = DataLoader(train_dataset, batch_size=params['BATCH_SIZE'], shuffle=True)\n",
    "        \n",
    "        # Initialize model\n",
    "        model = StockAnalysisModel(\n",
    "            input_size=X_train.shape[2],\n",
    "            lstm_layers=params['LSTM_LAYERS'],\n",
    "            linear_layers=params['LINEAR_LAYERS'],\n",
    "            dropout_rate_lstm=params['DROPOUT_RATE_LSTM'],\n",
    "            dropout_rate_linear=params['DROPOUT_RATE_LINEAR'],\n",
    "            skip_connections=params['SKIP_CONNECTIONS']\n",
    "        ).to(device)\n",
    "        logger.info(f\"Model architecture: {model}\")\n",
    "        \n",
    "        # Define loss function and optimizer\n",
    "        criterion = nn.BCELoss(reduction='none')\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=params['LEARNING_RATE'], weight_decay=params['L2_REGULARIZATION'])\n",
    "        \n",
    "        # Training loop\n",
    "        n_epochs = 50\n",
    "        best_f1 = 0\n",
    "        best_model = None\n",
    "        for epoch in range(n_epochs):\n",
    "            model.train()\n",
    "            total_loss = 0\n",
    "            all_train_preds = []\n",
    "            all_train_labels = []\n",
    "            for batch_X, batch_y in train_loader:\n",
    "                optimizer.zero_grad()\n",
    "                try:\n",
    "                    outputs = model(batch_X)\n",
    "                    # Apply class weights manually\n",
    "                    weights = class_weights[batch_y.long()]\n",
    "                    losses = criterion(outputs, batch_y)\n",
    "                    weighted_losses = losses * weights\n",
    "                    loss = weighted_losses.mean()\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    total_loss += loss.item()\n",
    "                    \n",
    "                    all_train_preds.extend(outputs.detach().cpu().numpy())\n",
    "                    all_train_labels.extend(batch_y.cpu().numpy())\n",
    "                except RuntimeError as e:\n",
    "                    logger.error(f\"Error in forward pass: {str(e)}\")\n",
    "                    logger.error(f\"batch_X shape: {batch_X.shape}, batch_y shape: {batch_y.shape}\")\n",
    "                    return 0  # Return 0 to indicate failure to Optuna\n",
    "            \n",
    "            # Calculate training metrics\n",
    "            train_preds_binary = (np.array(all_train_preds) > params['PREDICTION_THRESHOLD']).astype(int)\n",
    "            train_precision = precision_score(all_train_labels, train_preds_binary, zero_division=0)\n",
    "            train_recall = recall_score(all_train_labels, train_preds_binary, zero_division=0)\n",
    "            train_f1 = f1_score(all_train_labels, train_preds_binary, zero_division=0)\n",
    "            \n",
    "            # Validation\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                y_pred = model(X_test_tensor)\n",
    "                y_pred_binary = (y_pred > params['PREDICTION_THRESHOLD']).float()\n",
    "                \n",
    "                # Calculate metrics\n",
    "                precision = precision_score(y_test_binary, y_pred_binary.cpu().numpy(), zero_division=0)\n",
    "                recall = recall_score(y_test_binary, y_pred_binary.cpu().numpy(), zero_division=0)\n",
    "                f1 = f1_score(y_test_binary, y_pred_binary.cpu().numpy(), zero_division=0)\n",
    "                \n",
    "                logger.info(f\"Epoch {epoch}: Loss: {total_loss/len(train_loader):.4f}\")\n",
    "                logger.info(f\"Train - Precision: {train_precision:.4f}, Recall: {train_recall:.4f}, F1: {train_f1:.4f}\")\n",
    "                logger.info(f\"Val - Precision: {precision:.4f}, Recall: {recall:.4f}, F1: {f1:.4f}\")\n",
    "                \n",
    "                if f1 > best_f1:\n",
    "                    best_f1 = f1\n",
    "                    best_model = copy.deepcopy(model)\n",
    "            \n",
    "            # Log metrics\n",
    "            mlflow.log_metric(\"train_loss\", total_loss / len(train_loader), step=epoch)\n",
    "            mlflow.log_metric(\"train_precision\", train_precision, step=epoch)\n",
    "            mlflow.log_metric(\"train_recall\", train_recall, step=epoch)\n",
    "            mlflow.log_metric(\"train_f1\", train_f1, step=epoch)\n",
    "            mlflow.log_metric(\"val_precision\", precision, step=epoch)\n",
    "            mlflow.log_metric(\"val_recall\", recall, step=epoch)\n",
    "            mlflow.log_metric(\"val_f1\", f1, step=epoch)\n",
    "        \n",
    "        # Final evaluation\n",
    "        best_model.eval()\n",
    "        with torch.no_grad():\n",
    "            y_pred = best_model(X_test_tensor)\n",
    "            y_pred_binary = (y_pred > params['PREDICTION_THRESHOLD']).float()\n",
    "            \n",
    "            precision = precision_score(y_test_binary, y_pred_binary.cpu().numpy(), zero_division=0)\n",
    "            recall = recall_score(y_test_binary, y_pred_binary.cpu().numpy(), zero_division=0)\n",
    "            f1 = f1_score(y_test_binary, y_pred_binary.cpu().numpy(), zero_division=0)\n",
    "            conf_matrix = confusion_matrix(y_test_binary, y_pred_binary.cpu().numpy())\n",
    "            \n",
    "            # Calculate the percentage of positive predictions\n",
    "            positive_pred_percentage = y_pred_binary.mean().item() * 100\n",
    "            \n",
    "            # Log final metrics\n",
    "            mlflow.log_metric(\"test_precision\", precision)\n",
    "            mlflow.log_metric(\"test_recall\", recall)\n",
    "            mlflow.log_metric(\"test_f1\", f1)\n",
    "            mlflow.log_metric(\"positive_pred_percentage\", positive_pred_percentage)\n",
    "            \n",
    "            logger.info(f\"Test Precision: {precision:.4f}, Recall: {recall:.4f}, F1: {f1:.4f}\")\n",
    "            logger.info(f\"Positive prediction percentage: {positive_pred_percentage:.2f}%\")\n",
    "            logger.info(f\"Confusion Matrix:\\n{conf_matrix}\")\n",
    "            \n",
    "            # Log confusion matrix as a figure\n",
    "            plt.figure(figsize=(10,8))\n",
    "            sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues')\n",
    "            plt.title('Confusion Matrix')\n",
    "            plt.ylabel('True label')\n",
    "            plt.xlabel('Predicted label')\n",
    "            plt.savefig(\"confusion_matrix.png\")\n",
    "            mlflow.log_artifact(\"confusion_matrix.png\")\n",
    "            \n",
    "            # Log the PyTorch model\n",
    "            mlflow.pytorch.log_model(best_model, \"model\")\n",
    "        \n",
    "        # Penalize if the model is just predicting the majority class\n",
    "        if positive_pred_percentage < 0.1 or positive_pred_percentage > 99.9:\n",
    "            f1 = 0\n",
    "        \n",
    "        logger.info(f\"Trial completed. Best F1 score: {f1:.4f}\")\n",
    "        return f1\n",
    "    \n",
    "# Run Optuna optimization\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=50)\n",
    "\n",
    "# Print and log the best parameters and score\n",
    "best_params = study.best_params\n",
    "best_score = study.best_value\n",
    "logger.info(f\"Best parameters: {best_params}\")\n",
    "logger.info(f\"Best precision score: {best_score}\")\n",
    "\n",
    "# Save the best parameters to a JSON file\n",
    "with open('best_params.json', 'w') as f:\n",
    "    json.dump(best_params, f)\n",
    "\n",
    "logger.info(\"Hyperparameter tuning completed. Best parameters saved to 'best_params.json'.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
