{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequence Model Research\n",
    "\n",
    "The scope of this notebook is to assess and train different sequence models given the training data generated.\n",
    "\n",
    "Training data is generated based on financial time series data labeled with potential profits using a buy-sell system.\n",
    "\n",
    "The goal is to create a sequence model that can choose favourable stock charts equal to or better than a human can via traditional technical analysis.\n",
    "\n",
    "## Import Libraries and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded sequences shape: (115000, 84, 15)\n",
      "Loaded sequences size: 144900000\n",
      "Loaded labels shape: (115000,)\n",
      "Loaded metadata shape: (115000, 2)\n",
      "Expected total size: 434700000\n",
      "\n",
      "Price-related columns:\n",
      "Index: 0, Column: Open\n",
      "Index: 1, Column: High\n",
      "Index: 2, Column: Low\n",
      "Index: 3, Column: Close\n",
      "Index: 9, Column: Close_21_bar_ema\n",
      "Index: 10, Column: Close_50_bar_sma\n",
      "Index: 11, Column: Close_150_bar_sma\n",
      "Index: 12, Column: Close_200_bar_sma\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# Define the data directory relative to the script location\n",
    "data_dir = 'data'\n",
    "\n",
    "# Define the file paths\n",
    "sequences_path = os.path.join(data_dir, 'sequences.npy')\n",
    "labels_path = os.path.join(data_dir, 'labels.npy')\n",
    "metadata_path = os.path.join(data_dir, 'metadata.npy')\n",
    "\n",
    "# Load the data\n",
    "try:\n",
    "    data_sequences = np.load(sequences_path)\n",
    "    data_labels = np.load(labels_path)\n",
    "    data_metadata = np.load(metadata_path)\n",
    "\n",
    "    # Number of examples to select\n",
    "    num_examples = 115000\n",
    "\n",
    "    # Generate a random permutation of indices\n",
    "    indices = np.random.permutation(len(data_sequences))\n",
    "\n",
    "    # Select the first `num_examples` indices\n",
    "    selected_indices = indices[:num_examples]\n",
    "\n",
    "    # Use the selected indices to create the random subset\n",
    "    data_sequences = data_sequences[selected_indices, -84:, :]\n",
    "    data_labels = data_labels[selected_indices]\n",
    "    data_metadata = data_metadata[selected_indices]\n",
    "\n",
    "    # Inspect the shape and size of the loaded data before slicing\n",
    "    print(f'Loaded sequences shape: {data_sequences.shape}')\n",
    "    print(f'Loaded sequences size: {data_sequences.size}')\n",
    "    print(f'Loaded labels shape: {data_labels.shape}')\n",
    "    print(f'Loaded metadata shape: {data_metadata.shape}')\n",
    "\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Error loading files: {e}\")\n",
    "except ValueError as e:\n",
    "    print(f\"Value error: {e}\")\n",
    "\n",
    "# Calculate and print the expected total size\n",
    "expected_total_size = num_examples * 252 * 15\n",
    "print(f'Expected total size: {expected_total_size}')\n",
    "\n",
    "# Define relevant columns and indices for normalization\n",
    "relevant_columns = [\n",
    "    'Open', 'High', 'Low', 'Close', 'Volume', 'Turnover', 'Consol_Detected',\n",
    "    'Consol_Len_Bars', 'Consol_Depth_Percent', 'Close_21_bar_ema',\n",
    "    'Close_50_bar_sma', 'Close_150_bar_sma', 'Close_200_bar_sma',\n",
    "    'RSL', 'RSL_NH'\n",
    "]\n",
    "\n",
    "price_columns_indices = [0, 1, 2, 3, 9, 10, 11, 12]  # Indices of price-related columns in the sequence data\n",
    "\n",
    "# Map indices to column names\n",
    "price_columns = [relevant_columns[i] for i in price_columns_indices]\n",
    "\n",
    "print(\"\\nPrice-related columns:\")\n",
    "for index, column in zip(price_columns_indices, price_columns):\n",
    "    print(f\"Index: {index}, Column: {column}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "### NaN Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaNs in data_sequences: 1471552\n",
      "NaNs remaining in data_sequences after removal: 0\n",
      "NaNs in data_labels: 0\n",
      "Data Seq Min: 0.0\n",
      "Data Seq Max: 468223510183936.0\n"
     ]
    }
   ],
   "source": [
    "# Replace all NaNs with 0 due to moving averages having insufficient data to compute anything, leaving blank inputs.\n",
    "# Check if NaNs exist\n",
    "\n",
    "# Dictionary to map variable names to their corresponding data arrays\n",
    "data_dict = {\n",
    "    'data_sequences': data_sequences,\n",
    "    'data_labels': data_labels,\n",
    "}\n",
    "\n",
    "# Using a dictionary to iterate over variables\n",
    "for var_name, data in data_dict.items():\n",
    "    num_nans = np.sum(np.isnan(data))\n",
    "    print(f\"NaNs in {var_name}: {num_nans}\")\n",
    "\n",
    "    # Remove NaNs\n",
    "    if num_nans > 0:\n",
    "        data_dict[var_name][:] = np.nan_to_num(data)\n",
    "        num_nans = np.sum(np.isnan(data))\n",
    "        print(f\"NaNs remaining in {var_name} after removal: {num_nans}\")\n",
    "\n",
    "print(f\"Data Seq Min: {np.min(data_sequences[:,:,price_columns_indices])}\")\n",
    "print(f\"Data Seq Max: {np.max(data_sequences[:,:,price_columns_indices])}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Corrupted sequence removal\n",
    "\n",
    "99% of stocks I buy will be below 1000, with a few above 1000, although they are important.\n",
    "\n",
    "I also noticed quite a few training examples have weird price data, which I filter out below.\n",
    "\n",
    "I noticed with thresholds above 3e3, the max is the threshold, which is very suspect.\n",
    "\n",
    "The loss of training examples is insignificant, and the result is better normalization of the data and obviously no corrupted sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Abnormal Sequence Count: 1040\n",
      "Indices of abnormal sequences: [234, 247, 274, 544, 589, 638, 666, 745, 839, 849, 1004, 1061, 1095, 1161, 1279, 1462, 1473, 1580, 1591, 1808, 1823, 1824, 2015, 2055, 2125, 2238, 2325, 2511, 2600, 2647, 2683, 2747, 2762, 2771, 2813, 2945, 3073, 3090, 3147, 3180, 3193, 3231, 3553, 3554, 3684, 3778, 4015, 4183, 4230, 4292, 4475, 4617, 4642, 4701, 4747, 4787, 4837, 4951, 5091, 5150, 5262, 5460, 5478, 5824, 5882, 5974, 6278, 6390, 6408, 6452, 6563, 6707, 6882, 6927, 7010, 7150, 7544, 7593, 7601, 7679, 7682, 7768, 7811, 7826, 7977, 8013, 8096, 8113, 8141, 8263, 8452, 8466, 8479, 8647, 8669, 8695, 8915, 9042, 9044, 9071, 9399, 9418, 9560, 9834, 9914, 9981, 10026, 10375, 10564, 10631, 10965, 11113, 11315, 11383, 11658, 12047, 12094, 12206, 12234, 12284, 12313, 12643, 12775, 12814, 12827, 12912, 13034, 13065, 13153, 13241, 13428, 13508, 13642, 13986, 14055, 14115, 14157, 14163, 14415, 14418, 14448, 14763, 14814, 14912, 14913, 14958, 14981, 15106, 15200, 15289, 15317, 15372, 15590, 15632, 15662, 15704, 16002, 16281, 16421, 16553, 16595, 16626, 16684, 16728, 16879, 16892, 17031, 17033, 17053, 17126, 17250, 17381, 17408, 17456, 17457, 17544, 17802, 18137, 18272, 18463, 18599, 18614, 18748, 18942, 19085, 19107, 19137, 19245, 19265, 19305, 19314, 19324, 19364, 19758, 19875, 19919, 20248, 20386, 20403, 20439, 20471, 20737, 20953, 21076, 21209, 21633, 22225, 22266, 22402, 22478, 22482, 22614, 22758, 22801, 23024, 23050, 23071, 23134, 23155, 23215, 23224, 23440, 23576, 23624, 23648, 23768, 23776, 23843, 23854, 24014, 24145, 24187, 24220, 24276, 24332, 24369, 24377, 24695, 24755, 24893, 25390, 25399, 25473, 25483, 25500, 25716, 25914, 26002, 26161, 26511, 26575, 26615, 26689, 26857, 27014, 27264, 27402, 27656, 27666, 27890, 27900, 27914, 28229, 28486, 28559, 28613, 28681, 28693, 28780, 28889, 28965, 29078, 29171, 29238, 29291, 29470, 29655, 29687, 29868, 29962, 29992, 29997, 30032, 30295, 30299, 30357, 30443, 30455, 30482, 30525, 30709, 30882, 30893, 30912, 30969, 31178, 31192, 31198, 31361, 31497, 31522, 31563, 31570, 31592, 31614, 31653, 31725, 31803, 31879, 31902, 31917, 32201, 32229, 32264, 32320, 32472, 32617, 32673, 32749, 32776, 32854, 32875, 32895, 32902, 33031, 33148, 33439, 33780, 33926, 34037, 34228, 34320, 34418, 34448, 34746, 34911, 34993, 35040, 35042, 35280, 35359, 35551, 35598, 35740, 35808, 35864, 36013, 36182, 36242, 36247, 36484, 36568, 36572, 36600, 36715, 36719, 36798, 36825, 36946, 37029, 37247, 37276, 37478, 37787, 37851, 37869, 37957, 38027, 38280, 38314, 38336, 38706, 38765, 38812, 38818, 39025, 39076, 39102, 39152, 39155, 39425, 39465, 39622, 39736, 39770, 40174, 40181, 40190, 40412, 40486, 40627, 40630, 40982, 41183, 41297, 41302, 41458, 41650, 41954, 41955, 41957, 41997, 42048, 42128, 42377, 42404, 42475, 42567, 42625, 42660, 42934, 42986, 43046, 43059, 43110, 43127, 43214, 43242, 43255, 43303, 43336, 43385, 43582, 43749, 43864, 44169, 44208, 44230, 44238, 44372, 44803, 44855, 44946, 45044, 45098, 45124, 45192, 45343, 45449, 45691, 45921, 46273, 46324, 46364, 46455, 46501, 46585, 46613, 46624, 46762, 46935, 47062, 47103, 47130, 47194, 47199, 47376, 47694, 47738, 47946, 48229, 48254, 48351, 48442, 48493, 48562, 48834, 48932, 48943, 49039, 49043, 49142, 49475, 49616, 49699, 49768, 49798, 49884, 50065, 50097, 50144, 50354, 50392, 50542, 50580, 50621, 51310, 51350, 51377, 51445, 51538, 51627, 51628, 51630, 51661, 51688, 51775, 51796, 51801, 51854, 51997, 52031, 52046, 52096, 52110, 52206, 52233, 52238, 52347, 52395, 52747, 52753, 52837, 52890, 52956, 53272, 53279, 53385, 53463, 53831, 53985, 54021, 54091, 54097, 54242, 54334, 54610, 54666, 54684, 54818, 54843, 54866, 54932, 55146, 55292, 55429, 55467, 55882, 56105, 56140, 56146, 56187, 56367, 56521, 56690, 56703, 56910, 57104, 57172, 57328, 57422, 57456, 57769, 57835, 57871, 57887, 57996, 58035, 58091, 58123, 58144, 58328, 58329, 58496, 58628, 58740, 59510, 59521, 59573, 59898, 60082, 60087, 60165, 60187, 60199, 60229, 60463, 60563, 60657, 60819, 60917, 60945, 61122, 61146, 61367, 61509, 61835, 61929, 61959, 62038, 62176, 62280, 62309, 62362, 62466, 62618, 62763, 62920, 63079, 63091, 63358, 63434, 63446, 63557, 63698, 63822, 63945, 63952, 64863, 65075, 65117, 65218, 65457, 65556, 65558, 65692, 65798, 66065, 66095, 66307, 66340, 66403, 66576, 66809, 66926, 66948, 67116, 67145, 67283, 67407, 67527, 67536, 67549, 67748, 67981, 68011, 68044, 68320, 68334, 68448, 68455, 68526, 68532, 68552, 68578, 68634, 68674, 69006, 69222, 69314, 69315, 69368, 69468, 69574, 69672, 69911, 69951, 70160, 70276, 70308, 70472, 70705, 71493, 71495, 71593, 71683, 71718, 71736, 71859, 71975, 72394, 72416, 72449, 72459, 72580, 72894, 73133, 73288, 73305, 73337, 73405, 73479, 73591, 73639, 73675, 73778, 73942, 74247, 74256, 74467, 74546, 74752, 74760, 74857, 74860, 74899, 74906, 75027, 75152, 75194, 75271, 75332, 75453, 75820, 75838, 76148, 76421, 76484, 76620, 76898, 76928, 76979, 77131, 77168, 77185, 77251, 77254, 77640, 78009, 78017, 78031, 78103, 78155, 78588, 78719, 78776, 78872, 78980, 79564, 79693, 79708, 79728, 79794, 79803, 79879, 79988, 80063, 80437, 80458, 80581, 80648, 80738, 80759, 80862, 80877, 81042, 81149, 81258, 81262, 81270, 81283, 81312, 81316, 81578, 81654, 81718, 81858, 81924, 81964, 81973, 82314, 82528, 82606, 82688, 82820, 82890, 82935, 83140, 83341, 83678, 83690, 83787, 83967, 83992, 84225, 84301, 84368, 84381, 84456, 84486, 84542, 84657, 84795, 85141, 85201, 85228, 85261, 85637, 85704, 85806, 85835, 85844, 86077, 86198, 86301, 86535, 86568, 86685, 86773, 86994, 87262, 87332, 87376, 87431, 87521, 87581, 87636, 87752, 87755, 87870, 87982, 88031, 88254, 88391, 88412, 88619, 88890, 88904, 89238, 89265, 89290, 89387, 89609, 89695, 89741, 89968, 90061, 90273, 90332, 90351, 90456, 90508, 90551, 90674, 90770, 90885, 90895, 91057, 91254, 91340, 91530, 91641, 91737, 91841, 91873, 92108, 92112, 92284, 92316, 92398, 92450, 92559, 92566, 92604, 92608, 92646, 92778, 93066, 93472, 93517, 93520, 93726, 93784, 93857, 93938, 94003, 94191, 94220, 94237, 94516, 94569, 94575, 94682, 94734, 94757, 94965, 94969, 95004, 95719, 96029, 96223, 96630, 96701, 96731, 96735, 96903, 97011, 97324, 97344, 97382, 97451, 97478, 97501, 97620, 97674, 97724, 97857, 97903, 98193, 98378, 98770, 99089, 99161, 99223, 99239, 99282, 99497, 99658, 99666, 99739, 99760, 99961, 100074, 100377, 100393, 100817, 100838, 100955, 101026, 101222, 101263, 101274, 101305, 101367, 101469, 101540, 101681, 101790, 101803, 101972, 102096, 102154, 102156, 102383, 102413, 102595, 102868, 103240, 103446, 103541, 103637, 103640, 103818, 103848, 103901, 103928, 103956, 104069, 104087, 104432, 104495, 104658, 104772, 104851, 104961, 105061, 105109, 105228, 105550, 105702, 106032, 106099, 106285, 106386, 106698, 106703, 106892, 106964, 107033, 107548, 107565, 107730, 107880, 108420, 108464, 108807, 108843, 109039, 109110, 109138, 109145, 109384, 109385, 109590, 109763, 110052, 110057, 110063, 110105, 110181, 110205, 110518, 110548, 110618, 110673, 110868, 110954, 111037, 111105, 111150, 111248, 111264, 111336, 111408, 111470, 111684, 111779, 112036, 112167, 112321, 112419, 112422, 112491, 112688, 112725, 112779, 112984, 113175, 113192, 113311, 113397, 113560, 113628, 113738, 113755, 113811, 113914, 113970, 113988, 114005, 114102, 114180, 114311, 114346, 114566, 114586, 114597, 114636, 114773, 114992]\n",
      "Filtered data_sequences shape: (113960, 84, 15)\n",
      "Filtered data_labels shape: (113960,)\n",
      "Data Seq Min: 0.0\n",
      "Data Seq Max: 2999.6677624511717\n"
     ]
    }
   ],
   "source": [
    "# Set the threshold for abnormal values based on domain knowledge\n",
    "threshold = 3.0e3\n",
    "\n",
    "# Detect all sequences with abnormally large price data\n",
    "abnormal_sequences = []\n",
    "\n",
    "# Iterate through each sequence to check for abnormal values\n",
    "for sequence_index in range(data_sequences.shape[0]):\n",
    "    # Extract price-related columns for the current sequence\n",
    "    price_data = data_sequences[sequence_index, :, price_columns_indices]\n",
    "    \n",
    "    # Check if any value in the price_data exceeds the threshold\n",
    "    if np.any(price_data > threshold):\n",
    "        abnormal_sequences.append(sequence_index)\n",
    "\n",
    "# Print the indices of the abnormal sequences\n",
    "print(f\"Abnormal Sequence Count: {len(abnormal_sequences)}\")\n",
    "print(f\"Indices of abnormal sequences: {abnormal_sequences}\")\n",
    "\n",
    "# Create a mask for sequences that are not abnormal\n",
    "mask = np.ones(data_sequences.shape[0], dtype=bool)\n",
    "mask[abnormal_sequences] = False\n",
    "\n",
    "# Filter out abnormal sequences from data_sequences and data_labels\n",
    "filtered_data_sequences = data_sequences[mask]\n",
    "filtered_data_labels = data_labels[mask]\n",
    "\n",
    "# Print the shape of the filtered data\n",
    "print(f\"Filtered data_sequences shape: {filtered_data_sequences.shape}\")\n",
    "print(f\"Filtered data_labels shape: {filtered_data_labels.shape}\")\n",
    "\n",
    "print(f\"Data Seq Min: {np.min(filtered_data_sequences[:,:,price_columns_indices])}\")\n",
    "print(f\"Data Seq Max: {np.max(filtered_data_sequences[:,:,price_columns_indices])}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization of Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized Data Seq Min: -1000.0\n",
      "Normalized Data Seq Max: 1000.0\n",
      "Data Labels for > 10.0% Min: 0\n",
      "Data Labels for > 10.0% Max: 1\n",
      "Number of labels that are 1: 21254\n",
      "Number of labels that are 0: 93746\n",
      "Probability of randomly selecting a stock making 10.0% is 18.481739130434782%\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Indices of price-related columns\n",
    "price_columns_indices = [0, 1, 2, 3]\n",
    "ma_columns_indices = [9, 10, 11, 12]\n",
    "\n",
    "# Extract data shapes\n",
    "num_sequences, num_timesteps, num_features = data_sequences.shape\n",
    "\n",
    "# Calculate log transformation for price-related features\n",
    "price_data = data_sequences[:, :, price_columns_indices]\n",
    "log_price_data = np.log(price_data + 1e-8)\n",
    "\n",
    "# Replace the original price-related features with the log values\n",
    "data_sequences[:, :, price_columns_indices] = log_price_data\n",
    "\n",
    "# Calculate percentage away from the Close price for moving averages\n",
    "close_price_data = data_sequences[:, :, 3].reshape(num_sequences, num_timesteps, 1)  # Close price at index 3\n",
    "ma_data = data_sequences[:, :, ma_columns_indices]\n",
    "\n",
    "# Avoid division by zero by adding epsilon\n",
    "epsilon = 1e-8\n",
    "percentage_away_from_close = (close_price_data - ma_data) / (ma_data + epsilon)\n",
    "\n",
    "# Replace the original moving average features with the percentage away values\n",
    "data_sequences[:, :, ma_columns_indices] = percentage_away_from_close\n",
    "\n",
    "# Handle infinite values by replacing them with NaNs and then replacing NaNs with zero\n",
    "data_sequences = np.nan_to_num(data_sequences, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "\n",
    "# Clip extreme values to avoid large outliers\n",
    "data_sequences = np.clip(data_sequences, -1e3, 1e3)\n",
    "\n",
    "# Normalize the price-related features together\n",
    "price_scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "\n",
    "# Reshape the price-related features to fit the scaler\n",
    "original_shape = data_sequences[:, :, price_columns_indices].shape\n",
    "reshaped_data = data_sequences[:, :, price_columns_indices].reshape(-1, len(price_columns_indices))\n",
    "\n",
    "# Fit and transform the price-related features\n",
    "normalized_price_data = price_scaler.fit_transform(reshaped_data)\n",
    "\n",
    "# Reshape back to the original shape\n",
    "normalized_price_data = normalized_price_data.reshape(original_shape)\n",
    "\n",
    "# Replace the original price-related features with the normalized ones\n",
    "data_sequences[:, :, price_columns_indices] = normalized_price_data\n",
    "\n",
    "# Normalize the remaining features individually\n",
    "for feature_index in range(num_features):\n",
    "    if feature_index not in price_columns_indices and feature_index not in ma_columns_indices:\n",
    "        # Initialize a new scaler for each feature\n",
    "        feature_scaler = MinMaxScaler()\n",
    "\n",
    "        # Extract the feature data\n",
    "        feature_data = data_sequences[:, :, feature_index].reshape(-1, 1)\n",
    "\n",
    "        # Fit and transform the scaler\n",
    "        normalized_feature_data = feature_scaler.fit_transform(feature_data)\n",
    "\n",
    "        # Reshape back to the original shape\n",
    "        normalized_feature_data = normalized_feature_data.reshape(num_sequences, num_timesteps)\n",
    "\n",
    "        # Replace the original feature with the normalized one\n",
    "        data_sequences[:, :, feature_index] = normalized_feature_data\n",
    "\n",
    "# Print normalized data sequences to check\n",
    "print(f\"Normalized Data Seq Min: {np.min(data_sequences)}\")\n",
    "print(f\"Normalized Data Seq Max: {np.max(data_sequences)}\")\n",
    "\n",
    "# Make the labels a binary decision, rather than a profit\n",
    "min_profit = 0.1  # implies a good decision is a breakout that produces more than min_profit (*100 for percent, 0.2 = 20%)\n",
    "\n",
    "data_labels = (data_labels > min_profit).astype(int)\n",
    "\n",
    "print(f\"Data Labels for > {min_profit*100}% Min: {np.min(data_labels)}\")\n",
    "print(f\"Data Labels for > {min_profit*100}% Max: {np.max(data_labels)}\")\n",
    "\n",
    "# Count how many labels are 1 and how many are 0\n",
    "num_ones = np.sum(data_labels)\n",
    "num_zeros = len(data_labels) - num_ones\n",
    "\n",
    "print(f\"Number of labels that are 1: {num_ones}\")\n",
    "print(f\"Number of labels that are 0: {num_zeros}\")\n",
    "print(f\"Probability of randomly selecting a stock making {min_profit*100}% is {num_ones/(num_ones+num_zeros)*100}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model -> Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 20 Complete [00h 01m 48s]\n",
      "val_accuracy_reward: 0.4927482008934021\n",
      "\n",
      "Best val_accuracy_reward So Far: 0.49390295147895813\n",
      "Total elapsed time: 08h 19m 35s\n",
      "Fold 2/5\n",
      "Fold 3/5\n",
      "Fold 4/5\n",
      "Fold 5/5\n",
      "\n",
      "The optimal number of units in the first LSTM layer is 96.\n",
      "The optimal number of units in the second LSTM layer is 256.\n",
      "The optimal number of units in the third LSTM layer is 32.\n",
      "The optimal number of units in the dense layer is 32.\n",
      "The optimal dropout rate is 0.1.\n",
      "The optimal L2 regularization rate is 1.6599485653646474e-05.\n",
      "The optimal initial learning rate is 0.004138474276706615.\n",
      "The optimal decay rate is 0.9592295700572687.\n",
      "\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1718632636.280709  557502 op_level_cost_estimator.cc:699] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"GPU\" vendor: \"NVIDIA\" model: \"NVIDIA GeForce RTX 3070 Ti Laptop GPU\" frequency: 1410 num_cores: 46 environment { key: \"architecture\" value: \"8.6\" } environment { key: \"cuda\" value: \"12000\" } environment { key: \"cudnn\" value: \"8800\" } num_registers: 65536 l1_cache_size: 24576 l2_cache_size: 4194304 shared_memory_size_per_multiprocessor: 102400 memory_size: 5834276864 bandwidth: 448064000 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3748/3749 [============================>.] - ETA: 0s - loss: 0.6950 - accuracy: 0.5005 - accuracy_reward: 0.2285"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1718632699.649812  557502 op_level_cost_estimator.cc:699] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"GPU\" vendor: \"NVIDIA\" model: \"NVIDIA GeForce RTX 3070 Ti Laptop GPU\" frequency: 1410 num_cores: 46 environment { key: \"architecture\" value: \"8.6\" } environment { key: \"cuda\" value: \"12000\" } environment { key: \"cudnn\" value: \"8800\" } num_registers: 65536 l1_cache_size: 24576 l2_cache_size: 4194304 shared_memory_size_per_multiprocessor: 102400 memory_size: 5834276864 bandwidth: 448064000 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  8/575 [..............................] - ETA: 4s  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1718632705.287915  557502 op_level_cost_estimator.cc:699] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"GPU\" vendor: \"NVIDIA\" model: \"NVIDIA GeForce RTX 3070 Ti Laptop GPU\" frequency: 1410 num_cores: 46 environment { key: \"architecture\" value: \"8.6\" } environment { key: \"cuda\" value: \"12000\" } environment { key: \"cudnn\" value: \"8800\" } num_registers: 65536 l1_cache_size: 24576 l2_cache_size: 4194304 shared_memory_size_per_multiprocessor: 102400 memory_size: 5834276864 bandwidth: 448064000 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "575/575 [==============================] - 4s 6ms/step\n",
      "Stopping early: All predictions are the same at epoch 1.\n",
      "3749/3749 [==============================] - 76s 19ms/step - loss: 0.6950 - accuracy: 0.5005 - accuracy_reward: 0.2285 - val_loss: 0.6829 - val_accuracy: 0.8156 - val_accuracy_reward: 0.0000e+00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7f4681c0d5d0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Input, Attention, Flatten, Dropout, Conv1D, MaxPooling1D, GRU, BatchNormalization\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.regularizers import l1, l2\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import keras_tuner as kt\n",
    "import numpy as np\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Check if TensorFlow is using GPU\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "\n",
    "# Ensure data_sequences and data_labels are already normalized and available\n",
    "print(\"Shape of data_sequences:\", data_sequences.shape)\n",
    "print(\"Shape of data_labels:\", data_labels.shape)\n",
    "\n",
    "# Indices of the columns to be removed\n",
    "columns_to_remove = [0, 3, 5, 6, 13]\n",
    "\n",
    "# Remove specified columns\n",
    "data_sequences = np.delete(data_sequences, columns_to_remove, axis=2)\n",
    "\n",
    "# Verify the shapes before modification\n",
    "print(f\"Modified data_sequences shape: {data_sequences.shape}\")\n",
    "\n",
    "# Split data into training, validation, and test sets\n",
    "test_size = 0.2\n",
    "val_size = 0.2\n",
    "X, X_test, y, y_test = train_test_split(data_sequences, data_labels, test_size=test_size, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=val_size, random_state=42)\n",
    "\n",
    "print(\"Shape of X_train:\", X_train.shape)\n",
    "print(\"Shape of X_val:\", X_val.shape)\n",
    "print(\"Shape of X_test:\", X_test.shape)\n",
    "print(\"Shape of y_train:\", y_train.shape)\n",
    "print(\"Shape of y_val:\", y_val.shape)\n",
    "print(\"Shape of y_test:\", y_test.shape)\n",
    "\n",
    "# Oversample the minority class using SMOTE\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_reshaped = X_train.reshape((X_train.shape[0], -1))  # Reshape for SMOTE\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train_reshaped, y_train)\n",
    "X_train_resampled = X_train_resampled.reshape((X_train_resampled.shape[0], X_train.shape[1], X_train.shape[2]))\n",
    "\n",
    "# Calculate class weights\n",
    "class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(y_train_resampled), y=y_train_resampled)\n",
    "class_weights = {i: class_weights[i] for i in range(len(class_weights))}\n",
    "\n",
    "print(\"Class weights:\", class_weights)\n",
    "\n",
    "class AccuracyReward(tf.keras.metrics.Metric):\n",
    "    def __init__(self, name='accuracy_reward', **kwargs):\n",
    "        super(AccuracyReward, self).__init__(name=name, **kwargs)\n",
    "        self.tp = self.add_weight(name='tp', initializer='zeros')\n",
    "        self.fp = self.add_weight(name='fp', initializer='zeros')\n",
    "        self.total = self.add_weight(name='total', initializer='zeros')\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        y_pred = tf.cast(y_pred > 0.5, tf.int32)\n",
    "        y_true = tf.cast(y_true, tf.int32)\n",
    "        \n",
    "        tp = tf.reduce_sum(tf.cast((y_true == 1) & (y_pred == 1), tf.float32))\n",
    "        fp = tf.reduce_sum(tf.cast((y_true == 0) & (y_pred == 1), tf.float32))\n",
    "        total = tf.cast(tf.size(y_true), tf.float32)\n",
    "        \n",
    "        self.tp.assign_add(tp)\n",
    "        self.fp.assign_add(fp)\n",
    "        self.total.assign_add(total)\n",
    "\n",
    "    def result(self):\n",
    "        batting_average = self.tp / (self.tp + self.fp + tf.keras.backend.epsilon())\n",
    "        opportunities = self.tp + self.fp\n",
    "        accuracy_reward = batting_average * (opportunities / self.total)\n",
    "        return accuracy_reward\n",
    "\n",
    "    def reset_state(self):\n",
    "        self.tp.assign(0)\n",
    "        self.fp.assign(0)\n",
    "        self.total.assign(0)\n",
    "\n",
    "# Define the custom callback to monitor predictions and display custom metrics\n",
    "class CustomMetricsCallback(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, validation_data, model_stop_on_one_outcome=True):\n",
    "        super(CustomMetricsCallback, self).__init__()\n",
    "        self.validation_data = validation_data\n",
    "        self.model_stop_on_one_outcome = model_stop_on_one_outcome\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        X_val, y_val = self.validation_data\n",
    "        y_pred = (self.model.predict(X_val) > 0.5).astype(int).flatten()\n",
    "        y_true = y_val.flatten()\n",
    "\n",
    "        # Check if all predictions are the same\n",
    "        if np.all(y_pred == y_pred[0]):\n",
    "            print(f\"Stopping early: All predictions are the same at epoch {epoch + 1}.\")\n",
    "            self.model.stop_training = True\n",
    "            return\n",
    "\n",
    "        tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "\n",
    "        total = len(y_true)\n",
    "        tp_percent = tp / total * 100\n",
    "        tn_percent = tn / total * 100\n",
    "        fp_percent = fp / total * 100\n",
    "        fn_percent = fn / total * 100\n",
    "\n",
    "        batting_average = tp_percent / (tp_percent + fp_percent) if (tp_percent + fp_percent) != 0 else np.nan\n",
    "\n",
    "        # Calculate opportunities taken\n",
    "        opportunities = tp + fp\n",
    "        accuracy_reward = batting_average * (opportunities / total)\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}:\")\n",
    "        print(f\"True Positives: {tp_percent:.2f}%\")\n",
    "        print(f\"True Negatives: {tn_percent:.2f}%\")\n",
    "        print(f\"False Positives: {fp_percent:.2f}%\")\n",
    "        print(f\"False Negatives: {fn_percent:.2f}%\")\n",
    "        print(f\"Batting Average: {batting_average:.2f}\")\n",
    "        print(f\"Opportunities Taken: {opportunities}\")\n",
    "        print(f\"Accuracy Reward: {accuracy_reward:.2f}\")\n",
    "\n",
    "        logs['tp_percent'] = tp_percent\n",
    "        logs['tn_percent'] = tn_percent\n",
    "        logs['fp_percent'] = fp_percent\n",
    "        logs['fn_percent'] = fn_percent\n",
    "        logs['batting_average'] = batting_average\n",
    "        logs['opportunities'] = opportunities\n",
    "        logs['accuracy_reward'] = accuracy_reward\n",
    "\n",
    "# Define the hypermodel for Keras Tuner\n",
    "def build_model(hp):\n",
    "    input_layer = Input(shape=(X_train.shape[1], X_train.shape[2]))\n",
    "\n",
    "    # Add a Conv1D layer for feature extraction, include as hyperparameter\n",
    "    if hp.Boolean('use_conv'):\n",
    "        conv_out = Conv1D(filters=hp.Int('conv_filters', min_value=32, max_value=128, step=32), \n",
    "                          kernel_size=hp.Int('conv_kernel_size', min_value=3, max_value=7, step=2), \n",
    "                          activation='relu')(input_layer)\n",
    "        conv_out = MaxPooling1D(pool_size=2)(conv_out)\n",
    "    else:\n",
    "        conv_out = input_layer\n",
    "\n",
    "    # Add LSTM layers\n",
    "    lstm_out = LSTM(\n",
    "        units=hp.Int('lstm_units_l1', min_value=64, max_value=256, step=32), \n",
    "        return_sequences=True,\n",
    "        dropout=hp.Float('dropout_rate', min_value=0.1, max_value=0.5, step=0.1),\n",
    "        kernel_regularizer=l2(hp.Float('l2_regularization', min_value=1e-5, max_value=1e-2, sampling='log')),\n",
    "        kernel_initializer='glorot_uniform',\n",
    "        recurrent_initializer='orthogonal',\n",
    "        bias_initializer='zeros'\n",
    "    )(conv_out)\n",
    "\n",
    "    lstm_out = LSTM(\n",
    "        units=hp.Int('lstm_units_l2', min_value=64, max_value=256, step=32), \n",
    "        return_sequences=True,\n",
    "        dropout=hp.Float('dropout_rate', min_value=0.1, max_value=0.5, step=0.1),\n",
    "        kernel_regularizer=l2(hp.Float('l2_regularization', min_value=1e-5, max_value=1e-2, sampling='log')),\n",
    "        kernel_initializer='glorot_uniform',\n",
    "        recurrent_initializer='orthogonal',\n",
    "        bias_initializer='zeros'\n",
    "    )(lstm_out)\n",
    "\n",
    "    lstm_out = LSTM(\n",
    "        units=hp.Int('lstm_units_l3', min_value=32, max_value=128, step=32),\n",
    "        return_sequences=True,\n",
    "        dropout=hp.Float('dropout_rate', min_value=0.1, max_value=0.5, step=0.1),\n",
    "        kernel_regularizer=l2(hp.Float('l2_regularization', min_value=1e-5, max_value=1e-2, sampling='log')),\n",
    "        kernel_initializer='glorot_uniform',\n",
    "        recurrent_initializer='orthogonal',\n",
    "        bias_initializer='zeros'\n",
    "    )(lstm_out)\n",
    "\n",
    "    # Add Attention layer\n",
    "    attention = Attention()([lstm_out, lstm_out])  \n",
    "    attention_flatten = Flatten()(attention)\n",
    "\n",
    "    dense_out = Dense(\n",
    "        units=hp.Int('dense_units', min_value=32, max_value=128, step=32), \n",
    "        activation='relu',\n",
    "        kernel_initializer='glorot_uniform',\n",
    "        bias_initializer='zeros'\n",
    "    )(attention_flatten)\n",
    "    \n",
    "    dense_out = Dropout(\n",
    "        rate=hp.Float('dropout_rate', min_value=0.1, max_value=0.5, step=0.1)\n",
    "    )(dense_out)\n",
    "    \n",
    "    output_layer = Dense(\n",
    "        1, \n",
    "        activation='sigmoid',\n",
    "        kernel_initializer='glorot_uniform',\n",
    "        bias_initializer='zeros'\n",
    "    )(dense_out)\n",
    "\n",
    "    model = tf.keras.Model(inputs=input_layer, outputs=output_layer)\n",
    "\n",
    "    learning_rate = hp.Float('learning_rate', min_value=1e-5, max_value=1e-2, sampling='log')\n",
    "    decay_steps = 1000\n",
    "    decay_rate = hp.Float('decay_rate', min_value=0.9, max_value=0.999, sampling='log')\n",
    "    \n",
    "    lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "        initial_learning_rate=learning_rate,\n",
    "        decay_steps=decay_steps,\n",
    "        decay_rate=decay_rate\n",
    "    )\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=lr_schedule),\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy', AccuracyReward()]\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Use BayesianOptimization for hyperparameter tuning\n",
    "tuner = kt.BayesianOptimization(\n",
    "    hypermodel=build_model,\n",
    "    objective=kt.Objective(\"val_accuracy_reward\", direction=\"max\"),\n",
    "    max_trials=20,  # Increased number of trials\n",
    "    executions_per_trial=1,\n",
    "    directory='my_dir',\n",
    "    project_name='hyperparameter_tuning'\n",
    ")\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, min_delta=1e-5, restore_best_weights=True)\n",
    "\n",
    "# Perform K-Fold Cross-Validation\n",
    "k = 5  # Number of folds\n",
    "kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
    "\n",
    "for fold, (train_index, val_index) in enumerate(kf.split(X_train_resampled)):\n",
    "    print(f\"Fold {fold+1}/{k}\")\n",
    "    X_train_fold, X_val_fold = X_train_resampled[train_index], X_train_resampled[val_index]\n",
    "    y_train_fold, y_val_fold = y_train_resampled[train_index], y_train_resampled[val_index]\n",
    "    \n",
    "    # Oversample the minority class in the training fold using SMOTE\n",
    "    X_train_reshaped = X_train_fold.reshape((X_train_fold.shape[0], -1))\n",
    "    smote = SMOTE(random_state=42)\n",
    "    X_train_resampled_fold, y_train_resampled_fold = smote.fit_resample(X_train_reshaped, y_train_fold)\n",
    "    X_train_resampled_fold = X_train_resampled_fold.reshape((X_train_resampled_fold.shape[0], X_train_fold.shape[1], X_train_fold.shape[2]))\n",
    "\n",
    "    # Calculate class weights for the current fold\n",
    "    class_weights_fold = compute_class_weight(class_weight='balanced', classes=np.unique(y_train_resampled_fold), y=y_train_resampled_fold)\n",
    "    class_weights_fold = {i: class_weights_fold[i] for i in range(len(class_weights_fold))}\n",
    "\n",
    "    # Create unique checkpoint path\n",
    "    checkpoint_path = f'best_model_fold_{fold+1}.keras'\n",
    "    model_checkpoint = ModelCheckpoint(checkpoint_path, monitor='val_loss', save_best_only=True)\n",
    "    \n",
    "    # Perform hyperparameter search with the custom callback\n",
    "    tuner.search(X_train_resampled_fold, y_train_resampled_fold, epochs=50, validation_data=(X_val_fold, y_val_fold), callbacks=[early_stopping, model_checkpoint, CustomMetricsCallback(validation_data=(X_val_fold, y_val_fold))], class_weight=class_weights_fold)\n",
    "\n",
    "# Extract the best hyperparameters\n",
    "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "print(f\"\"\"\n",
    "Should you use conv? {best_hps.get('use_conv')}\n",
    "The optimal number of conv1d filters is {best_hps.get('conv_filters')}.\n",
    "The optimal number of units in the first LSTM layer is {best_hps.get('lstm_units_l1')}.\n",
    "The optimal number of units in the second LSTM layer is {best_hps.get('lstm_units_l2')}.\n",
    "The optimal number of units in the third LSTM layer is {best_hps.get('lstm_units_l3')}.\n",
    "The optimal number of units in the dense layer is {best_hps.get('dense_units')}.\n",
    "The optimal dropout rate is {best_hps.get('dropout_rate')}.\n",
    "The optimal L2 regularization rate is {best_hps.get('l2_regularization')}.\n",
    "The optimal initial learning rate is {best_hps.get('learning_rate')}.\n",
    "The optimal decay rate is {best_hps.get('decay_rate')}.\n",
    "\"\"\")\n",
    "\n",
    "# Build the final model using the best hyperparameters\n",
    "model = tuner.hypermodel.build(best_hps)\n",
    "\n",
    "# Train the final model on the entire training data\n",
    "model.fit(X_train_resampled, y_train_resampled, epochs=50, validation_data=(X_val, y_val), callbacks=[early_stopping, ModelCheckpoint('best_model_final.keras', monitor='val_loss', save_best_only=True), CustomMetricsCallback(validation_data=(X_val, y_val))], class_weight=class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'trial_4.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 16\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjson\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Load trials data\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrial_4.json\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m     17\u001b[0m     trial_4 \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(f)\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrial_5.json\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "File \u001b[0;32m~/anaconda3/envs/TradeTronic/lib/python3.11/site-packages/IPython/core/interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    322\u001b[0m     )\n\u001b[0;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'trial_4.json'"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Input, Attention, Flatten, Dropout, Conv1D, MaxPooling1D, GRU, BatchNormalization\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.regularizers import l1, l2\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import keras_tuner as kt\n",
    "import numpy as np\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import json\n",
    "\n",
    "# Load trials data\n",
    "with open('trial_4.json') as f:\n",
    "    trial_4 = json.load(f)\n",
    "\n",
    "with open('trial_5.json') as f:\n",
    "    trial_5 = json.load(f)\n",
    "\n",
    "# Add more trials if needed\n",
    "# with open('trial_X.json') as f:\n",
    "#     trial_X = json.load(f)\n",
    "\n",
    "# Function to get hyperparameters from trial\n",
    "def get_hyperparameters(trial):\n",
    "    return {\n",
    "        \"use_conv\": trial['hyperparameters']['values']['use_conv'],\n",
    "        \"lstm_units_l1\": trial['hyperparameters']['values']['lstm_units_l1'],\n",
    "        \"dropout_rate\": trial['hyperparameters']['values']['dropout_rate'],\n",
    "        \"l2_regularization\": trial['hyperparameters']['values']['l2_regularization'],\n",
    "        \"lstm_units_l2\": trial['hyperparameters']['values']['lstm_units_l2'],\n",
    "        \"lstm_units_l3\": trial['hyperparameters']['values']['lstm_units_l3'],\n",
    "        \"dense_units\": trial['hyperparameters']['values']['dense_units'],\n",
    "        \"learning_rate\": trial['hyperparameters']['values']['learning_rate'],\n",
    "        \"decay_rate\": trial['hyperparameters']['values']['decay_rate'],\n",
    "        \"conv_filters\": trial['hyperparameters']['values']['conv_filters'],\n",
    "        \"conv_kernel_size\": trial['hyperparameters']['values']['conv_kernel_size']\n",
    "    }\n",
    "\n",
    "# Select trial to run\n",
    "trial_to_run = 4\n",
    "if trial_to_run == 4:\n",
    "    hyperparameters = get_hyperparameters(trial_4)\n",
    "elif trial_to_run == 5:\n",
    "    hyperparameters = get_hyperparameters(trial_5)\n",
    "# Add more trials if needed\n",
    "# elif trial_to_run == X:\n",
    "#     hyperparameters = get_hyperparameters(trial_X)\n",
    "\n",
    "# Check if TensorFlow is using GPU\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "\n",
    "# Ensure data_sequences and data_labels are already normalized and available\n",
    "print(\"Shape of data_sequences:\", data_sequences.shape)\n",
    "print(\"Shape of data_labels:\", data_labels.shape)\n",
    "\n",
    "# Indices of the columns to be removed\n",
    "columns_to_remove = [0, 3, 5, 6, 13]\n",
    "\n",
    "# Remove specified columns\n",
    "#data_sequences = np.delete(data_sequences, columns_to_remove, axis=2)\n",
    "\n",
    "# Verify the shapes before modification\n",
    "print(f\"Modified data_sequences shape: {data_sequences.shape}\")\n",
    "\n",
    "# Split data into training, validation, and test sets\n",
    "test_size = 0.2\n",
    "val_size = 0.2\n",
    "X, X_test, y, y_test = train_test_split(data_sequences, data_labels, test_size=test_size, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=val_size, random_state=42)\n",
    "\n",
    "print(\"Shape of X_train:\", X_train.shape)\n",
    "print(\"Shape of X_val:\", X_val.shape)\n",
    "print(\"Shape of X_test:\", X_test.shape)\n",
    "print(\"Shape of y_train:\", y_train.shape)\n",
    "print(\"Shape of y_val:\", y_val.shape)\n",
    "print(\"Shape of y_test:\", y_test.shape)\n",
    "\n",
    "# Oversample the minority class using SMOTE\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_reshaped = X_train.reshape((X_train.shape[0], -1))  # Reshape for SMOTE\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train_reshaped, y_train)\n",
    "X_train_resampled = X_train_resampled.reshape((X_train_resampled.shape[0], X_train.shape[1], X_train.shape[2]))\n",
    "\n",
    "# Calculate class weights\n",
    "class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(y_train_resampled), y=y_train_resampled)\n",
    "class_weights = {i: class_weights[i] for i in range(len(class_weights))}\n",
    "\n",
    "print(\"Class weights:\", class_weights)\n",
    "\n",
    "class AccuracyReward(tf.keras.metrics.Metric):\n",
    "    def __init__(self, name='accuracy_reward', **kwargs):\n",
    "        super(AccuracyReward, self).__init__(name=name, **kwargs)\n",
    "        self.tp = self.add_weight(name='tp', initializer='zeros')\n",
    "        self.fp = self.add_weight(name='fp', initializer='zeros')\n",
    "        self.total = self.add_weight(name='total', initializer='zeros')\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        y_pred = tf.cast(y_pred > 0.5, tf.int32)\n",
    "        y_true = tf.cast(y_true, tf.int32)\n",
    "        \n",
    "        tp = tf.reduce_sum(tf.cast((y_true == 1) & (y_pred == 1), tf.float32))\n",
    "        fp = tf.reduce_sum(tf.cast((y_true == 0) & (y_pred == 1), tf.float32))\n",
    "        total = tf.cast(tf.size(y_true), tf.float32)\n",
    "        \n",
    "        self.tp.assign_add(tp)\n",
    "        self.fp.assign_add(fp)\n",
    "        self.total.assign_add(total)\n",
    "\n",
    "    def result(self):\n",
    "        batting_average = self.tp / (self.tp + self.fp + tf.keras.backend.epsilon())\n",
    "        opportunities = self.tp + self.fp\n",
    "        optimal_trades = 4000  # Optimal number of trades\n",
    "        deviation = tf.abs(opportunities - optimal_trades)\n",
    "        reward_factor = tf.exp(-deviation / optimal_trades)\n",
    "        accuracy_reward = batting_average * reward_factor\n",
    "        return accuracy_reward\n",
    "\n",
    "    def reset_state(self):\n",
    "        self.tp.assign(0)\n",
    "        self.fp.assign(0)\n",
    "        self.total.assign(0)\n",
    "\n",
    "# Define the custom callback to monitor predictions and display custom metrics\n",
    "class CustomMetricsCallback(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, validation_data, model_stop_on_one_outcome=True):\n",
    "        super(CustomMetricsCallback, self).__init__()\n",
    "        self.validation_data = validation_data\n",
    "        self.model_stop_on_one_outcome = model_stop_on_one_outcome\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        X_val, y_val = self.validation_data\n",
    "        y_pred = (self.model.predict(X_val) > 0.5).astype(int).flatten()\n",
    "        y_true = y_val.flatten()\n",
    "\n",
    "        # Check if all predictions are the same\n",
    "        if np.all(y_pred == y_pred[0]):\n",
    "            print(f\"Stopping early: All predictions are the same at epoch {epoch + 1}.\")\n",
    "            self.model.stop_training = True\n",
    "            return\n",
    "\n",
    "        tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "\n",
    "        total = len(y_true)\n",
    "        tp_percent = tp / total * 100\n",
    "        tn_percent = tn / total * 100\n",
    "        fp_percent = fp / total * 100\n",
    "        fn_percent = fn / total * 100\n",
    "\n",
    "        batting_average = tp_percent / (tp_percent + fp_percent) if (tp_percent + fp_percent) != 0 else np.nan\n",
    "\n",
    "        # Calculate opportunities taken\n",
    "        opportunities = tp + fp\n",
    "        accuracy_reward = batting_average * tf.exp(-tf.abs(opportunities - 4000) / 4000)  # Adjusted accuracy reward\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}:\")\n",
    "        print(f\"True Positives: {tp_percent:.2f}%\")\n",
    "        print(f\"True Negatives: {tn_percent:.2f}%\")\n",
    "        print(f\"False Positives: {fp_percent:.2f}%\")\n",
    "        print(f\"False Negatives: {fn_percent:.2f}%\")\n",
    "        print(f\"Batting Average: {batting_average:.2f}\")\n",
    "        print(f\"Opportunities Taken: {opportunities}\")\n",
    "        print(f\"Accuracy Reward: {accuracy_reward:.2f}\")\n",
    "\n",
    "        logs['tp_percent'] = tp_percent\n",
    "        logs['tn_percent'] = tn_percent\n",
    "        logs['fp_percent'] = fp_percent\n",
    "        logs['fn_percent'] = fn_percent\n",
    "        logs['batting_average'] = batting_average\n",
    "        logs['opportunities'] = opportunities\n",
    "        logs['accuracy_reward'] = accuracy_reward\n",
    "\n",
    "# Build the model with the selected hyperparameters\n",
    "def build_model(hyperparameters):\n",
    "    input_layer = Input(shape=(X_train.shape[1], X_train.shape[2]))\n",
    "\n",
    "    if hyperparameters[\"use_conv\"]:\n",
    "        conv_out = Conv1D(filters=hyperparameters[\"conv_filters\"], \n",
    "                          kernel_size=hyperparameters[\"conv_kernel_size\"], \n",
    "                          activation='relu')(input_layer)\n",
    "        conv_out = MaxPooling1D(pool_size=2)(conv_out)\n",
    "    else:\n",
    "        conv_out = input_layer\n",
    "\n",
    "    lstm_out = LSTM(\n",
    "        units=hyperparameters[\"lstm_units_l1\"], \n",
    "        return_sequences=True,\n",
    "        dropout=hyperparameters[\"dropout_rate\"],\n",
    "        kernel_regularizer=l2(hyperparameters[\"l2_regularization\"]),\n",
    "        kernel_initializer='glorot_uniform',\n",
    "        recurrent_initializer='orthogonal',\n",
    "        bias_initializer='zeros'\n",
    "    )(conv_out)\n",
    "\n",
    "    lstm_out = LSTM(\n",
    "        units=hyperparameters[\"lstm_units_l2\"], \n",
    "        return_sequences=True,\n",
    "        dropout=hyperparameters[\"dropout_rate\"],\n",
    "        kernel_regularizer=l2(hyperparameters[\"l2_regularization\"]),\n",
    "        kernel_initializer='glorot_uniform',\n",
    "        recurrent_initializer='orthogonal',\n",
    "        bias_initializer='zeros'\n",
    "    )(lstm_out)\n",
    "\n",
    "    lstm_out = LSTM(\n",
    "        units=hyperparameters[\"lstm_units_l3\"], \n",
    "        return_sequences=True,\n",
    "        dropout=hyperparameters[\"dropout_rate\"],\n",
    "        kernel_regularizer=l2(hyperparameters[\"l2_regularization\"]),\n",
    "        kernel_initializer='glorot_uniform',\n",
    "        recurrent_initializer='orthogonal',\n",
    "        bias_initializer='zeros'\n",
    "    )(lstm_out)\n",
    "\n",
    "    attention = Attention()([lstm_out, lstm_out])  \n",
    "    attention_flatten = Flatten()(attention)\n",
    "\n",
    "    dense_out = Dense(\n",
    "        units=hyperparameters[\"dense_units\"], \n",
    "        activation='relu',\n",
    "        kernel_initializer='glorot_uniform',\n",
    "        bias_initializer='zeros'\n",
    "    )(attention_flatten)\n",
    "    \n",
    "    dense_out = Dropout(\n",
    "        rate=hyperparameters[\"dropout_rate\"]\n",
    "    )(dense_out)\n",
    "    \n",
    "    output_layer = Dense(\n",
    "        1, \n",
    "        activation='sigmoid',\n",
    "        kernel_initializer='glorot_uniform',\n",
    "        bias_initializer='zeros'\n",
    "    )(dense_out)\n",
    "\n",
    "    model = tf.keras.Model(inputs=input_layer, outputs=output_layer)\n",
    "\n",
    "    decay_steps = 1000\n",
    "    lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "        initial_learning_rate=hyperparameters[\"learning_rate\"],\n",
    "        decay_steps=decay_steps,\n",
    "        decay_rate=hyperparameters[\"decay_rate\"]\n",
    "    )\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=lr_schedule),\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy', AccuracyReward()]\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Build the final model using the selected hyperparameters\n",
    "model = build_model(hyperparameters)\n",
    "\n",
    "# Train the final model on the entire training data\n",
    "model.fit(X_train_resampled, y_train_resampled, epochs=200, validation_data=(X_val, y_val), callbacks=[EarlyStopping(monitor='val_loss', patience=10, min_delta=1e-5, restore_best_weights=True), ModelCheckpoint('best_model_final.keras', monitor='val_loss', save_best_only=True), CustomMetricsCallback(validation_data=(X_val, y_val))], class_weight=class_weights)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tuning results\n",
    "\n",
    "| Trial Number | Validation Accuracy | Batting Average | Validation Accuracy Reward |\n",
    "|--------------|---------------------|-----------------|----------------------------|\n",
    "| 00           | 0.613756            | 0.712339        | 0.382110                   |\n",
    "| 01           | 0.516621            | 0.505742        | 0.463459                   |\n",
    "| 02           | 0.733901            | 0.712339        | 0.382110                   |\n",
    "| 03           | 0.523929            | 0.509944        | 0.457237                   |\n",
    "| 04           | 0.757245            | 0.770901        | 0.357346                   |\n",
    "| 05           | 0.765639            | 0.751111        | 0.388165                   |\n",
    "| 06           | 0.586869            | 0.565           | 0.3460                     |\n",
    "| 07           | 0.493903            | Not available   | 0.225436                   |\n",
    "| 08           | 0.493903            | Not available   | 0.236490                   |\n",
    "| 09           | 0.765639            | 0.751111        | 0.388165                   |\n",
    "| 10           | 0.582828            | 0.5708          | 0.3042                     |\n",
    "| 11           | 0.589697            | 0.5781          | 0.3052                     |\n",
    "| 12           | 0.589026            | 0.5737          | 0.3182                     |\n",
    "| 13           | 0.748241            | 0.509944        | 0.457237                   |\n",
    "| 14           | 0.757733            | 0.357346        | 0.357346                   |\n",
    "| 15           | 0.523929            | 0.509944        | 0.457237                   |\n",
    "| 16           | 0.586869            | 0.5650          | 0.3460                     |\n",
    "| 17           | 0.493903            | Not available   | 0.225436                   |\n",
    "| 18           | 0.586869            | 0.5650          | 0.3460                     |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model -> Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
