{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequence Model Research\n",
    "\n",
    "The scope of this notebook is to assess and train different sequence models given the training data generated.\n",
    "\n",
    "Training data is generated based on financial time series data labeled with potential profits using a buy-sell system.\n",
    "\n",
    "The goal is to create a sequence model that can choose favourable stock charts equal to or better than a human can via traditional technical analysis.\n",
    "\n",
    "## Import Libraries and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# Define the data directory relative to the script location\n",
    "data_dir = 'data'\n",
    "\n",
    "# Define the file paths\n",
    "sequences_path = os.path.join(data_dir, 'sequences.npy')\n",
    "labels_path = os.path.join(data_dir, 'labels.npy')\n",
    "metadata_path = os.path.join(data_dir, 'metadata.npy')\n",
    "\n",
    "# Load the data\n",
    "try:\n",
    "    data_sequences = np.load(sequences_path)\n",
    "    data_labels = np.load(labels_path)\n",
    "    data_metadata = np.load(metadata_path)\n",
    "\n",
    "    # Number of examples to select\n",
    "    num_examples = 115000\n",
    "\n",
    "    # Generate a random permutation of indices\n",
    "    indices = np.random.permutation(len(data_sequences))\n",
    "\n",
    "    # Select the first `num_examples` indices\n",
    "    selected_indices = indices[:num_examples]\n",
    "\n",
    "    # Use the selected indices to create the random subset\n",
    "    data_sequences = data_sequences[selected_indices, -84:, :]\n",
    "    data_labels = data_labels[selected_indices]\n",
    "    data_metadata = data_metadata[selected_indices]\n",
    "\n",
    "    # Inspect the shape and size of the loaded data before slicing\n",
    "    print(f'Loaded sequences shape: {data_sequences.shape}')\n",
    "    print(f'Loaded sequences size: {data_sequences.size}')\n",
    "    print(f'Loaded labels shape: {data_labels.shape}')\n",
    "    print(f'Loaded metadata shape: {data_metadata.shape}')\n",
    "\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Error loading files: {e}\")\n",
    "except ValueError as e:\n",
    "    print(f\"Value error: {e}\")\n",
    "\n",
    "# Calculate and print the expected total size\n",
    "expected_total_size = num_examples * 252 * 15\n",
    "print(f'Expected total size: {expected_total_size}')\n",
    "\n",
    "# Define relevant columns and indices for normalization\n",
    "relevant_columns = [\n",
    "    'Open', 'High', 'Low', 'Close', 'Volume', 'Turnover', 'Consol_Detected',\n",
    "    'Consol_Len_Bars', 'Consol_Depth_Percent', 'Close_21_bar_ema',\n",
    "    'Close_50_bar_sma', 'Close_150_bar_sma', 'Close_200_bar_sma',\n",
    "    'RSL', 'RSL_NH'\n",
    "]\n",
    "\n",
    "price_columns_indices = [0, 1, 2, 3, 9, 10, 11, 12]  # Indices of price-related columns in the sequence data\n",
    "\n",
    "# Map indices to column names\n",
    "price_columns = [relevant_columns[i] for i in price_columns_indices]\n",
    "\n",
    "print(\"\\nPrice-related columns:\")\n",
    "for index, column in zip(price_columns_indices, price_columns):\n",
    "    print(f\"Index: {index}, Column: {column}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "### NaN Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace all NaNs with 0 due to moving averages having insufficient data to compute anything, leaving blank inputs.\n",
    "# Check if NaNs exist\n",
    "\n",
    "# Dictionary to map variable names to their corresponding data arrays\n",
    "data_dict = {\n",
    "    'data_sequences': data_sequences,\n",
    "    'data_labels': data_labels,\n",
    "}\n",
    "\n",
    "# Using a dictionary to iterate over variables\n",
    "for var_name, data in data_dict.items():\n",
    "    num_nans = np.sum(np.isnan(data))\n",
    "    print(f\"NaNs in {var_name}: {num_nans}\")\n",
    "\n",
    "    # Remove NaNs\n",
    "    if num_nans > 0:\n",
    "        data_dict[var_name][:] = np.nan_to_num(data)\n",
    "        num_nans = np.sum(np.isnan(data))\n",
    "        print(f\"NaNs remaining in {var_name} after removal: {num_nans}\")\n",
    "\n",
    "print(f\"Data Seq Min: {np.min(data_sequences[:,:,price_columns_indices])}\")\n",
    "print(f\"Data Seq Max: {np.max(data_sequences[:,:,price_columns_indices])}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Corrupted sequence removal\n",
    "\n",
    "99% of stocks I buy will be below 1000, with a few above 1000, although they are important.\n",
    "\n",
    "I also noticed quite a few training examples have weird price data, which I filter out below.\n",
    "\n",
    "I noticed with thresholds above 3e3, the max is the threshold, which is very suspect.\n",
    "\n",
    "The loss of training examples is insignificant, and the result is better normalization of the data and obviously no corrupted sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the threshold for abnormal values based on domain knowledge\n",
    "threshold = 3.0e3\n",
    "\n",
    "# Detect all sequences with abnormally large price data\n",
    "abnormal_sequences = []\n",
    "\n",
    "# Iterate through each sequence to check for abnormal values\n",
    "for sequence_index in range(data_sequences.shape[0]):\n",
    "    # Extract price-related columns for the current sequence\n",
    "    price_data = data_sequences[sequence_index, :, price_columns_indices]\n",
    "    \n",
    "    # Check if any value in the price_data exceeds the threshold\n",
    "    if np.any(price_data > threshold):\n",
    "        abnormal_sequences.append(sequence_index)\n",
    "\n",
    "# Print the indices of the abnormal sequences\n",
    "print(f\"Abnormal Sequence Count: {len(abnormal_sequences)}\")\n",
    "print(f\"Indices of abnormal sequences: {abnormal_sequences}\")\n",
    "\n",
    "# Create a mask for sequences that are not abnormal\n",
    "mask = np.ones(data_sequences.shape[0], dtype=bool)\n",
    "mask[abnormal_sequences] = False\n",
    "\n",
    "# Filter out abnormal sequences from data_sequences and data_labels\n",
    "filtered_data_sequences = data_sequences[mask]\n",
    "filtered_data_labels = data_labels[mask]\n",
    "\n",
    "# Print the shape of the filtered data\n",
    "print(f\"Filtered data_sequences shape: {filtered_data_sequences.shape}\")\n",
    "print(f\"Filtered data_labels shape: {filtered_data_labels.shape}\")\n",
    "\n",
    "print(f\"Data Seq Min: {np.min(filtered_data_sequences[:,:,price_columns_indices])}\")\n",
    "print(f\"Data Seq Max: {np.max(filtered_data_sequences[:,:,price_columns_indices])}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization of Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Indices of price-related columns\n",
    "price_columns_indices = [0, 1, 2, 3]\n",
    "ma_columns_indices = [9, 10, 11, 12]\n",
    "\n",
    "# Extract data shapes\n",
    "num_sequences, num_timesteps, num_features = data_sequences.shape\n",
    "\n",
    "# Calculate log transformation for price-related features\n",
    "price_data = data_sequences[:, :, price_columns_indices]\n",
    "log_price_data = np.log(price_data + 1e-8)\n",
    "\n",
    "# Replace the original price-related features with the log values\n",
    "data_sequences[:, :, price_columns_indices] = log_price_data\n",
    "\n",
    "# Calculate percentage away from the Close price for moving averages\n",
    "close_price_data = data_sequences[:, :, 3].reshape(num_sequences, num_timesteps, 1)  # Close price at index 3\n",
    "ma_data = data_sequences[:, :, ma_columns_indices]\n",
    "\n",
    "# Avoid division by zero by adding epsilon\n",
    "epsilon = 1e-8\n",
    "percentage_away_from_close = (close_price_data - ma_data) / (ma_data + epsilon)\n",
    "\n",
    "# Replace the original moving average features with the percentage away values\n",
    "data_sequences[:, :, ma_columns_indices] = percentage_away_from_close\n",
    "\n",
    "# Handle infinite values by replacing them with NaNs and then replacing NaNs with zero\n",
    "data_sequences = np.nan_to_num(data_sequences, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "\n",
    "# Clip extreme values to avoid large outliers\n",
    "data_sequences = np.clip(data_sequences, -1e3, 1e3)\n",
    "\n",
    "# Normalize the price-related features together\n",
    "price_scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "\n",
    "# Reshape the price-related features to fit the scaler\n",
    "original_shape = data_sequences[:, :, price_columns_indices].shape\n",
    "reshaped_data = data_sequences[:, :, price_columns_indices].reshape(-1, len(price_columns_indices))\n",
    "\n",
    "# Fit and transform the price-related features\n",
    "normalized_price_data = price_scaler.fit_transform(reshaped_data)\n",
    "\n",
    "# Reshape back to the original shape\n",
    "normalized_price_data = normalized_price_data.reshape(original_shape)\n",
    "\n",
    "# Replace the original price-related features with the normalized ones\n",
    "data_sequences[:, :, price_columns_indices] = normalized_price_data\n",
    "\n",
    "# Normalize the remaining features individually\n",
    "for feature_index in range(num_features):\n",
    "    if feature_index not in price_columns_indices and feature_index not in ma_columns_indices:\n",
    "        # Initialize a new scaler for each feature\n",
    "        feature_scaler = MinMaxScaler()\n",
    "\n",
    "        # Extract the feature data\n",
    "        feature_data = data_sequences[:, :, feature_index].reshape(-1, 1)\n",
    "\n",
    "        # Fit and transform the scaler\n",
    "        normalized_feature_data = feature_scaler.fit_transform(feature_data)\n",
    "\n",
    "        # Reshape back to the original shape\n",
    "        normalized_feature_data = normalized_feature_data.reshape(num_sequences, num_timesteps)\n",
    "\n",
    "        # Replace the original feature with the normalized one\n",
    "        data_sequences[:, :, feature_index] = normalized_feature_data\n",
    "\n",
    "# Print normalized data sequences to check\n",
    "print(f\"Normalized Data Seq Min: {np.min(data_sequences)}\")\n",
    "print(f\"Normalized Data Seq Max: {np.max(data_sequences)}\")\n",
    "\n",
    "# Make the labels a binary decision, rather than a profit\n",
    "min_profit = 0.1  # implies a good decision is a breakout that produces more than min_profit (*100 for percent, 0.2 = 20%)\n",
    "\n",
    "data_labels = (data_labels > min_profit).astype(int)\n",
    "\n",
    "print(f\"Data Labels for > {min_profit*100}% Min: {np.min(data_labels)}\")\n",
    "print(f\"Data Labels for > {min_profit*100}% Max: {np.max(data_labels)}\")\n",
    "\n",
    "# Count how many labels are 1 and how many are 0\n",
    "num_ones = np.sum(data_labels)\n",
    "num_zeros = len(data_labels) - num_ones\n",
    "\n",
    "print(f\"Number of labels that are 1: {num_ones}\")\n",
    "print(f\"Number of labels that are 0: {num_zeros}\")\n",
    "print(f\"Probability of randomly selecting a stock making {min_profit*100}% is {num_ones/(num_ones+num_zeros)*100}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model -> Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
