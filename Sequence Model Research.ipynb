{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequence Model Research\n",
    "\n",
    "The scope of this notebook is to assess and train different sequence models given the training data generated.\n",
    "\n",
    "Training data is generated based on financial time series data labeled with potential profits using a buy-sell system.\n",
    "\n",
    "The goal is to create a sequence model that can choose favourable stock charts equal to or better than a human can via traditional technical analysis.\n",
    "\n",
    "# Import Libraries and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded sequences shape: (115000, 63, 12)\n",
      "Loaded sequences size: 86940000\n",
      "Loaded labels shape: (115000,)\n",
      "Loaded metadata shape: (115000, 2)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# Define the data directory relative to the script location\n",
    "data_dir = 'data'\n",
    "\n",
    "# Define the file paths\n",
    "sequences_path = os.path.join(data_dir, 'sequences.npy')\n",
    "labels_path = os.path.join(data_dir, 'labels.npy')\n",
    "metadata_path = os.path.join(data_dir, 'metadata.npy')\n",
    "\n",
    "# List of feature names\n",
    "feature_names = [\n",
    "    'Consol_Len_Bars', 'Consol_Depth_Percent',\n",
    "    'Distance_to_21EMA', 'Distance_to_50SMA', 'Distance_to_200SMA', \n",
    "    'RSL_NH_Count', 'RSL_Slope', 'Up_Down_Days', \n",
    "    'Stage 2', 'UpDownVolumeRatio', 'ATR', '%B'\n",
    "]\n",
    "\n",
    "# Load the data\n",
    "try:\n",
    "    data_sequences = np.load(sequences_path)\n",
    "    data_labels = np.load(labels_path)\n",
    "    data_metadata = np.load(metadata_path)\n",
    "\n",
    "    # Number of examples to select\n",
    "    num_examples = 115000\n",
    "\n",
    "    # Generate a random permutation of indices\n",
    "    indices = np.random.permutation(len(data_sequences))\n",
    "\n",
    "    # Select the first `num_examples` indices\n",
    "    selected_indices = indices[:num_examples]\n",
    "\n",
    "    # Use the selected indices to create the random subset\n",
    "    data_sequences = data_sequences[selected_indices, :, :]\n",
    "    data_labels = data_labels[selected_indices]\n",
    "    data_metadata = data_metadata[selected_indices]\n",
    "\n",
    "    # Inspect the shape and size of the loaded data before slicing\n",
    "    print(f'Loaded sequences shape: {data_sequences.shape}')\n",
    "    print(f'Loaded sequences size: {data_sequences.size}')\n",
    "    print(f'Loaded labels shape: {data_labels.shape}')\n",
    "    print(f'Loaded metadata shape: {data_metadata.shape}')\n",
    "\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Error loading files: {e}\")\n",
    "except ValueError as e:\n",
    "    print(f\"Value error: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "### NaN anf INF Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaNs in data_sequences: 904983\n",
      "Infs in data_sequences: 33\n",
      "NaNs remaining in data_sequences after removal: 0\n",
      "Infs remaining in data_sequences after removal: 0\n",
      "NaNs in data_labels: 0\n",
      "Infs in data_labels: 0\n",
      "NaN and Inf removal completed.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Dictionary to map variable names to their corresponding data arrays\n",
    "data_dict = {\n",
    "    'data_sequences': data_sequences,\n",
    "    'data_labels': data_labels,\n",
    "}\n",
    "\n",
    "# Using a dictionary to iterate over variables\n",
    "for var_name, data in data_dict.items():\n",
    "    num_nans = np.sum(np.isnan(data))\n",
    "    num_infs = np.sum(np.isinf(data))\n",
    "    print(f\"NaNs in {var_name}: {num_nans}\")\n",
    "    print(f\"Infs in {var_name}: {num_infs}\")\n",
    "\n",
    "    # Remove NaNs and Infs\n",
    "    if num_nans > 0 or num_infs > 0:\n",
    "        data_dict[var_name][:] = np.nan_to_num(data, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "        num_nans_after = np.sum(np.isnan(data))\n",
    "        num_infs_after = np.sum(np.isinf(data))\n",
    "        print(f\"NaNs remaining in {var_name} after removal: {num_nans_after}\")\n",
    "        print(f\"Infs remaining in {var_name} after removal: {num_infs_after}\")\n",
    "\n",
    "print(\"NaN and Inf removal completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Corrupted sequence removal\n",
    "\n",
    "99% of stocks I buy will be below 1000, with a few above 1000, although they are important.\n",
    "\n",
    "I also noticed quite a few training examples have weird price data, which I filter out below.\n",
    "\n",
    "I noticed with thresholds above 3e3, the max is the threshold, which is very suspect.\n",
    "\n",
    "The loss of training examples is insignificant, and the result is better normalization of the data and obviously no corrupted sequences.\n",
    "\n",
    "#### Feature Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Statistics:\n",
      "--------------------------------------------------\n",
      "Feature: Consol_Len_Bars\n",
      "  Mean: 69.4729\n",
      "  Median: 27.0000\n",
      "  Std Dev: 121.2268\n",
      "  Min: 0.0000\n",
      "  Max: 3142.0000\n",
      "  25th Percentile: 6.0000\n",
      "  75th Percentile: 77.0000\n",
      "  Skewness: 4.4634\n",
      "  Kurtosis: 37.2080\n",
      "  Zero Count: 1416240.0000\n",
      "  Zero Percentage: 19.5478\n",
      "--------------------------------------------------\n",
      "Feature: Consol_Depth_Percent\n",
      "  Mean: 16.4673\n",
      "  Median: 15.6804\n",
      "  Std Dev: 11.7338\n",
      "  Min: 0.0000\n",
      "  Max: 54.4286\n",
      "  25th Percentile: 7.3934\n",
      "  75th Percentile: 25.9320\n",
      "  Skewness: 0.2050\n",
      "  Kurtosis: -0.8840\n",
      "  Zero Count: 1340533.0000\n",
      "  Zero Percentage: 18.5029\n",
      "--------------------------------------------------\n",
      "Feature: Distance_to_21EMA\n",
      "  Mean: 0.8504\n",
      "  Median: 0.8903\n",
      "  Std Dev: 5.5301\n",
      "  Min: -99.9886\n",
      "  Max: 712.1856\n",
      "  25th Percentile: -1.2132\n",
      "  75th Percentile: 3.0191\n",
      "  Skewness: 1.0681\n",
      "  Kurtosis: 95.0132\n",
      "  Zero Count: 17874.0000\n",
      "  Zero Percentage: 0.2467\n",
      "--------------------------------------------------\n",
      "Feature: Distance_to_50SMA\n",
      "  Mean: 2.0727\n",
      "  Median: 1.9102\n",
      "  Std Dev: 10.0856\n",
      "  Min: -99.9896\n",
      "  Max: 1092.4742\n",
      "  25th Percentile: -1.5319\n",
      "  75th Percentile: 5.9305\n",
      "  Skewness: 1.5870\n",
      "  Kurtosis: 72.8593\n",
      "  Zero Count: 119241.0000\n",
      "  Zero Percentage: 1.6458\n",
      "--------------------------------------------------\n",
      "Feature: Distance_to_200SMA\n",
      "  Mean: 5.5681\n",
      "  Median: 5.5032\n",
      "  Std Dev: 21.3108\n",
      "  Min: -98.0358\n",
      "  Max: 2523.0145\n",
      "  25th Percentile: -0.1621\n",
      "  75th Percentile: 13.8508\n",
      "  Skewness: 2.3063\n",
      "  Kurtosis: 114.4534\n",
      "  Zero Count: 536617.0000\n",
      "  Zero Percentage: 7.4067\n",
      "--------------------------------------------------\n",
      "Feature: RSL_NH_Count\n",
      "  Mean: 115.9196\n",
      "  Median: 94.0000\n",
      "  Std Dev: 99.1641\n",
      "  Min: 0.0000\n",
      "  Max: 1253.0000\n",
      "  25th Percentile: 32.0000\n",
      "  75th Percentile: 180.0000\n",
      "  Skewness: 1.0307\n",
      "  Kurtosis: 1.8597\n",
      "  Zero Count: 473995.0000\n",
      "  Zero Percentage: 6.5424\n",
      "--------------------------------------------------\n",
      "Feature: RSL_Slope\n",
      "  Mean: 0.0000\n",
      "  Median: 0.0000\n",
      "  Std Dev: 0.0004\n",
      "  Min: -0.1085\n",
      "  Max: 0.0804\n",
      "  25th Percentile: -0.0000\n",
      "  75th Percentile: 0.0000\n",
      "  Skewness: -12.0752\n",
      "  Kurtosis: 6336.7356\n",
      "  Zero Count: 125795.0000\n",
      "  Zero Percentage: 1.7363\n",
      "--------------------------------------------------\n",
      "Feature: Up_Down_Days\n",
      "  Mean: 0.7128\n",
      "  Median: 0.0000\n",
      "  Std Dev: 3.4856\n",
      "  Min: -14.0000\n",
      "  Max: 14.0000\n",
      "  25th Percentile: -2.0000\n",
      "  75th Percentile: 3.0000\n",
      "  Skewness: 0.0044\n",
      "  Kurtosis: -0.0727\n",
      "  Zero Count: 1332790.0000\n",
      "  Zero Percentage: 18.3960\n",
      "--------------------------------------------------\n",
      "Feature: Stage 2\n",
      "  Mean: 0.3486\n",
      "  Median: 0.0000\n",
      "  Std Dev: 0.4765\n",
      "  Min: 0.0000\n",
      "  Max: 1.0000\n",
      "  25th Percentile: 0.0000\n",
      "  75th Percentile: 1.0000\n",
      "  Skewness: 0.6356\n",
      "  Kurtosis: -1.5960\n",
      "  Zero Count: 4719638.0000\n",
      "  Zero Percentage: 65.1434\n",
      "--------------------------------------------------\n",
      "Feature: UpDownVolumeRatio\n",
      "  Mean: 1.2821\n",
      "  Median: 1.1213\n",
      "  Std Dev: 8.8197\n",
      "  Min: 0.0000\n",
      "  Max: 9897.7948\n",
      "  25th Percentile: 0.8672\n",
      "  75th Percentile: 1.4420\n",
      "  Skewness: 823.3607\n",
      "  Kurtosis: 838069.3112\n",
      "  Zero Count: 119241.0000\n",
      "  Zero Percentage: 1.6458\n",
      "--------------------------------------------------\n",
      "Feature: ATR\n",
      "  Mean: 1.1288\n",
      "  Median: 0.4657\n",
      "  Std Dev: 3.2608\n",
      "  Min: 0.0000\n",
      "  Max: 311.1429\n",
      "  25th Percentile: 0.2195\n",
      "  75th Percentile: 0.9918\n",
      "  Skewness: 15.9599\n",
      "  Kurtosis: 471.2494\n",
      "  Zero Count: 35821.0000\n",
      "  Zero Percentage: 0.4944\n",
      "--------------------------------------------------\n",
      "Feature: %B\n",
      "  Mean: 0.5695\n",
      "  Median: 0.6076\n",
      "  Std Dev: 0.3137\n",
      "  Min: -0.5621\n",
      "  Max: 1.5611\n",
      "  25th Percentile: 0.3294\n",
      "  75th Percentile: 0.8153\n",
      "  Skewness: -0.2710\n",
      "  Kurtosis: -0.6336\n",
      "  Zero Count: 47323.0000\n",
      "  Zero Percentage: 0.6532\n",
      "--------------------------------------------------\n",
      "Overall Dataset Statistics:\n",
      "Total number of sequences: 115000\n",
      "Sequence length: 63\n",
      "Number of features: 12\n",
      "Total number of data points: 86940000\n",
      "Memory usage: 663.30 MB\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def print_feature_stats(data_sequences, feature_names):\n",
    "    print(\"Feature Statistics:\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    for i, feature_name in enumerate(feature_names):\n",
    "        feature_data = data_sequences[:, :, i].flatten()\n",
    "        \n",
    "        stats = {\n",
    "            \"Mean\": np.mean(feature_data),\n",
    "            \"Median\": np.median(feature_data),\n",
    "            \"Std Dev\": np.std(feature_data),\n",
    "            \"Min\": np.min(feature_data),\n",
    "            \"Max\": np.max(feature_data),\n",
    "            \"25th Percentile\": np.percentile(feature_data, 25),\n",
    "            \"75th Percentile\": np.percentile(feature_data, 75),\n",
    "            \"Skewness\": pd.Series(feature_data).skew(),\n",
    "            \"Kurtosis\": pd.Series(feature_data).kurtosis(),\n",
    "            \"Zero Count\": np.sum(feature_data == 0),\n",
    "            \"Zero Percentage\": np.mean(feature_data == 0) * 100\n",
    "        }\n",
    "        \n",
    "        print(f\"Feature: {feature_name}\")\n",
    "        for stat_name, stat_value in stats.items():\n",
    "            print(f\"  {stat_name}: {stat_value:.4f}\")\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "# Call the function to print statistics\n",
    "print_feature_stats(data_sequences, feature_names)\n",
    "\n",
    "# Additional overall statistics\n",
    "print(\"Overall Dataset Statistics:\")\n",
    "print(f\"Total number of sequences: {data_sequences.shape[0]}\")\n",
    "print(f\"Sequence length: {data_sequences.shape[1]}\")\n",
    "print(f\"Number of features: {data_sequences.shape[2]}\")\n",
    "print(f\"Total number of data points: {data_sequences.size}\")\n",
    "print(f\"Memory usage: {data_sequences.nbytes / (1024 * 1024):.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization of Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Define the indices based on the provided feature names\n",
    "feature_names = [\n",
    "    'Consol_Len_Bars', 'Consol_Depth_Percent',\n",
    "    'Distance_to_21EMA', 'Distance_to_50SMA', 'Distance_to_200SMA', \n",
    "    'RSL_NH_Count', 'RSL_Slope', 'Up_Down_Days', \n",
    "    'Stage 2', 'UpDownVolumeRatio', 'ATR', '%B'\n",
    "]\n",
    "\n",
    "feature_indices = {name: idx for idx, name in enumerate(feature_names)}\n",
    "\n",
    "# Function to remove outliers and cap values\n",
    "def preprocess_data(sequences, labels):\n",
    "    # Reshape sequences to 2D array for easier processing (flatten the timesteps)\n",
    "    num_sequences, num_timesteps, num_features = sequences.shape\n",
    "    sequences_reshaped = sequences.reshape(-1, num_features)\n",
    "    \n",
    "    # Create a mask to filter out invalid sequences\n",
    "    valid_mask = (\n",
    "        (sequences_reshaped[:, feature_indices['Distance_to_21EMA']] <= 100) &\n",
    "        (sequences_reshaped[:, feature_indices['Distance_to_50SMA']] <= 200) &\n",
    "        (sequences_reshaped[:, feature_indices['Distance_to_200SMA']] <= 500)\n",
    "    )\n",
    "    \n",
    "    # Reshape the valid_mask to match the original sequence shape\n",
    "    valid_mask_reshaped = valid_mask.reshape(num_sequences, num_timesteps)\n",
    "    \n",
    "    # Filter out sequences with any invalid timesteps\n",
    "    valid_sequences_mask = valid_mask_reshaped.all(axis=1)\n",
    "    filtered_sequences = sequences[valid_sequences_mask]\n",
    "    filtered_labels = labels[valid_sequences_mask]\n",
    "    \n",
    "    # Cap 'UpDownVolumeRatio' at 10\n",
    "    filtered_sequences[:, :, feature_indices['UpDownVolumeRatio']] = np.minimum(\n",
    "        filtered_sequences[:, :, feature_indices['UpDownVolumeRatio']], 10\n",
    "    )\n",
    "    \n",
    "    # Normalize the features using Z-score normalization\n",
    "    scaler = StandardScaler()\n",
    "    \n",
    "    # Flatten the sequence again for normalization\n",
    "    filtered_sequences_reshaped = filtered_sequences.reshape(-1, num_features)\n",
    "    \n",
    "    # Normalize\n",
    "    normalized_data_reshaped = scaler.fit_transform(filtered_sequences_reshaped)\n",
    "    \n",
    "    # Reshape back to the original 3D shape\n",
    "    normalized_data = normalized_data_reshaped.reshape(filtered_sequences.shape)\n",
    "    \n",
    "    return normalized_data, filtered_labels\n",
    "\n",
    "# Function to print feature statistics\n",
    "def print_feature_stats(data_sequences, feature_names):\n",
    "    print(\"Feature Statistics:\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    for i, feature_name in enumerate(feature_names):\n",
    "        feature_data = data_sequences[:, :, i].flatten()\n",
    "        \n",
    "        stats = {\n",
    "            \"Mean\": np.mean(feature_data),\n",
    "            \"Median\": np.median(feature_data),\n",
    "            \"Std Dev\": np.std(feature_data),\n",
    "            \"Min\": np.min(feature_data),\n",
    "            \"Max\": np.max(feature_data),\n",
    "            \"25th Percentile\": np.percentile(feature_data, 25),\n",
    "            \"75th Percentile\": np.percentile(feature_data, 75),\n",
    "            \"Skewness\": pd.Series(feature_data).skew(),\n",
    "            \"Kurtosis\": pd.Series(feature_data).kurtosis(),\n",
    "            \"Zero Count\": np.sum(feature_data == 0),\n",
    "            \"Zero Percentage\": np.mean(feature_data == 0) * 100\n",
    "        }\n",
    "        \n",
    "        print(f\"Feature: {feature_name}\")\n",
    "        for stat_name, stat_value in stats.items():\n",
    "            print(f\"  {stat_name}: {stat_value:.4f}\")\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "# Example usage with data_sequences and data_labels\n",
    "# Assuming data_sequences is loaded and has shape (115000, 63, 12)\n",
    "\n",
    "# Process the data\n",
    "normalized_data, processed_labels = preprocess_data(data_sequences, data_labels)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stats again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Statistics:\n",
      "--------------------------------------------------\n",
      "Feature: Consol_Len_Bars\n",
      "  Mean: 0.0000\n",
      "  Median: -0.3506\n",
      "  Std Dev: 1.0000\n",
      "  Min: -0.5733\n",
      "  Max: 25.3382\n",
      "  25th Percentile: -0.5238\n",
      "  75th Percentile: 0.0617\n",
      "  Skewness: 4.4621\n",
      "  Kurtosis: 37.1883\n",
      "  Zero Count: 0.0000\n",
      "  Zero Percentage: 0.0000\n",
      "--------------------------------------------------\n",
      "Feature: Consol_Depth_Percent\n",
      "  Mean: 0.0000\n",
      "  Median: -0.0671\n",
      "  Std Dev: 1.0000\n",
      "  Min: -1.4040\n",
      "  Max: 3.2357\n",
      "  25th Percentile: -0.7730\n",
      "  75th Percentile: 0.8062\n",
      "  Skewness: 0.2051\n",
      "  Kurtosis: -0.8829\n",
      "  Zero Count: 0.0000\n",
      "  Zero Percentage: 0.0000\n",
      "--------------------------------------------------\n",
      "Feature: Distance_to_21EMA\n",
      "  Mean: -0.0000\n",
      "  Median: 0.0091\n",
      "  Std Dev: 1.0000\n",
      "  Min: -18.5588\n",
      "  Max: 18.0986\n",
      "  25th Percentile: -0.3779\n",
      "  75th Percentile: 0.4006\n",
      "  Skewness: -0.1968\n",
      "  Kurtosis: 14.0993\n",
      "  Zero Count: 0.0000\n",
      "  Zero Percentage: 0.0000\n",
      "--------------------------------------------------\n",
      "Feature: Distance_to_50SMA\n",
      "  Mean: 0.0000\n",
      "  Median: -0.0140\n",
      "  Std Dev: 1.0000\n",
      "  Min: -10.3287\n",
      "  Max: 18.7984\n",
      "  25th Percentile: -0.3624\n",
      "  75th Percentile: 0.3925\n",
      "  Skewness: 0.3989\n",
      "  Kurtosis: 10.9574\n",
      "  Zero Count: 0.0000\n",
      "  Zero Percentage: 0.0000\n",
      "--------------------------------------------------\n",
      "Feature: Distance_to_200SMA\n",
      "  Mean: 0.0000\n",
      "  Median: -0.0008\n",
      "  Std Dev: 1.0000\n",
      "  Min: -4.9658\n",
      "  Max: 23.2765\n",
      "  25th Percentile: -0.2724\n",
      "  75th Percentile: 0.3991\n",
      "  Skewness: 0.7887\n",
      "  Kurtosis: 9.6825\n",
      "  Zero Count: 0.0000\n",
      "  Zero Percentage: 0.0000\n",
      "--------------------------------------------------\n",
      "Feature: RSL_NH_Count\n",
      "  Mean: 0.0000\n",
      "  Median: -0.2215\n",
      "  Std Dev: 1.0000\n",
      "  Min: -1.1694\n",
      "  Max: 11.4654\n",
      "  25th Percentile: -0.8467\n",
      "  75th Percentile: 0.6457\n",
      "  Skewness: 1.0303\n",
      "  Kurtosis: 1.8594\n",
      "  Zero Count: 0.0000\n",
      "  Zero Percentage: 0.0000\n",
      "--------------------------------------------------\n",
      "Feature: RSL_Slope\n",
      "  Mean: 0.0000\n",
      "  Median: -0.0170\n",
      "  Std Dev: 1.0000\n",
      "  Min: -296.5560\n",
      "  Max: 219.7486\n",
      "  25th Percentile: -0.0646\n",
      "  75th Percentile: 0.0500\n",
      "  Skewness: -12.5193\n",
      "  Kurtosis: 6430.5240\n",
      "  Zero Count: 0.0000\n",
      "  Zero Percentage: 0.0000\n",
      "--------------------------------------------------\n",
      "Feature: Up_Down_Days\n",
      "  Mean: 0.0000\n",
      "  Median: -0.2046\n",
      "  Std Dev: 1.0000\n",
      "  Min: -4.2212\n",
      "  Max: 3.8120\n",
      "  25th Percentile: -0.7784\n",
      "  75th Percentile: 0.6561\n",
      "  Skewness: 0.0044\n",
      "  Kurtosis: -0.0727\n",
      "  Zero Count: 0.0000\n",
      "  Zero Percentage: 0.0000\n",
      "--------------------------------------------------\n",
      "Feature: Stage 2\n",
      "  Mean: -0.0000\n",
      "  Median: -0.7317\n",
      "  Std Dev: 1.0000\n",
      "  Min: -0.7317\n",
      "  Max: 1.3668\n",
      "  25th Percentile: -0.7317\n",
      "  75th Percentile: 1.3668\n",
      "  Skewness: 0.6351\n",
      "  Kurtosis: -1.5966\n",
      "  Zero Count: 0.0000\n",
      "  Zero Percentage: 0.0000\n",
      "--------------------------------------------------\n",
      "Feature: UpDownVolumeRatio\n",
      "  Mean: -0.0000\n",
      "  Median: -0.1565\n",
      "  Std Dev: 1.0000\n",
      "  Min: -1.6561\n",
      "  Max: 11.7189\n",
      "  25th Percentile: -0.4962\n",
      "  75th Percentile: 0.2719\n",
      "  Skewness: 4.9902\n",
      "  Kurtosis: 45.5447\n",
      "  Zero Count: 0.0000\n",
      "  Zero Percentage: 0.0000\n",
      "--------------------------------------------------\n",
      "Feature: ATR\n",
      "  Mean: -0.0000\n",
      "  Median: -0.2034\n",
      "  Std Dev: 1.0000\n",
      "  Min: -0.3465\n",
      "  Max: 95.3309\n",
      "  25th Percentile: -0.2790\n",
      "  75th Percentile: -0.0417\n",
      "  Skewness: 16.0078\n",
      "  Kurtosis: 474.8149\n",
      "  Zero Count: 0.0000\n",
      "  Zero Percentage: 0.0000\n",
      "--------------------------------------------------\n",
      "Feature: %B\n",
      "  Mean: 0.0000\n",
      "  Median: 0.1214\n",
      "  Std Dev: 1.0000\n",
      "  Min: -3.6073\n",
      "  Max: 3.1602\n",
      "  25th Percentile: -0.7655\n",
      "  75th Percentile: 0.7835\n",
      "  Skewness: -0.2713\n",
      "  Kurtosis: -0.6341\n",
      "  Zero Count: 0.0000\n",
      "  Zero Percentage: 0.0000\n",
      "--------------------------------------------------\n",
      "Overall Dataset Statistics:\n",
      "Total number of sequences: 115000\n",
      "Sequence length: 63\n",
      "Number of features: 12\n",
      "Total number of data points: 86940000\n",
      "Memory usage: 663.30 MB\n"
     ]
    }
   ],
   "source": [
    "# Print statistics\n",
    "print_feature_stats(normalized_data, feature_names)\n",
    "\n",
    "# Additional overall statistics\n",
    "print(\"Overall Dataset Statistics:\")\n",
    "print(f\"Total number of sequences: {data_sequences.shape[0]}\")\n",
    "print(f\"Sequence length: {data_sequences.shape[1]}\")\n",
    "print(f\"Number of features: {data_sequences.shape[2]}\")\n",
    "print(f\"Total number of data points: {data_sequences.size}\")\n",
    "print(f\"Memory usage: {data_sequences.nbytes / (1024 * 1024):.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Definition\n",
    "\n",
    "## Model Class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HP Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-29 12:36:50,815] A new study created in memory with name: no-name-bb33d8e3-bc2b-4ebf-89d5-c2b84302c01d\n",
      "2024/06/29 12:48:57 WARNING mlflow.utils.requirements_utils: Found torch version (2.3.1+cu118) contains a local version label (+cu118). MLflow logged a pip requirement for this package as 'torch==2.3.1' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n",
      "2024/06/29 12:49:02 WARNING mlflow.utils.requirements_utils: Found torch version (2.3.1+cu118) contains a local version label (+cu118). MLflow logged a pip requirement for this package as 'torch==2.3.1' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n",
      "[I 2024-06-29 12:49:03,033] Trial 0 finished with value: 0.1447596988998263 and parameters: {'PROFIT_THRESH': 0.023150876110827015, 'LEARNING_RATE': 0.0010222049641216344, 'DROPOUT_RATE_LSTM': 0.15407138296264156, 'DROPOUT_RATE_LINEAR': 0.14224996926224467, 'L2_REGULARIZATION': 6.622362238404638e-05, 'PREDICTION_THRESHOLD': 0.7560477604199054, 'LSTM_LAYERS': '64', 'LINEAR_LAYERS': '32,16', 'BATCH_SIZE': 32, 'SKIP_CONNECTIONS': True}. Best is trial 0 with value: 0.1447596988998263.\n",
      "2024/06/29 13:12:48 WARNING mlflow.utils.requirements_utils: Found torch version (2.3.1+cu118) contains a local version label (+cu118). MLflow logged a pip requirement for this package as 'torch==2.3.1' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n",
      "2024/06/29 13:12:51 WARNING mlflow.utils.requirements_utils: Found torch version (2.3.1+cu118) contains a local version label (+cu118). MLflow logged a pip requirement for this package as 'torch==2.3.1' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n",
      "c:\\Users\\admin\\GitHub\\TradeTronic\\.venv\\lib\\site-packages\\_distutils_hack\\__init__.py:18: UserWarning: Distutils was imported before Setuptools, but importing Setuptools also replaces the `distutils` module in `sys.modules`. This may lead to undesirable behaviors or errors. To avoid these issues, avoid using distutils directly, ensure that setuptools is installed in the traditional way (e.g. not an editable install), and/or make sure that setuptools is always imported before distutils.\n",
      "  warnings.warn(\n",
      "c:\\Users\\admin\\GitHub\\TradeTronic\\.venv\\lib\\site-packages\\_distutils_hack\\__init__.py:33: UserWarning: Setuptools is replacing distutils.\n",
      "  warnings.warn(\"Setuptools is replacing distutils.\")\n",
      "[I 2024-06-29 13:12:51,859] Trial 1 finished with value: 0.40064077518168323 and parameters: {'PROFIT_THRESH': 0.06695732950977802, 'LEARNING_RATE': 0.00022522170251920855, 'DROPOUT_RATE_LSTM': 0.37186109772199216, 'DROPOUT_RATE_LINEAR': 0.19082263840371635, 'L2_REGULARIZATION': 0.0004900910988285004, 'PREDICTION_THRESHOLD': 0.31667228066604947, 'LSTM_LAYERS': '128,64', 'LINEAR_LAYERS': '64,32', 'BATCH_SIZE': 32, 'SKIP_CONNECTIONS': True}. Best is trial 1 with value: 0.40064077518168323.\n",
      "2024/06/29 13:36:48 WARNING mlflow.utils.requirements_utils: Found torch version (2.3.1+cu118) contains a local version label (+cu118). MLflow logged a pip requirement for this package as 'torch==2.3.1' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n",
      "2024/06/29 13:36:51 WARNING mlflow.utils.requirements_utils: Found torch version (2.3.1+cu118) contains a local version label (+cu118). MLflow logged a pip requirement for this package as 'torch==2.3.1' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n",
      "c:\\Users\\admin\\GitHub\\TradeTronic\\.venv\\lib\\site-packages\\_distutils_hack\\__init__.py:18: UserWarning: Distutils was imported before Setuptools, but importing Setuptools also replaces the `distutils` module in `sys.modules`. This may lead to undesirable behaviors or errors. To avoid these issues, avoid using distutils directly, ensure that setuptools is installed in the traditional way (e.g. not an editable install), and/or make sure that setuptools is always imported before distutils.\n",
      "  warnings.warn(\n",
      "c:\\Users\\admin\\GitHub\\TradeTronic\\.venv\\lib\\site-packages\\_distutils_hack\\__init__.py:33: UserWarning: Setuptools is replacing distutils.\n",
      "  warnings.warn(\"Setuptools is replacing distutils.\")\n",
      "[I 2024-06-29 13:36:51,863] Trial 2 finished with value: 0.0 and parameters: {'PROFIT_THRESH': 0.04947070301399295, 'LEARNING_RATE': 0.0004615464752917334, 'DROPOUT_RATE_LSTM': 0.48484016546287645, 'DROPOUT_RATE_LINEAR': 0.3687381356089754, 'L2_REGULARIZATION': 7.5408711469324785e-06, 'PREDICTION_THRESHOLD': 0.156910062486788, 'LSTM_LAYERS': '128,64', 'LINEAR_LAYERS': '64,32', 'BATCH_SIZE': 32, 'SKIP_CONNECTIONS': False}. Best is trial 1 with value: 0.40064077518168323.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score\n",
    "import seaborn as sns\n",
    "import copy\n",
    "import time\n",
    "import optuna\n",
    "import json\n",
    "import logging\n",
    "import mlflow\n",
    "import mlflow.pytorch\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "RANDOM_SEED = 42\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "# Check for CUDA availability\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "logger.info(f\"Using device: {device}\")\n",
    "\n",
    "# Assuming normalized_data and processed_labels are defined\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    normalized_data, processed_labels, test_size=0.2, random_state=RANDOM_SEED\n",
    ")\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train_tensor = torch.FloatTensor(X_train).to(device)\n",
    "y_train_tensor = torch.FloatTensor(y_train).to(device)\n",
    "X_test_tensor = torch.FloatTensor(X_test).to(device)\n",
    "y_test_tensor = torch.FloatTensor(y_test).to(device)\n",
    "\n",
    "# Define the model architecture (same as before)\n",
    "class StockAnalysisModel(nn.Module):\n",
    "    # ... (same as before)\n",
    "\n",
    "def save_checkpoint(epoch, model, optimizer, best_f1, params, checkpoint_path):\n",
    "    # ... (same as before)\n",
    "\n",
    "def load_checkpoint(checkpoint_path, model, optimizer):\n",
    "    # ... (same as before)\n",
    "\n",
    "def objective(trial):\n",
    "    # Hyperparameter search space\n",
    "    params = {\n",
    "        'PROFIT_THRESH': trial.suggest_float('PROFIT_THRESH', 0.0, 0.1),\n",
    "        'LEARNING_RATE': trial.suggest_float('LEARNING_RATE', 1e-5, 1e-2, log=True),\n",
    "        'DROPOUT_RATE_LSTM': trial.suggest_float('DROPOUT_RATE_LSTM', 0.1, 0.5),\n",
    "        'DROPOUT_RATE_LINEAR': trial.suggest_float('DROPOUT_RATE_LINEAR', 0.1, 0.5),\n",
    "        'L2_REGULARIZATION': trial.suggest_float('L2_REGULARIZATION', 1e-6, 1e-3, log=True),\n",
    "        'PREDICTION_THRESHOLD': trial.suggest_float('PREDICTION_THRESHOLD', 0.1, 0.9),\n",
    "        'LSTM_LAYERS': trial.suggest_categorical('LSTM_LAYERS', ['64', '128', '64,32', '128,64', '128,64,32']),\n",
    "        'LINEAR_LAYERS': trial.suggest_categorical('LINEAR_LAYERS', ['32', '64', '32,16', '64,32', '64,32,16']),\n",
    "        'BATCH_SIZE': trial.suggest_categorical('BATCH_SIZE', [32, 64, 128, 256]),\n",
    "        'SKIP_CONNECTIONS': trial.suggest_categorical('SKIP_CONNECTIONS', [False, True])\n",
    "    }\n",
    "    \n",
    "    # Set up MLflow\n",
    "    mlflow.set_experiment(\"Stock Analysis Model Tuning\")\n",
    "    with mlflow.start_run():\n",
    "        # Log hyperparameters\n",
    "        mlflow.log_params(params)\n",
    "        \n",
    "        # Prepare data\n",
    "        y_train_binary = (y_train > params['PROFIT_THRESH']).astype(int)\n",
    "        y_test_binary = (y_test > params['PROFIT_THRESH']).astype(int)\n",
    "        \n",
    "        # Log class distribution\n",
    "        train_class_distribution = np.bincount(y_train_binary)\n",
    "        test_class_distribution = np.bincount(y_test_binary)\n",
    "        logger.info(f\"Train class distribution: {train_class_distribution}\")\n",
    "        logger.info(f\"Test class distribution: {test_class_distribution}\")\n",
    "        train_positive_class_percentage = train_class_distribution[1] / len(y_train_binary) * 100\n",
    "        test_positive_class_percentage = test_class_distribution[1] / len(y_test_binary) * 100\n",
    "        mlflow.log_metric(\"train_positive_class_percentage\", train_positive_class_percentage)\n",
    "        mlflow.log_metric(\"test_positive_class_percentage\", test_positive_class_percentage)\n",
    "        logger.info(f\"Train positive class percentage: {train_positive_class_percentage:.2f}%\")\n",
    "        logger.info(f\"Test positive class percentage: {test_positive_class_percentage:.2f}%\")\n",
    "        \n",
    "        # Compute class weights\n",
    "        class_weights = compute_class_weight('balanced', classes=np.unique(y_train_binary), y=y_train_binary)\n",
    "        class_weights = torch.FloatTensor(class_weights).to(device)\n",
    "        logger.info(f\"Class weights: {class_weights}\")\n",
    "        \n",
    "        train_dataset = TensorDataset(X_train_tensor, torch.FloatTensor(y_train_binary).to(device))\n",
    "        train_loader = DataLoader(train_dataset, batch_size=params['BATCH_SIZE'], shuffle=True)\n",
    "        \n",
    "        # Initialize model\n",
    "        model = StockAnalysisModel(\n",
    "            input_size=X_train.shape[2],\n",
    "            lstm_layers=params['LSTM_LAYERS'],\n",
    "            linear_layers=params['LINEAR_LAYERS'],\n",
    "            dropout_rate_lstm=params['DROPOUT_RATE_LSTM'],\n",
    "            dropout_rate_linear=params['DROPOUT_RATE_LINEAR'],\n",
    "            skip_connections=params['SKIP_CONNECTIONS']\n",
    "        ).to(device)\n",
    "        logger.info(f\"Model architecture: {model}\")\n",
    "        \n",
    "        # Define loss function and optimizer\n",
    "        criterion = nn.BCELoss(reduction='none')\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=params['LEARNING_RATE'], weight_decay=params['L2_REGULARIZATION'])\n",
    "        \n",
    "        # Directory to save checkpoints\n",
    "        checkpoint_dir = './checkpoints'\n",
    "        os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "        checkpoint_path = os.path.join(checkpoint_dir, 'model_checkpoint.pth')\n",
    "        start_epoch = 0\n",
    "        best_f1 = 0\n",
    "        previous_params = None\n",
    "\n",
    "        # Load checkpoint if exists\n",
    "        if os.path.exists(checkpoint_path):\n",
    "            try:\n",
    "                start_epoch, best_f1, previous_params = load_checkpoint(checkpoint_path, model, optimizer)\n",
    "                logger.info(f\"Resuming training from epoch {start_epoch} with best F1 score {best_f1:.4f}\")\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Could not load checkpoint: {e}. Starting from scratch.\")\n",
    "\n",
    "        # Ensure model architecture matches\n",
    "        if previous_params and params != previous_params:\n",
    "            logger.info(f\"Model architecture changed, not loading from checkpoint. Previous params: {previous_params}, Current params: {params}\")\n",
    "            start_epoch = 0\n",
    "            best_f1 = 0\n",
    "\n",
    "        # Training loop\n",
    "        n_epochs = 50\n",
    "        patience = 10\n",
    "        no_improve_count = 0\n",
    "        for epoch in range(start_epoch, n_epochs):\n",
    "            model.train()\n",
    "            total_loss = 0\n",
    "            all_train_preds = []\n",
    "            all_train_labels = []\n",
    "            for batch_X, batch_y in train_loader:\n",
    "                optimizer.zero_grad()\n",
    "                try:\n",
    "                    outputs = model(batch_X)\n",
    "                    # Apply class weights manually\n",
    "                    weights = class_weights[batch_y.long()]\n",
    "                    losses = criterion(outputs, batch_y)\n",
    "                    weighted_losses = losses * weights\n",
    "                    loss = weighted_losses.mean()\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    total_loss += loss.item()\n",
    "                    \n",
    "                    all_train_preds.extend(outputs.detach().cpu().numpy())\n",
    "                    all_train_labels.extend(batch_y.cpu().numpy())\n",
    "                except RuntimeError as e:\n",
    "                    logger.error(f\"Error in forward pass: {str(e)}\")\n",
    "                    logger.error(f\"batch_X shape: {batch_X.shape}, batch_y shape: {batch_y.shape}\")\n",
    "                    return 0  # Return 0 to indicate failure to Optuna\n",
    "            \n",
    "            # Calculate training metrics\n",
    "            train_preds_binary = (np.array(all_train_preds) > params['PREDICTION_THRESHOLD']).astype(int)\n",
    "            train_precision = precision_score(all_train_labels, train_preds_binary, zero_division=0)\n",
    "            train_recall = recall_score(all_train_labels, train_preds_binary, zero_division=0)\n",
    "            train_f1 = f1_score(all_train_labels, train_preds_binary, zero_division=0)\n",
    "            \n",
    "            # Validation\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                y_pred = model(X_test_tensor)\n",
    "                y_pred_binary = (y_pred > params['PREDICTION_THRESHOLD']).float()\n",
    "                \n",
    "                # Calculate metrics\n",
    "                precision = precision_score(y_test_binary, y_pred_binary.cpu().numpy(), zero_division=0)\n",
    "                recall = recall_score(y_test_binary, y_pred_binary.cpu().numpy(), zero_division=0)\n",
    "                f1 = f1_score(y_test_binary, y_pred_binary.cpu().numpy(), zero_division=0)\n",
    "                \n",
    "                logger.info(f\"Epoch {epoch}: Loss: {total_loss/len(train_loader):.4f}\")\n",
    "                logger.info(f\"Train - Precision: {train_precision:.4f}, Recall: {train_recall:.4f}, F1: {train_f1:.4f}\")\n",
    "                logger.info(f\"Val - Precision: {precision:.4f}, Recall: {recall:.4f}, F1: {f1:.4f}\")\n",
    "                \n",
    "                if f1 > best_f1:\n",
    "                    best_f1 = f1\n",
    "                    best_model = copy.deepcopy(model)\n",
    "                    no_improve_count = 0\n",
    "                else:\n",
    "                    no_improve_count += 1\n",
    "            \n",
    "            # Save checkpoint\n",
    "            save_checkpoint(epoch + 1, model, optimizer, best_f1, params, checkpoint_path)\n",
    "            \n",
    "            # Log metrics\n",
    "            mlflow.log_metric(\"train_loss\", total_loss / len(train_loader), step=epoch)\n",
    "            mlflow.log_metric(\"train_precision\", train_precision, step=epoch)\n",
    "            mlflow.log_metric(\"train_recall\", train_recall, step=epoch)\n",
    "            mlflow.log_metric(\"train_f1\", train_f1, step=epoch)\n",
    "            mlflow.log_metric(\"val_precision\", precision, step=epoch)\n",
    "            mlflow.log_metric(\"val_recall\", recall, step=epoch)\n",
    "            mlflow.log_metric(\"val_f1\", f1, step=epoch)\n",
    "\n",
    "            # Early stopping\n",
    "            if no_improve_count >= patience:\n",
    "                logger.info(f\"Early stopping triggered. No improvement for {patience} epochs.\")\n",
    "                break\n",
    "        \n",
    "        # Final evaluation\n",
    "        best_model.eval()\n",
    "        with torch.no_grad():\n",
    "            y_pred = best_model(X_test_tensor)\n",
    "            y_pred_binary = (y_pred > params['PREDICTION_THRESHOLD']).float()\n",
    "            \n",
    "            precision = precision_score(y_test_binary, y_pred_binary.cpu().numpy(), zero_division=0)\n",
    "            recall = recall_score(y_test_binary, y_pred_binary.cpu().numpy(), zero_division=0)\n",
    "            f1 = f1_score(y_test_binary, y_pred_binary.cpu().numpy(), zero_division=0)\n",
    "            conf_matrix = confusion_matrix(y_test_binary, y_pred_binary.cpu().numpy())\n",
    "            \n",
    "            # Calculate the percentage of positive predictions\n",
    "            positive_pred_percentage = y_pred_binary.mean().item() * 100\n",
    "            \n",
    "            # Log final metrics\n",
    "            mlflow.log_metric(\"test_precision\", precision)\n",
    "            mlflow.log_metric(\"test_recall\", recall)\n",
    "            mlflow.log_metric(\"test_f1\", f1)\n",
    "            mlflow.log_metric(\"positive_pred_percentage\", positive_pred_percentage)\n",
    "            \n",
    "            logger.info(f\"Test Precision: {precision:.4f}, Recall: {recall:.4f}, F1: {f1:.4f}\")\n",
    "            logger.info(f\"Positive prediction percentage: {positive_pred_percentage:.2f}%\")\n",
    "            logger.info(f\"Confusion Matrix:\\n{conf_matrix}\")\n",
    "            \n",
    "            # Log confusion matrix as a figure\n",
    "            plt.figure(figsize=(10,8))\n",
    "            sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues')\n",
    "            plt.title('Confusion Matrix')\n",
    "            plt.ylabel('True label')\n",
    "            plt.xlabel('Predicted label')\n",
    "            plt.savefig(\"confusion_matrix.png\")\n",
    "            mlflow.log_artifact(\"confusion_matrix.png\")\n",
    "            \n",
    "            # Log the PyTorch model\n",
    "            mlflow.pytorch.log_model(best_model, \"model\")\n",
    "        \n",
    "        # Penalize if the model is just predicting the majority class\n",
    "        if positive_pred_percentage < 0.1 or positive_pred_percentage > 99.9:\n",
    "            f1 = 0\n",
    "        \n",
    "        logger.info(f\"Trial completed. Best F1 score: {f1:.4f}\")\n",
    "        return f1\n",
    "\n",
    "# Run Optuna optimization\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=50)\n",
    "\n",
    "# Print and log the best parameters and score\n",
    "best_params = study.best_params\n",
    "best_score = study.best_value\n",
    "logger.info(f\"Best parameters: {best_params}\")\n",
    "logger.info(f\"Best precision score: {best_score}\")\n",
    "\n",
    "# Save the best parameters to a JSON file\n",
    "with open('best_params.json', 'w') as f:\n",
    "    json.dump(best_params, f)\n",
    "\n",
    "logger.info(\"Hyperparameter tuning completed. Best parameters saved to 'best_params.json'.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
