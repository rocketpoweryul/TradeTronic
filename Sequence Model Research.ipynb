{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequence Model Research\n",
    "\n",
    "The scope of this notebook is to assess and train different sequence models given the training data generated.\n",
    "\n",
    "Training data is generated based on financial time series data labeled with potential profits using a buy-sell system.\n",
    "\n",
    "The goal is to create a sequence model that can choose favourable stock charts equal to or better than a human can via traditional technical analysis.\n",
    "\n",
    "## Import Libraries and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPUs available:  [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "\n",
      "TensorFlow is using the GPU\n",
      "\n",
      "Sequences shape: (126133, 252, 15)\n",
      "Labels shape: (126133,)\n",
      "Metadata shape: (126133, 2)\n",
      "\n",
      "Price-related columns:\n",
      "Index: 0, Column: Open\n",
      "Index: 1, Column: High\n",
      "Index: 2, Column: Low\n",
      "Index: 3, Column: Close\n",
      "Index: 9, Column: Close_21_bar_ema\n",
      "Index: 10, Column: Close_50_bar_sma\n",
      "Index: 11, Column: Close_150_bar_sma\n",
      "Index: 12, Column: Close_200_bar_sma\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "# List available devices\n",
    "devices = tf.config.list_physical_devices('GPU')\n",
    "print(\"GPUs available: \", devices)\n",
    "\n",
    "# Confirm TensorFlow is using the GPU\n",
    "if devices:\n",
    "    print(\"\\nTensorFlow is using the GPU\\n\")\n",
    "else:\n",
    "    print(\"\\nTensorFlow is not using the GPU\\n\")\n",
    "\n",
    "# Define the data directory relative to the script location\n",
    "data_dir = 'data'\n",
    "\n",
    "# Define the file paths\n",
    "sequences_path = os.path.join(data_dir, 'sequences.npy')\n",
    "labels_path = os.path.join(data_dir, 'labels.npy')\n",
    "metadata_path = os.path.join(data_dir, 'metadata.npy')\n",
    "\n",
    "# Load the data\n",
    "try:\n",
    "    data_sequences = np.load(sequences_path)\n",
    "    data_labels = np.load(labels_path)\n",
    "    data_metadata = np.load(metadata_path)\n",
    "\n",
    "    # Inspect the shape of the loaded data\n",
    "    print(f'Sequences shape: {data_sequences.shape}')\n",
    "    print(f'Labels shape: {data_labels.shape}')\n",
    "    print(f'Metadata shape: {data_metadata.shape}')\n",
    "\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Error loading files: {e}\")\n",
    "\n",
    "# Define relevant columns and indices for normalization\n",
    "relevant_columns = [\n",
    "    'Open', 'High', 'Low', 'Close', 'Volume', 'Turnover', 'Consol_Detected',\n",
    "    'Consol_Len_Bars', 'Consol_Depth_Percent', 'Close_21_bar_ema',\n",
    "    'Close_50_bar_sma', 'Close_150_bar_sma', 'Close_200_bar_sma',\n",
    "    'RSL', 'RSL_NH'\n",
    "]\n",
    "\n",
    "price_columns_indices = [0, 1, 2, 3, 9, 10, 11, 12]  # Indices of price-related columns in the sequence data\n",
    "\n",
    "# Map indices to column names\n",
    "price_columns = [relevant_columns[i] for i in price_columns_indices]\n",
    "\n",
    "print(\"\\nPrice-related columns:\")\n",
    "for index, column in zip(price_columns_indices, price_columns):\n",
    "    print(f\"Index: {index}, Column: {column}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "### NaN Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaNs in data_sequences: 4995309\n",
      "NaNs remaining in data_sequences after removal: 0\n",
      "NaNs in data_labels: 0\n",
      "Data Seq Min: 0.0\n",
      "Data Seq Max: 468223510183936.0\n"
     ]
    }
   ],
   "source": [
    "# replace all nan with 0. These nans are due to the moving averages having insufficient data to compute anything leaving blank inputs\n",
    "# check if nans exist\n",
    "\n",
    "# Dictionary to map variable names to their corresponding data arrays\n",
    "data_dict = {\n",
    "    'data_sequences': data_sequences,\n",
    "    'data_labels': data_labels,\n",
    "}\n",
    "\n",
    "# Using a dictionary to iterate over variables\n",
    "for var_name, data in data_dict.items():\n",
    "    num_nans = np.sum(np.isnan(data))\n",
    "    print(f\"NaNs in {var_name}: {num_nans}\")\n",
    "\n",
    "    # Remove NaNs\n",
    "    if num_nans > 0:\n",
    "        data_dict[var_name][:] = np.nan_to_num(data)\n",
    "        num_nans = np.sum(np.isnan(data))\n",
    "        print(f\"NaNs remaining in {var_name} after removal: {num_nans}\")\n",
    "\n",
    "print(f\"Data Seq Min: {np.min(data_sequences[:,:,price_columns_indices])}\")\n",
    "print(f\"Data Seq Max: {np.max(data_sequences[:,:,price_columns_indices])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Corrupted sequence removal\n",
    "\n",
    "99% of stocks I buy will be below 1000, with a few above 1000, although they are important.\n",
    "\n",
    "I also noticed quite a few training examples have weird price data, which I filter out below.\n",
    "\n",
    "I noticed with thresholds above 3e3, the max is the threshold, which is very suspect.\n",
    "\n",
    "The loss of training examples is insignificant, and the result is better normalization of the data and obviously no corrupted sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Abnormal Sequence Count: 1335\n",
      "Indices of abnormal sequences: [1495, 1496, 1497, 1498, 1499, 1500, 1501, 1523, 1524, 1525, 1526, 1527, 1528, 1552, 1553, 1554, 1555, 2113, 2202, 2203, 2204, 2205, 2206, 2207, 2208, 2209, 2210, 2211, 2212, 2213, 2214, 2215, 2216, 2217, 2218, 2610, 2611, 2612, 2613, 2614, 3269, 3407, 3408, 3409, 3410, 3411, 3412, 3413, 3414, 3415, 3416, 3417, 3418, 3419, 3420, 3421, 3422, 3423, 3424, 3425, 3687, 4769, 4770, 4771, 4772, 5940, 5941, 5942, 5943, 5944, 6386, 6387, 6390, 6496, 6497, 6498, 6499, 6500, 6501, 6502, 6503, 6767, 6768, 7924, 7925, 7926, 7927, 8028, 8283, 8284, 8285, 8286, 8287, 8464, 8465, 8466, 8467, 8468, 8469, 8470, 8471, 8646, 8647, 8648, 8649, 8650, 8651, 8652, 8653, 9262, 9263, 9264, 9265, 9369, 9370, 9371, 9372, 9373, 9374, 9375, 9376, 9377, 9885, 9886, 9887, 9888, 9889, 9890, 9891, 9892, 9893, 9894, 9895, 9896, 9897, 9898, 9899, 9900, 9901, 9902, 9903, 9904, 9905, 9906, 9907, 11473, 11474, 11475, 11476, 11477, 11478, 11479, 11480, 11481, 11482, 11483, 11484, 11485, 11486, 11487, 11488, 11810, 11811, 11812, 11813, 11814, 13241, 13242, 13244, 13245, 13605, 14124, 14125, 14126, 14394, 14642, 14643, 14644, 14686, 14687, 14688, 14689, 14690, 14691, 14692, 14693, 14694, 14695, 14696, 14697, 14698, 14699, 14700, 14701, 14702, 14703, 14704, 14705, 14706, 14707, 14708, 14709, 14710, 14711, 14712, 14713, 14791, 14792, 14793, 14794, 14795, 14840, 14841, 14842, 14843, 14844, 14845, 14846, 14847, 14848, 14849, 14850, 14851, 14852, 14853, 14854, 14855, 14856, 15264, 15265, 15266, 15267, 15268, 15269, 15418, 15419, 15420, 15421, 15422, 15423, 15424, 15425, 15426, 15427, 15428, 15579, 15580, 15581, 15600, 15602, 15603, 15604, 15605, 15606, 15607, 16056, 16057, 16058, 16059, 16064, 16065, 16325, 16326, 16327, 16328, 17916, 19316, 19317, 19318, 19319, 20399, 20400, 20401, 20402, 20403, 20404, 20405, 20406, 20407, 20408, 20409, 20410, 21117, 21118, 21119, 21991, 21992, 21993, 21994, 21995, 21996, 21997, 21998, 21999, 25111, 25112, 25525, 25526, 25527, 25528, 25529, 25530, 27377, 27378, 27379, 27380, 27381, 27382, 27383, 27384, 27385, 27990, 27991, 27992, 27993, 27994, 27995, 27996, 27997, 27998, 27999, 28000, 28001, 28002, 30194, 30195, 30196, 30197, 30198, 30199, 30200, 30201, 30202, 30203, 30204, 30205, 30206, 30207, 30208, 30209, 30210, 30211, 30212, 30213, 30214, 30215, 30216, 30217, 30218, 30889, 30890, 30933, 30934, 31076, 31077, 31078, 31302, 31303, 31304, 31305, 31306, 31307, 31308, 31309, 31310, 31311, 31312, 31319, 31320, 31596, 31597, 31598, 31599, 31600, 31601, 31602, 31603, 31731, 31732, 31733, 31790, 31791, 31792, 31793, 31794, 31936, 31937, 31938, 31939, 31940, 31941, 31942, 31943, 32198, 32199, 32200, 32201, 32202, 32203, 32204, 32205, 32206, 32207, 32208, 32209, 32210, 32211, 32212, 32213, 32214, 32215, 32216, 33513, 33528, 33530, 33531, 33532, 33862, 33863, 33864, 34391, 34392, 34779, 34780, 34781, 34978, 34979, 34980, 34981, 34982, 34983, 35178, 35179, 35180, 35181, 35182, 35880, 35881, 35882, 35883, 35884, 37132, 37133, 37134, 37135, 37136, 37137, 37138, 37139, 37150, 37774, 37775, 37776, 37777, 37778, 39996, 39997, 39998, 39999, 40725, 42748, 42749, 42750, 42751, 43338, 43460, 45305, 45751, 45752, 45753, 45754, 45755, 46809, 46810, 46811, 47988, 47989, 47990, 47991, 47992, 47993, 47994, 48976, 48977, 48978, 49142, 49143, 49144, 49145, 49146, 49147, 49148, 49149, 49150, 49868, 49869, 49870, 49908, 49909, 49910, 49911, 49912, 49913, 49914, 50182, 50183, 50184, 50185, 51329, 51330, 51682, 51683, 51684, 51685, 51686, 51687, 51688, 51725, 51726, 51727, 51728, 53392, 53393, 53394, 53395, 53396, 53397, 55593, 55594, 55595, 55596, 55597, 55598, 55599, 55600, 55601, 55602, 55603, 58106, 58107, 58108, 58109, 58110, 58111, 58112, 58113, 58114, 58115, 61646, 61647, 61648, 62289, 62290, 62291, 62292, 62293, 62756, 62757, 62758, 62759, 64594, 64595, 65125, 65126, 65127, 65128, 65129, 65633, 65685, 66416, 66802, 67250, 67251, 67252, 67253, 68804, 68805, 68806, 68807, 70832, 70833, 70834, 70835, 70837, 72319, 72320, 72321, 72322, 72323, 72324, 74972, 75556, 75557, 75558, 75559, 75560, 75561, 75745, 75746, 75782, 75783, 75784, 75785, 75786, 76044, 76045, 76046, 76047, 76048, 76053, 76054, 76055, 76710, 76711, 76712, 76713, 76714, 76715, 76716, 76717, 76718, 76719, 76720, 76721, 77108, 77109, 77110, 77111, 77112, 77113, 77114, 77115, 77116, 77117, 77154, 77155, 77156, 77157, 78571, 78572, 78573, 78574, 78857, 78858, 78859, 78860, 78861, 78862, 79611, 79612, 79613, 79614, 79615, 79869, 79870, 79871, 79872, 79873, 79874, 79875, 79876, 79877, 79878, 79879, 79880, 79881, 79882, 79883, 79955, 79956, 79957, 79958, 80231, 80232, 80233, 80234, 80235, 80236, 80237, 80238, 80477, 80478, 80479, 80850, 80898, 80899, 80900, 80901, 80902, 80903, 80904, 80905, 80906, 80907, 80908, 80909, 80910, 80911, 80912, 80913, 82388, 82389, 82390, 82391, 82392, 82459, 82460, 82461, 82462, 82870, 82871, 83445, 83446, 83447, 83448, 83449, 83450, 83451, 83452, 83453, 83454, 83455, 83456, 83457, 83458, 83459, 83460, 83461, 83462, 83463, 83464, 83465, 83472, 83473, 83606, 83607, 84650, 84651, 84652, 84653, 84654, 84655, 85191, 85192, 85193, 85194, 85195, 85196, 85197, 85198, 85199, 85200, 85201, 85202, 85203, 85204, 85205, 85206, 85207, 85208, 85209, 85210, 85211, 85212, 86782, 87438, 87439, 87440, 87441, 87573, 87574, 87575, 88361, 89217, 89218, 89219, 90651, 90652, 90653, 90654, 90655, 90656, 90657, 90658, 90804, 90805, 90806, 90807, 90808, 90809, 90810, 90811, 90812, 90813, 90814, 90815, 90816, 90817, 90818, 90819, 90921, 90922, 91030, 91031, 91032, 91299, 91300, 91301, 91302, 91303, 91304, 91305, 91306, 91307, 91308, 91309, 91310, 91311, 91312, 91313, 91314, 91315, 91316, 91317, 91318, 91319, 91956, 91957, 91958, 91959, 91960, 91961, 91962, 91963, 91964, 91965, 91966, 91967, 91968, 91969, 91970, 91971, 91972, 91973, 92077, 92078, 92079, 92080, 92083, 92084, 92085, 92086, 92087, 92088, 92089, 92090, 92091, 92198, 92199, 92200, 92201, 92202, 92203, 92204, 93137, 93138, 93139, 93703, 93704, 94158, 94159, 94160, 94161, 94162, 94695, 94696, 95010, 95042, 95043, 96042, 96043, 96435, 96436, 98454, 98455, 98456, 98457, 98640, 98641, 98642, 98643, 98644, 98839, 99219, 99220, 99221, 99222, 99223, 99224, 99225, 99233, 99234, 99235, 99334, 99335, 99336, 99337, 99354, 99355, 99356, 99357, 100439, 100440, 100441, 100442, 100443, 100444, 100445, 100446, 100447, 100448, 100449, 100450, 100451, 100452, 100453, 100834, 100835, 100923, 100924, 100925, 100926, 100927, 100928, 100929, 100930, 100931, 100932, 100933, 100952, 101162, 101163, 101164, 101165, 101166, 101167, 101169, 101472, 101473, 101474, 101475, 101868, 102134, 102135, 102136, 102137, 102138, 102139, 102371, 102372, 102373, 102474, 102475, 102476, 102477, 102478, 102479, 102480, 102529, 102703, 102704, 102705, 102706, 102707, 102708, 102709, 102710, 102711, 102712, 102713, 102714, 102715, 103243, 103244, 103245, 103246, 103247, 103248, 103249, 103250, 103251, 103252, 103253, 103255, 103256, 103269, 103270, 103271, 103272, 103273, 103274, 103275, 103276, 103277, 103278, 103279, 103280, 103281, 103282, 103296, 103297, 104732, 104733, 104734, 104735, 104736, 104741, 104742, 104743, 104744, 104745, 104746, 104747, 104748, 104749, 104751, 104752, 104753, 105103, 105104, 105105, 105106, 105107, 105151, 105152, 105153, 105154, 105155, 105156, 105157, 105401, 105402, 105403, 105404, 105405, 105406, 105407, 105408, 105409, 105689, 105737, 105738, 105739, 105740, 105741, 105742, 105743, 105744, 105745, 105746, 105747, 105748, 105752, 105753, 105754, 105755, 105756, 105757, 105758, 105759, 105760, 105761, 105762, 107042, 107043, 107044, 107250, 107251, 107252, 107253, 107254, 107255, 107256, 107604, 107605, 107606, 107607, 107608, 107609, 107699, 107700, 107729, 107776, 108265, 108266, 109065, 109066, 109067, 109068, 109069, 109070, 109071, 109072, 109073, 109074, 109075, 109076, 109424, 109425, 109426, 109427, 109428, 109429, 109430, 109431, 109432, 109433, 109434, 109435, 109436, 109437, 109438, 109439, 109440, 109514, 109515, 109516, 109517, 110577, 110578, 110579, 110580, 110581, 110582, 110583, 110584, 110585, 110586, 110587, 110588, 110589, 110748, 110749, 110750, 110751, 110752, 110753, 110754, 110755, 110756, 111208, 111209, 111210, 111211, 111510, 111511, 111512, 111513, 111514, 111515, 112047, 112048, 112049, 112050, 112051, 112052, 112053, 112054, 112055, 112056, 112057, 112709, 112710, 112711, 112712, 112713, 112714, 112715, 112716, 112717, 113187, 113188, 113189, 113252, 113253, 113254, 113255, 113256, 113257, 113258, 113259, 113260, 113261, 113262, 113263, 113264, 113265, 113266, 113267, 113268, 113269, 113270, 113271, 113272, 113273, 113274, 113275, 113276, 113277, 113300, 113670, 113671, 113672, 113673, 113674, 114185, 114186, 114187, 114765, 114766, 114767, 114768, 114769, 114770, 114771, 114772, 115465, 115466, 116589, 116590, 116591, 116592, 116593, 116597, 117191, 117192, 117193, 117194, 117195, 117196, 117197, 117198, 117199, 117872, 117873, 117874, 117875, 118674, 118675, 118676, 118677, 118678, 118679, 118776, 118777, 118778, 118779, 119202, 119203, 119204, 119526, 119527, 119553, 119554, 121638, 121639, 121640, 121641, 122010, 122011, 122184, 122185, 122186, 122187, 122188, 123433, 123434, 123435, 123436, 123437, 123438, 123439, 123440, 123441, 123442, 123443, 123444, 123445, 123446, 123447, 123448, 123449, 123450, 123451, 123452, 123453, 123454, 123455, 123456, 123576, 123718, 123719, 123720, 123721, 124774, 124775, 124776, 124777, 124778, 124779, 124780, 124781, 125234, 125235, 125236, 125237, 125238, 125239, 125240, 125568, 125569, 125570, 125571, 125572, 125573, 125574, 125575, 125576, 125577, 125917, 125918, 125919, 125920, 126110, 126111]\n",
      "Filtered data_sequences shape: (124798, 252, 15)\n",
      "Filtered data_labels shape: (124798,)\n",
      "Data Seq Min: 0.0\n",
      "Data Seq Max: 2990.532794189453\n"
     ]
    }
   ],
   "source": [
    "# Map indices to column names\n",
    "price_columns = [relevant_columns[i] for i in price_columns_indices]\n",
    "\n",
    "# Set the threshold for abnormal values\n",
    "threshold = 3.0e3\n",
    "\n",
    "# Detect all sequences with abnormally large price data\n",
    "abnormal_sequences = []\n",
    "\n",
    "# Iterate through each sequence to check for abnormal values\n",
    "for sequence_index in range(data_sequences.shape[0]):\n",
    "    # Extract price-related columns for the current sequence\n",
    "    price_data = data_sequences[sequence_index, :, price_columns_indices]\n",
    "    \n",
    "    # Check if any value in the price_data exceeds the threshold\n",
    "    if np.any(price_data > threshold):\n",
    "        abnormal_sequences.append(sequence_index)\n",
    "\n",
    "# Print the indices of the abnormal sequences\n",
    "print(f\"Abnormal Sequence Count: {len(abnormal_sequences)}\")\n",
    "print(f\"Indices of abnormal sequences: {abnormal_sequences}\")\n",
    "\n",
    "# Create a mask for sequences that are not abnormal\n",
    "mask = np.ones(data_sequences.shape[0], dtype=bool)\n",
    "mask[abnormal_sequences] = False\n",
    "\n",
    "# Filter out abnormal sequences from data_sequences and data_labels\n",
    "filtered_data_sequences = data_sequences[mask]\n",
    "filtered_data_labels = data_labels[mask]\n",
    "\n",
    "# Print the shape of the filtered data\n",
    "print(f\"Filtered data_sequences shape: {filtered_data_sequences.shape}\")\n",
    "print(f\"Filtered data_labels shape: {filtered_data_labels.shape}\")\n",
    "\n",
    "print(f\"Data Seq Min: {np.min(filtered_data_sequences[:,:,price_columns_indices])}\")\n",
    "print(f\"Data Seq Max: {np.max(filtered_data_sequences[:,:,price_columns_indices])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization of Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized Data Seq Min: 0.0\n",
      "Normalized Data Seq Max: 1.0\n",
      "Normalized Data Labels Min: 0.0\n",
      "Normalized Data Labels Max: 1.0000000000000002\n"
     ]
    }
   ],
   "source": [
    "# Normalize the price-related features together\n",
    "price_scaler = MinMaxScaler()\n",
    "\n",
    "# Reshape the price-related features to fit the scaler\n",
    "original_shape = filtered_data_sequences[:, :, price_columns_indices].shape\n",
    "reshaped_data = filtered_data_sequences[:, :, price_columns_indices].reshape(-1, len(price_columns_indices))\n",
    "\n",
    "# Fit and transform the price-related features\n",
    "normalized_price_data = price_scaler.fit_transform(reshaped_data)\n",
    "\n",
    "# Reshape back to the original shape\n",
    "normalized_price_data = normalized_price_data.reshape(original_shape)\n",
    "\n",
    "# Replace the original price-related features with the normalized ones\n",
    "filtered_data_sequences[:, :, price_columns_indices] = normalized_price_data\n",
    "\n",
    "# Normalize the remaining features individually\n",
    "num_sequences, num_timesteps, num_features = filtered_data_sequences.shape\n",
    "for feature_index in range(num_features):\n",
    "    if feature_index not in price_columns_indices:\n",
    "        # Initialize a new scaler for each feature\n",
    "        feature_scaler = MinMaxScaler()\n",
    "        \n",
    "        # Extract the feature data\n",
    "        feature_data = filtered_data_sequences[:, :, feature_index].reshape(-1, 1)\n",
    "        \n",
    "        # Fit and transform the scaler\n",
    "        normalized_feature_data = feature_scaler.fit_transform(feature_data)\n",
    "        \n",
    "        # Reshape back to the original shape\n",
    "        normalized_feature_data = normalized_feature_data.reshape(num_sequences, num_timesteps)\n",
    "        \n",
    "        # Replace the original feature with the normalized one\n",
    "        filtered_data_sequences[:, :, feature_index] = normalized_feature_data\n",
    "\n",
    "# Print normalized data sequences to check\n",
    "print(f\"Normalized Data Seq Min: {np.min(filtered_data_sequences)}\")\n",
    "print(f\"Normalized Data Seq Max: {np.max(filtered_data_sequences)}\")\n",
    "\n",
    "# Scale the labels\n",
    "label_scaler = MinMaxScaler(feature_range=(0,1))\n",
    "\n",
    "# Reshape the labels to 2D array\n",
    "filtered_data_labels = filtered_data_labels.reshape(-1, 1)\n",
    "\n",
    "# Fit and transform the labels\n",
    "filtered_data_labels = label_scaler.fit_transform(filtered_data_labels)\n",
    "\n",
    "# Reshape the labels back to their original shape if needed\n",
    "filtered_data_labels = filtered_data_labels.reshape(-1)\n",
    "\n",
    "print(f\"Normalized Data Labels Min: {np.min(filtered_data_labels)}\")\n",
    "print(f\"Normalized Data Labels Max: {np.max(filtered_data_labels)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of filtered_data_sequences: (124798, 252, 15)\n",
      "Shape of filtered_data_labels: (124798,)\n",
      "Shape of X_train: (74878, 252, 15)\n",
      "Shape of X_val: (24960, 252, 15)\n",
      "Shape of X_test: (24960, 252, 15)\n",
      "Shape of y_train: (74878,)\n",
      "Shape of y_val: (24960,)\n",
      "Shape of y_test: (24960,)\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Check if TensorFlow is using GPU\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "\n",
    "# Hyperparameters\n",
    "test_size = 0.2\n",
    "val_size = 0.2\n",
    "lstm_units = 50\n",
    "dropout_rate = 0.2\n",
    "epochs = 100\n",
    "batch_size = 32\n",
    "patience = 10\n",
    "\n",
    "# Assume filtered_data_sequences and filtered_data_labels are already normalized\n",
    "print(\"Shape of filtered_data_sequences:\", filtered_data_sequences.shape)\n",
    "print(\"Shape of filtered_data_labels:\", filtered_data_labels.shape)\n",
    "\n",
    "# Split data into training, validation, and test sets\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(filtered_data_sequences, filtered_data_labels, test_size=test_size + val_size, random_state=42)\n",
    "val_ratio = val_size / (test_size + val_size)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=val_ratio, random_state=42)\n",
    "\n",
    "print(\"Shape of X_train:\", X_train.shape)\n",
    "print(\"Shape of X_val:\", X_val.shape)\n",
    "print(\"Shape of X_test:\", X_test.shape)\n",
    "print(\"Shape of y_train:\", y_train.shape)\n",
    "print(\"Shape of y_val:\", y_val.shape)\n",
    "print(\"Shape of y_test:\", y_test.shape)\n",
    "\n",
    "# Build the LSTM model\n",
    "model = Sequential()\n",
    "model.add(Input(shape=(X_train.shape[1], X_train.shape[2])))\n",
    "model.add(LSTM(lstm_units, activation='relu', return_sequences=True))\n",
    "model.add(Dropout(dropout_rate))\n",
    "model.add(LSTM(lstm_units, activation='relu'))\n",
    "model.add(Dropout(dropout_rate))\n",
    "model.add(Dense(1))\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# Print model summary\n",
    "model.summary()\n",
    "\n",
    "# Train the model\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=patience, restore_best_weights=True)\n",
    "history = model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, validation_data=(X_val, y_val), callbacks=[early_stopping])\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "print(f'Test MSE: {mse}')\n",
    "print(f'Test MAE: {mae}')\n",
    "\n",
    "# Plot training and validation loss\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(history.history['loss'], label='Train Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Model Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plot true vs predicted values\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(y_test, label='True Values')\n",
    "plt.plot(y_pred, label='Predicted Values')\n",
    "plt.title('True vs Predicted Values')\n",
    "plt.xlabel('Samples')\n",
    "plt.ylabel('Value')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
