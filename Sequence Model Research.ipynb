{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequence Model Research\n",
    "\n",
    "The scope of this notebook is to assess and train different sequence models given the training data generated.\n",
    "\n",
    "Training data is generated based on financial time series data labeled with potential profits using a buy-sell system.\n",
    "\n",
    "The goal is to create a sequence model that can choose favourable stock charts equal to or better than a human can via traditional technical analysis.\n",
    "\n",
    "## Import Libraries and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded sequences shape: (115000, 63, 12)\n",
      "Loaded sequences size: 86940000\n",
      "Loaded labels shape: (115000,)\n",
      "Loaded metadata shape: (115000, 2)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# Define the data directory relative to the script location\n",
    "data_dir = 'data'\n",
    "\n",
    "# Define the file paths\n",
    "sequences_path = os.path.join(data_dir, 'sequences.npy')\n",
    "labels_path = os.path.join(data_dir, 'labels.npy')\n",
    "metadata_path = os.path.join(data_dir, 'metadata.npy')\n",
    "\n",
    "# Load the data\n",
    "try:\n",
    "    data_sequences = np.load(sequences_path)\n",
    "    data_labels = np.load(labels_path)\n",
    "    data_metadata = np.load(metadata_path)\n",
    "\n",
    "    # Number of examples to select\n",
    "    num_examples = 115000\n",
    "\n",
    "    # Generate a random permutation of indices\n",
    "    indices = np.random.permutation(len(data_sequences))\n",
    "\n",
    "    # Select the first `num_examples` indices\n",
    "    selected_indices = indices[:num_examples]\n",
    "\n",
    "    # Use the selected indices to create the random subset\n",
    "    data_sequences = data_sequences[selected_indices, :, :]\n",
    "    data_labels = data_labels[selected_indices]\n",
    "    data_metadata = data_metadata[selected_indices]\n",
    "\n",
    "    # Inspect the shape and size of the loaded data before slicing\n",
    "    print(f'Loaded sequences shape: {data_sequences.shape}')\n",
    "    print(f'Loaded sequences size: {data_sequences.size}')\n",
    "    print(f'Loaded labels shape: {data_labels.shape}')\n",
    "    print(f'Loaded metadata shape: {data_metadata.shape}')\n",
    "\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Error loading files: {e}\")\n",
    "except ValueError as e:\n",
    "    print(f\"Value error: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "### NaN anf INF Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaNs in data_sequences: 905259\n",
      "Infs in data_sequences: 37\n",
      "NaNs remaining in data_sequences after removal: 0\n",
      "Infs remaining in data_sequences after removal: 0\n",
      "NaNs in data_labels: 0\n",
      "Infs in data_labels: 0\n",
      "NaN and Inf removal completed.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Dictionary to map variable names to their corresponding data arrays\n",
    "data_dict = {\n",
    "    'data_sequences': data_sequences,\n",
    "    'data_labels': data_labels,\n",
    "}\n",
    "\n",
    "# Using a dictionary to iterate over variables\n",
    "for var_name, data in data_dict.items():\n",
    "    num_nans = np.sum(np.isnan(data))\n",
    "    num_infs = np.sum(np.isinf(data))\n",
    "    print(f\"NaNs in {var_name}: {num_nans}\")\n",
    "    print(f\"Infs in {var_name}: {num_infs}\")\n",
    "\n",
    "    # Remove NaNs and Infs\n",
    "    if num_nans > 0 or num_infs > 0:\n",
    "        data_dict[var_name][:] = np.nan_to_num(data, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "        num_nans_after = np.sum(np.isnan(data))\n",
    "        num_infs_after = np.sum(np.isinf(data))\n",
    "        print(f\"NaNs remaining in {var_name} after removal: {num_nans_after}\")\n",
    "        print(f\"Infs remaining in {var_name} after removal: {num_infs_after}\")\n",
    "\n",
    "print(\"NaN and Inf removal completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Corrupted sequence removal\n",
    "\n",
    "99% of stocks I buy will be below 1000, with a few above 1000, although they are important.\n",
    "\n",
    "I also noticed quite a few training examples have weird price data, which I filter out below.\n",
    "\n",
    "I noticed with thresholds above 3e3, the max is the threshold, which is very suspect.\n",
    "\n",
    "The loss of training examples is insignificant, and the result is better normalization of the data and obviously no corrupted sequences.\n",
    "\n",
    "#### Feature Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Statistics:\n",
      "--------------------------------------------------\n",
      "Feature: Consol_Len_Bars\n",
      "  Mean: 69.4247\n",
      "  Median: 27.0000\n",
      "  Std Dev: 121.0637\n",
      "  Min: 0.0000\n",
      "  Max: 3142.0000\n",
      "  25th Percentile: 6.0000\n",
      "  75th Percentile: 77.0000\n",
      "  Skewness: 4.4888\n",
      "  Kurtosis: 38.1585\n",
      "  Zero Count: 1417116.0000\n",
      "  Zero Percentage: 19.5599\n",
      "--------------------------------------------------\n",
      "Feature: Consol_Depth_Percent\n",
      "  Mean: 16.4847\n",
      "  Median: 15.6965\n",
      "  Std Dev: 11.7504\n",
      "  Min: 0.0000\n",
      "  Max: 54.4286\n",
      "  25th Percentile: 7.3892\n",
      "  75th Percentile: 25.9770\n",
      "  Skewness: 0.2046\n",
      "  Kurtosis: -0.8865\n",
      "  Zero Count: 1341459.0000\n",
      "  Zero Percentage: 18.5157\n",
      "--------------------------------------------------\n",
      "Feature: Distance_to_21EMA\n",
      "  Mean: 0.8472\n",
      "  Median: 0.8899\n",
      "  Std Dev: 5.5330\n",
      "  Min: -99.9886\n",
      "  Max: 712.1856\n",
      "  25th Percentile: -1.2164\n",
      "  75th Percentile: 3.0212\n",
      "  Skewness: 1.0784\n",
      "  Kurtosis: 96.7708\n",
      "  Zero Count: 17893.0000\n",
      "  Zero Percentage: 0.2470\n",
      "--------------------------------------------------\n",
      "Feature: Distance_to_50SMA\n",
      "  Mean: 2.0616\n",
      "  Median: 1.9077\n",
      "  Std Dev: 10.0820\n",
      "  Min: -99.9896\n",
      "  Max: 1092.4742\n",
      "  25th Percentile: -1.5417\n",
      "  75th Percentile: 5.9325\n",
      "  Skewness: 1.5630\n",
      "  Kurtosis: 73.0485\n",
      "  Zero Count: 119200.0000\n",
      "  Zero Percentage: 1.6453\n",
      "--------------------------------------------------\n",
      "Feature: Distance_to_200SMA\n",
      "  Mean: 5.5608\n",
      "  Median: 5.4942\n",
      "  Std Dev: 21.2768\n",
      "  Min: -98.0358\n",
      "  Max: 2523.0145\n",
      "  25th Percentile: -0.1859\n",
      "  75th Percentile: 13.8591\n",
      "  Skewness: 2.3014\n",
      "  Kurtosis: 115.1370\n",
      "  Zero Count: 534650.0000\n",
      "  Zero Percentage: 7.3796\n",
      "--------------------------------------------------\n",
      "Feature: RSL_NH_Count\n",
      "  Mean: 115.9376\n",
      "  Median: 94.0000\n",
      "  Std Dev: 99.1009\n",
      "  Min: 0.0000\n",
      "  Max: 1253.0000\n",
      "  25th Percentile: 32.0000\n",
      "  75th Percentile: 180.0000\n",
      "  Skewness: 1.0216\n",
      "  Kurtosis: 1.7730\n",
      "  Zero Count: 471868.0000\n",
      "  Zero Percentage: 6.5130\n",
      "--------------------------------------------------\n",
      "Feature: RSL_Slope\n",
      "  Mean: 0.0000\n",
      "  Median: 0.0000\n",
      "  Std Dev: 0.0004\n",
      "  Min: -0.1085\n",
      "  Max: 0.0804\n",
      "  25th Percentile: -0.0000\n",
      "  75th Percentile: 0.0000\n",
      "  Skewness: -9.0249\n",
      "  Kurtosis: 6254.7809\n",
      "  Zero Count: 128048.0000\n",
      "  Zero Percentage: 1.7674\n",
      "--------------------------------------------------\n",
      "Feature: Up_Down_Days\n",
      "  Mean: 0.7114\n",
      "  Median: 0.0000\n",
      "  Std Dev: 3.4844\n",
      "  Min: -14.0000\n",
      "  Max: 14.0000\n",
      "  25th Percentile: -2.0000\n",
      "  75th Percentile: 3.0000\n",
      "  Skewness: 0.0044\n",
      "  Kurtosis: -0.0715\n",
      "  Zero Count: 1332956.0000\n",
      "  Zero Percentage: 18.3983\n",
      "--------------------------------------------------\n",
      "Feature: Stage 2\n",
      "  Mean: 0.3480\n",
      "  Median: 0.0000\n",
      "  Std Dev: 0.4763\n",
      "  Min: 0.0000\n",
      "  Max: 1.0000\n",
      "  25th Percentile: 0.0000\n",
      "  75th Percentile: 1.0000\n",
      "  Skewness: 0.6381\n",
      "  Kurtosis: -1.5928\n",
      "  Zero Count: 4723574.0000\n",
      "  Zero Percentage: 65.1977\n",
      "--------------------------------------------------\n",
      "Feature: UpDownVolumeRatio\n",
      "  Mean: 1.2797\n",
      "  Median: 1.1208\n",
      "  Std Dev: 8.7837\n",
      "  Min: 0.0000\n",
      "  Max: 9897.7948\n",
      "  25th Percentile: 0.8669\n",
      "  75th Percentile: 1.4414\n",
      "  Skewness: 832.4426\n",
      "  Kurtosis: 851710.8737\n",
      "  Zero Count: 119206.0000\n",
      "  Zero Percentage: 1.6454\n",
      "--------------------------------------------------\n",
      "Feature: ATR\n",
      "  Mean: 1.1323\n",
      "  Median: 0.4658\n",
      "  Std Dev: 3.2963\n",
      "  Min: 0.0000\n",
      "  Max: 311.1429\n",
      "  25th Percentile: 0.2199\n",
      "  75th Percentile: 0.9894\n",
      "  Skewness: 16.4100\n",
      "  Kurtosis: 508.8929\n",
      "  Zero Count: 35886.0000\n",
      "  Zero Percentage: 0.4953\n",
      "--------------------------------------------------\n",
      "Feature: %B\n",
      "  Mean: 0.5693\n",
      "  Median: 0.6074\n",
      "  Std Dev: 0.3138\n",
      "  Min: -0.5621\n",
      "  Max: 1.5611\n",
      "  25th Percentile: 0.3291\n",
      "  75th Percentile: 0.8152\n",
      "  Skewness: -0.2709\n",
      "  Kurtosis: -0.6343\n",
      "  Zero Count: 47428.0000\n",
      "  Zero Percentage: 0.6546\n",
      "--------------------------------------------------\n",
      "Overall Dataset Statistics:\n",
      "Total number of sequences: 115000\n",
      "Sequence length: 63\n",
      "Number of features: 12\n",
      "Total number of data points: 86940000\n",
      "Memory usage: 663.30 MB\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def print_feature_stats(data_sequences, feature_names):\n",
    "    print(\"Feature Statistics:\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    for i, feature_name in enumerate(feature_names):\n",
    "        feature_data = data_sequences[:, :, i].flatten()\n",
    "        \n",
    "        stats = {\n",
    "            \"Mean\": np.mean(feature_data),\n",
    "            \"Median\": np.median(feature_data),\n",
    "            \"Std Dev\": np.std(feature_data),\n",
    "            \"Min\": np.min(feature_data),\n",
    "            \"Max\": np.max(feature_data),\n",
    "            \"25th Percentile\": np.percentile(feature_data, 25),\n",
    "            \"75th Percentile\": np.percentile(feature_data, 75),\n",
    "            \"Skewness\": pd.Series(feature_data).skew(),\n",
    "            \"Kurtosis\": pd.Series(feature_data).kurtosis(),\n",
    "            \"Zero Count\": np.sum(feature_data == 0),\n",
    "            \"Zero Percentage\": np.mean(feature_data == 0) * 100\n",
    "        }\n",
    "        \n",
    "        print(f\"Feature: {feature_name}\")\n",
    "        for stat_name, stat_value in stats.items():\n",
    "            print(f\"  {stat_name}: {stat_value:.4f}\")\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "# List of feature names\n",
    "feature_names = [\n",
    "    'Consol_Len_Bars', 'Consol_Depth_Percent',\n",
    "    'Distance_to_21EMA', 'Distance_to_50SMA', 'Distance_to_200SMA', \n",
    "    'RSL_NH_Count', 'RSL_Slope', 'Up_Down_Days', \n",
    "    'Stage 2', 'UpDownVolumeRatio', 'ATR', '%B'\n",
    "]\n",
    "\n",
    "# Call the function to print statistics\n",
    "print_feature_stats(data_sequences, feature_names)\n",
    "\n",
    "# Additional overall statistics\n",
    "print(\"Overall Dataset Statistics:\")\n",
    "print(f\"Total number of sequences: {data_sequences.shape[0]}\")\n",
    "print(f\"Sequence length: {data_sequences.shape[1]}\")\n",
    "print(f\"Number of features: {data_sequences.shape[2]}\")\n",
    "print(f\"Total number of data points: {data_sequences.size}\")\n",
    "print(f\"Memory usage: {data_sequences.nbytes / (1024 * 1024):.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization of Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Define the indices based on the provided feature names\n",
    "feature_names = [\n",
    "    'Consol_Len_Bars', 'Consol_Depth_Percent',\n",
    "    'Distance_to_21EMA', 'Distance_to_50SMA', 'Distance_to_200SMA', \n",
    "    'RSL_NH_Count', 'RSL_Slope', 'Up_Down_Days', \n",
    "    'Stage 2', 'UpDownVolumeRatio', 'ATR', '%B'\n",
    "]\n",
    "\n",
    "feature_indices = {name: idx for idx, name in enumerate(feature_names)}\n",
    "\n",
    "# Function to remove outliers and cap values\n",
    "def preprocess_data(sequences, labels):\n",
    "    # Reshape sequences to 2D array for easier processing (flatten the timesteps)\n",
    "    num_sequences, num_timesteps, num_features = sequences.shape\n",
    "    sequences_reshaped = sequences.reshape(-1, num_features)\n",
    "    \n",
    "    # Create a mask to filter out invalid sequences\n",
    "    valid_mask = (\n",
    "        (sequences_reshaped[:, feature_indices['Distance_to_21EMA']] <= 100) &\n",
    "        (sequences_reshaped[:, feature_indices['Distance_to_50SMA']] <= 200) &\n",
    "        (sequences_reshaped[:, feature_indices['Distance_to_200SMA']] <= 500)\n",
    "    )\n",
    "    \n",
    "    # Reshape the valid_mask to match the original sequence shape\n",
    "    valid_mask_reshaped = valid_mask.reshape(num_sequences, num_timesteps)\n",
    "    \n",
    "    # Filter out sequences with any invalid timesteps\n",
    "    valid_sequences_mask = valid_mask_reshaped.all(axis=1)\n",
    "    filtered_sequences = sequences[valid_sequences_mask]\n",
    "    filtered_labels = labels[valid_sequences_mask]\n",
    "    \n",
    "    # Cap 'UpDownVolumeRatio' at 10\n",
    "    filtered_sequences[:, :, feature_indices['UpDownVolumeRatio']] = np.minimum(\n",
    "        filtered_sequences[:, :, feature_indices['UpDownVolumeRatio']], 10\n",
    "    )\n",
    "    \n",
    "    # Normalize the features using Z-score normalization\n",
    "    scaler = StandardScaler()\n",
    "    \n",
    "    # Flatten the sequence again for normalization\n",
    "    filtered_sequences_reshaped = filtered_sequences.reshape(-1, num_features)\n",
    "    \n",
    "    # Normalize\n",
    "    normalized_data_reshaped = scaler.fit_transform(filtered_sequences_reshaped)\n",
    "    \n",
    "    # Reshape back to the original 3D shape\n",
    "    normalized_data = normalized_data_reshaped.reshape(filtered_sequences.shape)\n",
    "    \n",
    "    return normalized_data, filtered_labels\n",
    "\n",
    "# Function to print feature statistics\n",
    "def print_feature_stats(data_sequences, feature_names):\n",
    "    print(\"Feature Statistics:\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    for i, feature_name in enumerate(feature_names):\n",
    "        feature_data = data_sequences[:, :, i].flatten()\n",
    "        \n",
    "        stats = {\n",
    "            \"Mean\": np.mean(feature_data),\n",
    "            \"Median\": np.median(feature_data),\n",
    "            \"Std Dev\": np.std(feature_data),\n",
    "            \"Min\": np.min(feature_data),\n",
    "            \"Max\": np.max(feature_data),\n",
    "            \"25th Percentile\": np.percentile(feature_data, 25),\n",
    "            \"75th Percentile\": np.percentile(feature_data, 75),\n",
    "            \"Skewness\": pd.Series(feature_data).skew(),\n",
    "            \"Kurtosis\": pd.Series(feature_data).kurtosis(),\n",
    "            \"Zero Count\": np.sum(feature_data == 0),\n",
    "            \"Zero Percentage\": np.mean(feature_data == 0) * 100\n",
    "        }\n",
    "        \n",
    "        print(f\"Feature: {feature_name}\")\n",
    "        for stat_name, stat_value in stats.items():\n",
    "            print(f\"  {stat_name}: {stat_value:.4f}\")\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "# Example usage with data_sequences and data_labels\n",
    "# Assuming data_sequences is loaded and has shape (115000, 63, 12)\n",
    "\n",
    "# Process the data\n",
    "normalized_data, processed_labels = preprocess_data(data_sequences, data_labels)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stats again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Statistics:\n",
      "--------------------------------------------------\n",
      "Feature: Consol_Len_Bars\n",
      "  Mean: 0.0000\n",
      "  Median: -0.3507\n",
      "  Std Dev: 1.0000\n",
      "  Min: -0.5737\n",
      "  Max: 25.3726\n",
      "  25th Percentile: -0.5241\n",
      "  75th Percentile: 0.0622\n",
      "  Skewness: 4.4875\n",
      "  Kurtosis: 38.1379\n",
      "  Zero Count: 0.0000\n",
      "  Zero Percentage: 0.0000\n",
      "--------------------------------------------------\n",
      "Feature: Consol_Depth_Percent\n",
      "  Mean: 0.0000\n",
      "  Median: -0.0672\n",
      "  Std Dev: 1.0000\n",
      "  Min: -1.4035\n",
      "  Max: 3.2297\n",
      "  25th Percentile: -0.7736\n",
      "  75th Percentile: 0.8075\n",
      "  Skewness: 0.2047\n",
      "  Kurtosis: -0.8855\n",
      "  Zero Count: 0.0000\n",
      "  Zero Percentage: 0.0000\n",
      "--------------------------------------------------\n",
      "Feature: Distance_to_21EMA\n",
      "  Mean: -0.0000\n",
      "  Median: 0.0097\n",
      "  Std Dev: 1.0000\n",
      "  Min: -18.5580\n",
      "  Max: 18.0991\n",
      "  25th Percentile: -0.3778\n",
      "  75th Percentile: 0.4016\n",
      "  Skewness: -0.2241\n",
      "  Kurtosis: 14.0967\n",
      "  Zero Count: 0.0000\n",
      "  Zero Percentage: 0.0000\n",
      "--------------------------------------------------\n",
      "Feature: Distance_to_50SMA\n",
      "  Mean: 0.0000\n",
      "  Median: -0.0130\n",
      "  Std Dev: 1.0000\n",
      "  Min: -10.3369\n",
      "  Max: 19.2472\n",
      "  25th Percentile: -0.3625\n",
      "  75th Percentile: 0.3942\n",
      "  Skewness: 0.3537\n",
      "  Kurtosis: 10.7450\n",
      "  Zero Count: 0.0000\n",
      "  Zero Percentage: 0.0000\n",
      "--------------------------------------------------\n",
      "Feature: Distance_to_200SMA\n",
      "  Mean: 0.0000\n",
      "  Median: -0.0008\n",
      "  Std Dev: 1.0000\n",
      "  Min: -4.9778\n",
      "  Max: 23.3355\n",
      "  25th Percentile: -0.2738\n",
      "  75th Percentile: 0.4009\n",
      "  Skewness: 0.7557\n",
      "  Kurtosis: 9.4335\n",
      "  Zero Count: 0.0000\n",
      "  Zero Percentage: 0.0000\n",
      "--------------------------------------------------\n",
      "Feature: RSL_NH_Count\n",
      "  Mean: 0.0000\n",
      "  Median: -0.2219\n",
      "  Std Dev: 1.0000\n",
      "  Min: -1.1703\n",
      "  Max: 11.4726\n",
      "  25th Percentile: -0.8475\n",
      "  75th Percentile: 0.6459\n",
      "  Skewness: 1.0212\n",
      "  Kurtosis: 1.7727\n",
      "  Zero Count: 0.0000\n",
      "  Zero Percentage: 0.0000\n",
      "--------------------------------------------------\n",
      "Feature: RSL_Slope\n",
      "  Mean: -0.0000\n",
      "  Median: -0.0168\n",
      "  Std Dev: 1.0000\n",
      "  Min: -292.4295\n",
      "  Max: 216.6910\n",
      "  25th Percentile: -0.0636\n",
      "  75th Percentile: 0.0492\n",
      "  Skewness: -9.4138\n",
      "  Kurtosis: 6344.9791\n",
      "  Zero Count: 0.0000\n",
      "  Zero Percentage: 0.0000\n",
      "--------------------------------------------------\n",
      "Feature: Up_Down_Days\n",
      "  Mean: -0.0000\n",
      "  Median: -0.2043\n",
      "  Std Dev: 1.0000\n",
      "  Min: -4.2222\n",
      "  Max: 3.8136\n",
      "  25th Percentile: -0.7783\n",
      "  75th Percentile: 0.6567\n",
      "  Skewness: 0.0044\n",
      "  Kurtosis: -0.0716\n",
      "  Zero Count: 0.0000\n",
      "  Zero Percentage: 0.0000\n",
      "--------------------------------------------------\n",
      "Feature: Stage 2\n",
      "  Mean: 0.0000\n",
      "  Median: -0.7308\n",
      "  Std Dev: 1.0000\n",
      "  Min: -0.7308\n",
      "  Max: 1.3684\n",
      "  25th Percentile: -0.7308\n",
      "  75th Percentile: 1.3684\n",
      "  Skewness: 0.6377\n",
      "  Kurtosis: -1.5934\n",
      "  Zero Count: 0.0000\n",
      "  Zero Percentage: 0.0000\n",
      "--------------------------------------------------\n",
      "Feature: UpDownVolumeRatio\n",
      "  Mean: -0.0000\n",
      "  Median: -0.1562\n",
      "  Std Dev: 1.0000\n",
      "  Min: -1.6617\n",
      "  Max: 11.7731\n",
      "  25th Percentile: -0.4971\n",
      "  75th Percentile: 0.2741\n",
      "  Skewness: 4.9959\n",
      "  Kurtosis: 45.8482\n",
      "  Zero Count: 0.0000\n",
      "  Zero Percentage: 0.0000\n",
      "--------------------------------------------------\n",
      "Feature: ATR\n",
      "  Mean: 0.0000\n",
      "  Median: -0.2022\n",
      "  Std Dev: 1.0000\n",
      "  Min: -0.3438\n",
      "  Max: 94.2950\n",
      "  25th Percentile: -0.2769\n",
      "  75th Percentile: -0.0430\n",
      "  Skewness: 16.4621\n",
      "  Kurtosis: 512.8552\n",
      "  Zero Count: 0.0000\n",
      "  Zero Percentage: 0.0000\n",
      "--------------------------------------------------\n",
      "Feature: %B\n",
      "  Mean: 0.0000\n",
      "  Median: 0.1214\n",
      "  Std Dev: 1.0000\n",
      "  Min: -3.6061\n",
      "  Max: 3.1602\n",
      "  25th Percentile: -0.7657\n",
      "  75th Percentile: 0.7836\n",
      "  Skewness: -0.2712\n",
      "  Kurtosis: -0.6348\n",
      "  Zero Count: 0.0000\n",
      "  Zero Percentage: 0.0000\n",
      "--------------------------------------------------\n",
      "Overall Dataset Statistics:\n",
      "Total number of sequences: 115000\n",
      "Sequence length: 63\n",
      "Number of features: 12\n",
      "Total number of data points: 86940000\n",
      "Memory usage: 663.30 MB\n"
     ]
    }
   ],
   "source": [
    "# Print statistics\n",
    "print_feature_stats(normalized_data, feature_names)\n",
    "\n",
    "# Additional overall statistics\n",
    "print(\"Overall Dataset Statistics:\")\n",
    "print(f\"Total number of sequences: {data_sequences.shape[0]}\")\n",
    "print(f\"Sequence length: {data_sequences.shape[1]}\")\n",
    "print(f\"Number of features: {data_sequences.shape[2]}\")\n",
    "print(f\"Total number of data points: {data_sequences.size}\")\n",
    "print(f\"Memory usage: {data_sequences.nbytes / (1024 * 1024):.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Label Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statistics for Original Labels:\n",
      "--------------------------------------------------\n",
      "  Mean: 0.0284\n",
      "  Median: -0.0209\n",
      "  Std Dev: 0.2813\n",
      "  Min: -0.9115\n",
      "  Max: 15.0370\n",
      "  25th Percentile: -0.0730\n",
      "  75th Percentile: 0.0591\n",
      "  Zero Count: 106.0000\n",
      "  Zero Percentage: 0.0922\n",
      "--------------------------------------------------\n",
      "Statistics for Binary Labels:\n",
      "--------------------------------------------------\n",
      "  Mean: 0.4094\n",
      "  Median: 0.0000\n",
      "  Std Dev: 0.4917\n",
      "  Min: 0.0000\n",
      "  Max: 1.0000\n",
      "  25th Percentile: 0.0000\n",
      "  75th Percentile: 1.0000\n",
      "  Zero Count: 67864.0000\n",
      "  Zero Percentage: 59.0563\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Function to print statistics of the labels\n",
    "def print_label_stats(labels, label_name=\"Labels\"):\n",
    "    stats = {\n",
    "        \"Mean\": np.mean(labels),\n",
    "        \"Median\": np.median(labels),\n",
    "        \"Std Dev\": np.std(labels),\n",
    "        \"Min\": np.min(labels),\n",
    "        \"Max\": np.max(labels),\n",
    "        \"25th Percentile\": np.percentile(labels, 25),\n",
    "        \"75th Percentile\": np.percentile(labels, 75),\n",
    "        \"Zero Count\": np.sum(labels == 0),\n",
    "        \"Zero Percentage\": np.mean(labels == 0) * 100\n",
    "    }\n",
    "    \n",
    "    print(f\"Statistics for {label_name}:\")\n",
    "    print(\"-\" * 50)\n",
    "    for stat_name, stat_value in stats.items():\n",
    "        print(f\"  {stat_name}: {stat_value:.4f}\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "# Assuming `normalized_data` and `processed_labels` are already available from the preprocessing step\n",
    "\n",
    "# Print statistics for the original labels\n",
    "print_label_stats(processed_labels, label_name=\"Original Labels\")\n",
    "\n",
    "# Convert processed_labels to binary labels: profit (1) or not profit (0)\n",
    "binary_labels = (processed_labels > 0).astype(int)\n",
    "\n",
    "# Print statistics for the binary labels\n",
    "print_label_stats(binary_labels, label_name=\"Binary Labels\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simple RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-27 13:14:03.730246: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-06-27 13:14:03.756993: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-06-27 13:14:04.248488: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class weights: {0: 0.8466491807143699, 1: 1.221190223166844}\n",
      "Shape of X_train: (91931, 63, 12)\n",
      "Shape of X_val: (22983, 63, 12)\n",
      "Shape of y_train: (91931,)\n",
      "Shape of y_val: (22983,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-27 13:14:05.177597: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-27 13:14:05.410425: W tensorflow/core/common_runtime/gpu/gpu_device.cc:2251] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m2873/2873\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 14ms/step - accuracy: 0.5269 - loss: 0.6908 - val_accuracy: 0.5394 - val_loss: 0.6872\n",
      "Epoch 2/20\n",
      "\u001b[1m2873/2873\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 14ms/step - accuracy: 0.5422 - loss: 0.6879 - val_accuracy: 0.5674 - val_loss: 0.6814\n",
      "Epoch 3/20\n",
      "\u001b[1m 581/2873\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m29s\u001b[0m 13ms/step - accuracy: 0.5520 - loss: 0.6856"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "# Compute class weights\n",
    "class_weights = class_weight.compute_class_weight(class_weight='balanced', classes=np.unique(binary_labels), y=binary_labels)\n",
    "class_weights_dict = {i : class_weights[i] for i in range(len(class_weights))}\n",
    "print(\"Class weights:\", class_weights_dict)\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(normalized_data, binary_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Print the shapes of the variables\n",
    "print(f\"Shape of X_train: {X_train.shape}\")\n",
    "print(f\"Shape of X_val: {X_val.shape}\")\n",
    "print(f\"Shape of y_train: {y_train.shape}\")\n",
    "print(f\"Shape of y_val: {y_val.shape}\")\n",
    "\n",
    "# Define the model\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=(normalized_data.shape[1], normalized_data.shape[2])),\n",
    "    tf.keras.layers.LSTM(64, return_sequences=False),  # LSTM layer with 64 units\n",
    "    tf.keras.layers.Dense(32, activation='relu'),     # Dense layer with 32 units\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')    # Output layer for binary classification\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model with class weights\n",
    "history = model.fit(X_train, y_train, epochs=20, batch_size=32, validation_data=(X_val, y_val), class_weight=class_weights_dict)\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_val, y_val)\n",
    "print(f\"Validation Loss: {loss}\")\n",
    "print(f\"Validation Accuracy: {accuracy}\")\n",
    "\n",
    "# Plot training & validation accuracy values\n",
    "plt.figure(figsize=(14, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add drop out and and early stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "# Compute class weights\n",
    "class_weights = class_weight.compute_class_weight(class_weight='balanced', classes=np.unique(binary_labels), y=binary_labels)\n",
    "class_weights_dict = {i : class_weights[i] for i in range(len(class_weights))}\n",
    "print(\"Class weights:\", class_weights_dict)\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(normalized_data, binary_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the model with Dropout and Early Stopping\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=(normalized_data.shape[1], normalized_data.shape[2])),\n",
    "    tf.keras.layers.LSTM(64, return_sequences=False),\n",
    "    tf.keras.layers.Dropout(0.5),  # Add Dropout with 50% rate\n",
    "    tf.keras.layers.Dense(32, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Define EarlyStopping callback\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "# Train the model with class weights and early stopping\n",
    "history = model.fit(X_train, y_train, epochs=50, batch_size=32, validation_data=(X_val, y_val), class_weight=class_weights_dict, callbacks=[early_stopping])\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_val, y_val)\n",
    "print(f\"Validation Loss: {loss}\")\n",
    "print(f\"Validation Accuracy: {accuracy}\")\n",
    "\n",
    "# Plot training & validation accuracy values\n",
    "plt.figure(figsize=(14, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Custom Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "def custom_loss(y_true, y_pred):\n",
    "    # Binary Cross-Entropy Loss\n",
    "    bce = tf.keras.losses.binary_crossentropy(y_true, y_pred)\n",
    "\n",
    "    # Calculate the number of trades (predicted positive class)\n",
    "    num_trades = tf.reduce_sum(y_pred)\n",
    "\n",
    "    # Penalize for not taking trades\n",
    "    trade_penalty = tf.maximum(0.0, 1.0 - num_trades / tf.cast(tf.shape(y_true)[0], tf.float32))\n",
    "\n",
    "    # Combine the losses\n",
    "    loss = bce + trade_penalty\n",
    "    return loss\n",
    "\n",
    "\n",
    "# Assuming binary_labels and normalized_data are already defined\n",
    "# Compute class weights\n",
    "class_weights = class_weight.compute_class_weight(class_weight='balanced', classes=np.unique(binary_labels), y=binary_labels)\n",
    "class_weights_dict = {i: class_weights[i] for i in range(len(class_weights))}\n",
    "print(\"Class weights:\", class_weights_dict)\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(normalized_data, binary_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the model with Dropout and Early Stopping\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=(normalized_data.shape[1], normalized_data.shape[2])),\n",
    "    tf.keras.layers.LSTM(64, return_sequences=False),\n",
    "    tf.keras.layers.Dropout(0.5),  # Add Dropout with 50% rate\n",
    "    tf.keras.layers.Dense(32, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the model with the custom loss function\n",
    "model.compile(optimizer='adam', loss=custom_loss, metrics=['accuracy'])\n",
    "\n",
    "# Define EarlyStopping callback\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "# Train the model with class weights and early stopping\n",
    "history = model.fit(X_train, y_train, epochs=50, batch_size=32, validation_data=(X_val, y_val), class_weight=class_weights_dict, callbacks=[early_stopping])\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_val, y_val)\n",
    "print(f\"Validation Loss: {loss}\")\n",
    "print(f\"Validation Accuracy: {accuracy}\")\n",
    "\n",
    "# Plot training & validation accuracy values\n",
    "plt.figure(figsize=(14, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
